{"Type": "Shape mismatch", "Buggy": "normal_dist = tf.truncated_normal(w_shape, stddev=stddev, name='normaldist')", "Fix": "normal_dist = tf.truncated_normal(w_shape, stddev=stddev, name='normaldist') normal_dist.set_shape([input_data.get_shape()[1], labels.get_shape()[1]])"}
{"Type": "Shape mismatch", "Buggy": "prop_1 = np.array((a,b)) prop_2 = np.array((x,y)) results = np.dot(prop_1, prop_2)", "Fix": "prop_1 = np.array((a,b)) prop_2 = np.array((x,y)) results = np.dot(prop_1.reshape(-1), prop_2.reshape(-1))"}
{"Type": "Shape mismatch", "Buggy": "import numpy as np a = np.array([1,2,3,4]) b = np.array([10,20,30,40]) from sklearn.linear_model import LinearRegression regr = LinearRegression() regr.fit(a,b)", "Fix": "import numpy as np a = np.array([1,2,3,4]).reshape(-1,1) b = np.array([10,20,30,40]) from sklearn.linear_model import LinearRegression regr = LinearRegression() regr.fit(a,b)"}
{"Type": "Shape mismatch", "Buggy": "a = tf.constant([[1,2], [3,4], [5,6]]) b = tf.squeeze(a)", "Fix": "a = tf.constant([[1,2], [3,4], [5,6]]) b = tf.reshape(a, [-1, 3, 1])"}
{"Type": "Shape mismatch", "Buggy": "import tensorflow as tf a = tf.constant([1,2,3,4,5]) b = tf.size(a) c = tf.constant([b,1]) d = tf.reshape(a, [b,1])", "Fix": "import tensorflow as tf a = tf.constant([1,2,3,4,5]) b = a. get_shape().as_list()[0] c = tf.constant([b,1]) d = tf.reshape(a, [b, 1])"}
{"Type": "Shape mismatch", "Buggy": "x = torch.randn(4, 4) y = x", "Fix": "x = torch.randn(4, 4) y = x.view(-1, 8)"}
{"Type": "Shape mismatch", "Buggy": "a=np.array([[1,2]]) b=np.array([[3,4]]) res = a@b", "Fix": "a=np.array([[1,2]]) b=np.array([[3,4]]) res = a@b.T"}
{"Type": "Shape mismatch", "Buggy": "res = np.multiply([1,0,0,1,0,0], [[0,1],[1,1],[1,0],[1,0],[1,1],[0,1]])", "Fix": "res = np.dot([1,0,0,1,0,0], [[0,1],[1,1],[1,0],[1,0],[1,1],[0,1]])"}
{"Type": "Numeric error", "Buggy": "cross_entropy = tf.reduce_sum(- y * tf.log(y_) - (1 - y) * tf.log(1 - y_), 1)", "Fix": "cross_entropy = tf.reduce_sum(- y * tf.log(tf.clip_by_value(y_, 1e-10, 1.0)) - (1 - y) * tf.log(tf.clip_by_value(1 - y_, 1e-10, 1.0)), 1)"}
{"Type": "Numeric error", "Buggy": "res = A / B", "Fix": "res = A / max(B, 1)"}
{"Type": "Numeric error", "Buggy": "self.softmax_out = tf.nn.softmax(tf.matmul(last_layer, self.softmax_W) + self.softmax_b) self.layer_nodes.append(self.softmax_out)", "Fix": "self.softmax_out = tf.matmul(last_layer, self.softmax_W) + self.softmax_b self.layer_nodes.append(self.softmax_out)"}
{"Type": "Numeric error", "Buggy": "correct_prediction = tf.equal(tf.greater(Y,0.5), tf.greater(Yhat,0.5))", "Fix": "Y_pred = tf.nn.sigmoid(Yhat) correct_prediction = tf.equal(tf.greater(Y,0.5), tf.greater(Y_pred,0.5))"}
{"Type": "Numeric error", "Buggy": "optimizer = tf.train.GradientDescentOptimizer(0.01)", "Fix": "optimizer = tf.train.GradientDescentOptimizer(0.001)"}
{"Type": "Numeric error", "Buggy": "loss = -tf.reduce_mean(tf.reduce_sum(y_label*tf.log(y_pred),reduction_indices=[1]))", "Fix": "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=y_pred,labels=y_label))"}
{"Type": "Numeric error", "Buggy": "cost = tf.reduce_mean(-tf.reduce_sum(y*tf.log(y_pred), reduction_indices=1))", "Fix": "cost = tf.reduce_mean(-tf.reduce_sum(y*tf.log(tf.maximum(y_pred, 1e-10)), reduction_indices=1))"}
{"Type": "Numeric error", "Buggy": "W3 = sum / span", "Fix": "W3 = sum / torch.clamp_min(span, 1)"}
{"Type": "Type mismatch", "Buggy": "mel_outputs, post_mel_outputs, stop_outputs, alignment_historys = tacotron2.inference(charactor,char_length,speaker_ids=tf.zeros(shape=[tf.shape(charactor)[0]]),)", "Fix": "mel_outputs, post_mel_outputs, stop_outputs, alignment_historys = tacotron2.inference(charactor,char_length,speaker_ids=tf.zeros(shape=[tf.shape(charactor)[0]], dtype=tf.int32),)"}
{"Type": "Type mismatch", "Buggy": "x = [1, 2, 7]", "Fix": "x = [1, 2, 7] x = tf.to_double(x)"}
{"Type": "Type mismatch", "Buggy": "batch_rows, batch_cols, batch_vals = next(batch)", "Fix": "batch_rows, batch_cols, batch_vals = next(batch) batch_rows = batch_rows.astype(np.int64) batch_cols = batch_cols.astype(np.int64) batch_vals = batch_vals.astype(np.float32)"}
{"Type": "Type mismatch", "Buggy": "num_a = np.arange(5) print(num_a.dtype)", "Fix": "num_a = np.arange(5) num_a = tf.cast(num_a, dtype=tf.float32) print(num_a.dtype)"}
{"Type": "Type mismatch", "Buggy": "c = np.ones((1,1))", "Fix": "c = np.ones((1,1)) c = tf.convert_to_tensor(c, dtype='float32')"}
{"Type": "Type mismatch", "Buggy": "a = tf.constant([2, 3])", "Fix": "a = tf.constant([2, 3]) sess = tf.Session() a = sess.run(a)"}
{"Type": "Type mismatch", "Buggy": "ten = torch.tensor([1, 2, 3])", "Fix": "ten = torch.tensor([1, 2, 3]) ten = ten.float()"}
{"Type": "Type mismatch", "Buggy": "res = MyData", "Fix": "res = MyData res = res.cuda()"}
{"Type": "Type mismatch", "Buggy": "c = 123", "Fix": "c = 123 c = float(c)"}
{"Type": "Type mismatch", "Buggy": "res = MyData", "Fix": "res = MyData res = res.cpu()"}
{"Type": "Type mismatch", "Buggy": "a = ['1', '4', '9']", "Fix": "a = ['1', '4', '9'] with tf.Session() as sess: a = tf.string_to_number(a, out_type = tf.int32)"}
{"Type": "API misuse", "Buggy": "losses = tf.nn.sparse_softmax_cross_entropy_with_logits(logits, targets)", "Fix": "losses = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits,labels=targets)"}
{"Type": "API misuse", "Buggy": "init_op = tf.initialize_local_variables()", "Fix": "init_op = tf.initialize_all_variables()"}
{"Type": "API misuse", "Buggy": "tf.initialize_all_variables().run()", "Fix": "tf.global_variables_initializer().run()"}
{"Type": "API misuse", "Buggy": "if args.cuda: model.cuda()", "Fix": "model = model.to(device)"}
{"Type": "API misuse", "Buggy": "a = 1", "Fix": "a = 1 device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"}
{"Type": "API misuse", "Buggy": "tf.losses.softmax_cross_entropy(logits, one_hot_labels)", "Fix": "tf.losses.softmax_cross_entropy(logits = logits, onehot_labels = one_hot_labels)"}
{"Type": "API misuse", "Buggy": "state = model.initial_state.eval()", "Fix": "state = sess.run(model.initial_state)"}
{"Type": "API misuse", "Buggy": "logits = torch.rand(2,2) pred = F.softmax(logits, dim=0) pred1 = F.log_softmax(logits)", "Fix": "logits = torch.rand(2,2) pred = F.softmax(logits, dim=0) pred1 = F.log_softmax(logits, dim=0)"}
{"Type": "Non-General", "Buggy": "samples = sess.run(dcgan.sampler, feed_dict={dcgan.z: z_sample})", "Fix": "if config.dataset == \"mnist\": y = np.random.choice(10, config.batch_size) y_one_hot = np.zeros((config.batch_size, 10)) y_one_hot[np.arange(config.batch_size), y] = 1 samples = sess.run(dcgan.sampler, feed_dict={dcgan.z: z_sample, dcgan.y: y_one_hot}) else: samples = sess.run(dcgan.sampler, feed_dict={dcgan.z: z_sample})"}
{"Type": "Non-General", "Buggy": "def __init__(self, n_input, n_hidden, optimizer = tf.train.AdamOptimizer(), gaussian_sample_size = 128): self.n_input = n_input self.n_hidden = n_hidden self.gaussian_sample_size = gaussian_sample_size", "Fix": "def __init__(self, n_input, n_hidden, optimizer = tf.train.AdamOptimizer()): self.n_input = n_input self.n_hidden = n_hidden"}
{"Type": "Non-General", "Buggy": "def test_batched_loss_is_correct(self): seq_length, batch_size, num_tags = 3, 10, 5 emissions = torch.autograd.Variable(torch.randn(seq_length, batch_size, num_tags), requires_grad=True) tags = torch.autograd.Variable(torch.LongTensor([[random.randrange(num_tags) for b in range(batch_size)] for _ in range(seq_length)])) crf = CRF(num_tags) initialize(crf)", "Fix": "def test_batched_loss_is_correct(self): crf = make_crf() batch_size = 10 emissions = make_emissions(batch_size=batch_size, num_tags=crf.num_tags) tags = make_tags(batch_size=batch_size, num_tags=crf.num_tags)"}
{"Type": "Non-General", "Buggy": "return T.nnet.bn.batch_normalization(x, gamma, beta, mean, sqrt(var) + epsilon, mode='high_mem')", "Fix": "return T.nnet.bn.batch_normalization(x, gamma, beta, mean, sqrt(var + epsilon), mode='high_mem')"}
{"Type": "Non-General", "Buggy": "scores = torch.max(model_output[\"scores\"], dim=-1)[1]", "Fix": "if \"captions\" in model_output: scores = model_output[\"captions\"] else: scores = torch.max(model_output[\"scores\"], dim=-1)[1]"}