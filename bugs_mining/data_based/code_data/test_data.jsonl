{"Type": "Shape mismatch", "Buggy": "discrim_A_grads = tf.gradients(discrim_A_interp, [A_interp])", "Fix": "discrim_A_grads = tf.gradients(discrim_A_interp, [A_interp]) discrim_A_grads = tf.squeeze(discrim_A_grads)"}
{"Type": "Shape mismatch", "Buggy": "edges = np.asarray(buckets[i, j])", "Fix": "edges = np.array(buckets[i, j], dtype=np.int64).reshape((-1, 2))"}
{"Type": "Shape mismatch", "Buggy": "data = np.array([0.1, 0.2]) x = tf.placeholder(\"float\", shape=[2]) T1 = tf.Variable(tf.ones([2,2])) l1 = tf.matmul(T1, x)", "Fix": "data = np.array([0.1, 0.2]) x = tf.placeholder(tf.float32, shape=[2]) T1 = tf.Variable(tf.ones([2, 2])) l1 = tf.matmul(T1, tf.expand_dims(x, 1))"}
{"Type": "Shape mismatch", "Buggy": "vs = ms.tex.verts_rgb_padded()", "Fix": "vs = ms.tex.verts_rgb_padded().view(-1, 3)"}
{"Type": "Shape mismatch", "Buggy": "def doit(): json_file = open('model.json', 'r') loaded_model_json = json_file.read() json_file.close() loaded_model = model_from_json(loaded_model_json) loaded_model.load_weights(\"model.h5\") x = [1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] z = np.array(x) prediction = loaded_model.predict(z) return prediction", "Fix": "def doit(): json_file = open('model.json', 'r') loaded_model_json = json_file.read() json_file.close() loaded_model = model_from_json(loaded_model_json) loaded_model.load_weights(\"model.h5\") x = [1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] z = np.array(x) z = z.reshape(-1,28) prediction = loaded_model.predict(z) return prediction"}
{"Type": "Shape mismatch", "Buggy": "c = np.array([[[0], [1], [2]]]) c = np.squeeze(c)", "Fix": "c = np.array([[[0], [1], [2]]]) c = np.squeeze(c, axis=0)"}
{"Type": "Shape mismatch", "Buggy": "a = np.array([[5, 1, 3], [1, 1, 1], [1, 2, 1]]) b = np.array([1, 2, 3]) W = a*b", "Fix": "a = np.array([[5, 1, 3], [1, 1, 1], [1, 2, 1]]) b = np.array([1, 2, 3]) W = np.matmul(a, b)"}
{"Type": "Shape mismatch", "Buggy": "a = torch.arange(0, 6) b = a.view(2, 3)", "Fix": "a = torch.arange(0, 6) b = a.view(2, 3).unsqueeze(1)"}
{"Type": "Numeric error", "Buggy": "cross_entropy = -tf.reduce_sum(y_*tf.log(y_conv))", "Fix": "cross_entropy = -tf.reduce_sum(y_*tf.log(tf.clip_by_value(y_conv,1e-10,1.0)))"}
{"Type": "Numeric error", "Buggy": "cross_entropy = -tf.reduce_sum(y*tf.log(yconv))", "Fix": "cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits, labels)"}
{"Type": "Numeric error", "Buggy": "trainer = tf.train.GradientDescentOptimizer(0.5).minimize(MSE)", "Fix": "trainer = tf.train.GradientDescentOptimizer(0.005).minimize(MSE)"}
{"Type": "Numeric error", "Buggy": "aa = 12 b = 0 aa = aa/b", "Fix": "aa = 12 b = 0 aa = aa/max(b, 1)"}
{"Type": "Numeric error", "Buggy": "loss = -tf.reduce_mean(tf.reduce_sum(y_label*tf.log(y_pred),reduction_indices=[1]))", "Fix": "y_ls = tf.nn.log_softmax(y_pred) loss = -tf.reduce_mean(tf.reduce_sum(y_label*y_ls,reduction_indices=[1]))"}
{"Type": "Numeric error", "Buggy": "cost = -tf.reduce_sum(y*tf.log(y_conv))", "Fix": "cost = tf.nn.softmax_cross_entropy_with_logits(logits, labels)"}
{"Type": "Numeric error", "Buggy": "loss = tf.reduce_mean(-tf.reduce_sum(y*tf.log(y_), reduction_indices=1))", "Fix": "loss = tf.reduce_mean(-tf.reduce_sum(y*tf.log(tf.maximum(y_, 1e-15)), reduction_indices=1))"}
{"Type": "Numeric error", "Buggy": "orgs = span_sum / span_len", "Fix": "orgs = span_sum / torch.clamp_min(span_len, 1)"}
{"Type": "Type mismatch", "Buggy": "return np.zeros(return_shape) + self.value", "Fix": "return np.zeros(return_shape, dtype=img.dtype) + self.value"}
{"Type": "Type mismatch", "Buggy": "r = '123'", "Fix": "r = '123' r = int(r)"}
{"Type": "Type mismatch", "Buggy": "feed_dict = {train_data_node: data, train_labels_node: label}", "Fix": "feed_dict = {train_data_node: data.eval(), train_labels_node: label.eval()}"}
{"Type": "Type mismatch", "Buggy": "self.items = data[:, :2]", "Fix": "self.items = data[:, :2].astype(np.int)"}
{"Type": "Type mismatch", "Buggy": "npn = [1, 2, 3, 4]", "Fix": "npn = [1, 2, 3, 4] npn = np.array(npn)"}
{"Type": "Type mismatch", "Buggy": "ten = tf.Variable(tf.random_normal([2, 2])) print(ten)", "Fix": "ten = tf.Variable(tf.random_normal([2, 2])) ten = tf.cast(ten,tf.float64) print(ten)"}
{"Type": "Type mismatch", "Buggy": "a = np.arange(5) print(a.dtype)", "Fix": "a = np.arange(5) a = tf.convert_to_tensor(a) print(a.dtype)"}
{"Type": "Type mismatch", "Buggy": "num_a = 56", "Fix": "num_a = 56 num_a = str(num_a)"}
{"Type": "Type mismatch", "Buggy": "arr_a = tf.constant([1, 2, 3])", "Fix": "arr_a = tf.constant([1, 2, 3]) sess = tf.Session() with sess.as_default(): arr_a = arr_a.eval()"}
{"Type": "Type mismatch", "Buggy": "tensor_a = arr = np.zeros(3)", "Fix": "tensor_a = arr = np.zeros(3) tensor_a = torch.from_numpy(tensor_a)"}
{"Type": "Type mismatch", "Buggy": "a = logs", "Fix": "a = logs a = a.cuda()"}
{"Type": "API misuse", "Buggy": "cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels, logits)", "Fix": "cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=labels)"}
{"Type": "API misuse", "Buggy": "return tf.nn.sampled_softmax_loss(w_t, v, inputs, labels, hps.num_softmax_samples, vsize)", "Fix": "return tf.nn.sampled_softmax_loss(weights=w_t, biases=v, labels=labels, inputs=inputs, num_sampled=hps.num_softmax_samples, num_classes=vsize)"}
{"Type": "API misuse", "Buggy": "cost = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(prediction,y))", "Fix": "cost = tf.nn.softmax_cross_entropy_with_logits(logits=prediction, labels=y)"}
{"Type": "API misuse", "Buggy": "def train(self, t_max): tf.initialize_all_variables().run()", "Fix": "def train(self, t_max): tf.global_variables_initializer().run()"}
{"Type": "API misuse", "Buggy": "model.cuda()", "Fix": "model = model.to(device)"}
{"Type": "API misuse", "Buggy": "if gpu: torch.set_default_tensor_type(\"torch.cuda.FloatTensor\")", "Fix": "if gpu and torch.cuda.is_available(): torch.cuda.set_device(device_id)"}
{"Type": "API misuse", "Buggy": "state = model.initial_state.eval()", "Fix": "state = sess.run(model.initial_state)"}
{"Type": "API misuse", "Buggy": "score = torch.rand(2,2) pred = F.log_softmax(score)", "Fix": "score = torch.rand(2,2) score = F.log_softmax(logits, dim=1)"}
{"Type": "Non-General", "Buggy": "for param in self.get_model().parameters(): param.requires_grad = False", "Fix": "if len(self.optimizers) > 1: for param in self.get_model().parameters(): param.requires_grad = False"}
{"Type": "Non-General", "Buggy": "if line.startswith(\"obj_info\"): items = line.split(\" \") if len(items) != 3: raise ValueError(\"Invalid line: %s\" % line) self.obj_info[items[1]] = items[2]", "Fix": "if line.startswith(\"obj_info \"): self.obj_info.append(line[9:])"}
{"Type": "Non-General", "Buggy": "def create_stem(in_chs, out_chs, stem_type='', preact=True, conv_layer=None, norm_layer=None): stem = OrderedDict()", "Fix": "def create_resnetv2_stem(in_chs, out_chs=64, stem_type='', preact=True,conv_layer=StdConv2d, norm_layer=partial(GroupNormAct, num_groups=32)): stem = OrderedDict()"}
{"Type": "Non-General", "Buggy": "def test_works_with_mask(self): seq_length, batch_size, num_tags = 3, 2, 5 emissions = torch.autograd.Variable(torch.randn(seq_length, batch_size, num_tags))", "Fix": "def test_works_with_mask(self): crf = make_crf() seq_length, batch_size = 3, 2 emissions = make_emissions(seq_length, batch_size, crf.num_tags)"}
{"Type": "Non-General", "Buggy": "current_sample.text_len = torch.tensor(len(sample_info[\"question_tokens\"]), dtype=torch.int)", "Fix": "if \"question_tokens\" in sample_info: current_sample.text_len = torch.tensor(len(sample_info[\"question_tokens\"]), dtype=torch.int)"}
{"Type": "Non-General", "Buggy": "task = config.task feature_config = task.features featurizer = create_featurizer(task.featurizer, feature_config)", "Fix": "supportedInputTensorizers = [FloatListTensorizer, GazetteerTensorizer, TokenTensorizer,] new_task = NewTask.from_config(config.task) input_tensorizers = { name: tensorizer for name, tensorizer in new_task.data.tensorizers.items() if any(isinstance(tensorizer, t) for t in supportedInputTensorizers) }"}
{"Type": "Non-General", "Buggy": "import pandas as pd self.result = pd.DataFrame(self.result) self.result.columns = self.result.iloc[0] self.result = self.result.drop(0)", "Fix": "import pandas as pd cols = self.result[0] self.result = pd.DataFrame(self.result[1:]) self.result.columns = cols"}
{"Type": "Non-General", "Buggy": "norm_locs = normalize_points((H,W), batch_locs.view(num_pts, 2).transpose(1,0)) norm_locs = torch.cat((norm_locs, torch.ones(1, num_pts)), dim=0) transtheta = transthetas[:2,:] norm_locs = torch.mm(transtheta, norm_locs) real_locs = denormalize_points(shape.tolist(), norm_locs)", "Fix": "norm_locs = torch.cat((batch_locs[0].transpose(1,0), torch.ones(1, num_pts)), dim=0) norm_locs = torch.mm(transthetas[:2, :], norm_locs) real_locs = denormalize_points(shape.tolist(), norm_locs)"}
{"Type": "Non-General", "Buggy": "read_weights = hidden['read_weights'].gather(1, hidden['read_positions']) write_weights = hidden['write_weights'].gather(1, hidden['read_positions'])", "Fix": "read_weights = hidden['read_weights'].gather(1, hidden['read_positions']) if self.timestep == 1: read_weights = read_weights + 1 write_weights = hidden['write_weights'].gather(1, hidden['read_positions'])"}
{"Type": "Non-General", "Buggy": "for time in range(max_length): chx = self.rnns[layer](input[time], chx)", "Fix": "for time in range(max_length): layer_input = input[time] hchx = [] for hlayer in range(self.num_hidden_layers): h = self.rnns[layer][hlayer](layer_input, chx[hlayer]) layer_input = h[0] if self.rnn_type.lower() == 'lstm' else h hchx.append(h) chx = hchx"}
{"Type": "Non-General", "Buggy": "model = models.resnet50().cuda(gpu) model.fc = nn.Identity() state_dict = torch.load(args.pretrained, map_location='cpu') model.load_state_dict(state_dict)", "Fix": "model = models.resnet50().cuda(gpu) state_dict = torch.load(args.pretrained, map_location='cpu') missing_keys, unexpected_keys = model.load_state_dict(state_dict, strict=False) assert missing_keys == ['fc.weight', 'fc.bias'] and unexpected_keys == [] model.fc.weight.data.normal_(mean=0.0, std=0.01) model.fc.bias.data.zero_()"}
{"Type": "Non-General", "Buggy": "mask = np.array([c == class_name for c in gt_names], dtype=np.bool_)", "Fix": "mask = np.array([c == class_name for c in gt_names], dtype=np.bool_) anchors = anchor_dict[\"anchors\"].reshape(-1, self.box_coder.code_size) num_anchors = anchors.shape[0] anchors_mask_class = anchors_mask[anchor_start_idx:anchor_start_idx+num_anchors] if anchors_mask is not None: prune_anchor_fn = lambda _: np.where(anchors_mask_class)[0] else: prune_anchor_fn = None"}
{"Type": "Non-General", "Buggy": "samples = get_samples(gen, batch_size)", "Fix": "samples = get_samples(gen, batch_size) sample_count += batch_size if reset_interval is not None and sample_count >= reset_interval: gen = make_gen()"}
{"Type": "Non-General", "Buggy": "best_valid_metrics = checkpoints[0][1]", "Fix": "all_epochs_metrics = [(f\"epoch_{order_index}\", valid_metric) for (order_index, valid_metric) in enumerate(self.epochs_metrics)] best_valid_metrics = top_best_checkpoints[0][1]"}
{"Type": "Non-General", "Buggy": "loss_func = NTXentLoss(temperature=0.1)", "Fix": "loss_func = NTXentLoss(temperature=0.1) all_embedding_angles = [[0], [0, 10, 20]] all_labels = [torch.LongTensor([0]), torch.LongTensor([0, 0, 0])]"}
{"Type": "Non-General", "Buggy": "self.writer = Logger(self.config) registry.register(\"writer\", self.writer)", "Fix": "writer = registry.get(\"writer\", no_warning=True) if writer: self.writer = writer else: self.writer = Logger(self.config) registry.register(\"writer\", self.writer)"}
{"Type": "Non-General", "Buggy": "def __call__(self, item): return {\"text\": self.tokenizer(item[\"text\"])}", "Fix": "def __call__(self, item, *args, **kwargs): return {\"text\": self.tokenizer(item[\"text\"], *args, **kwargs)}"}
{"Type": "Non-General", "Buggy": "user_dir = get_mmf_env(key=\"user_dir\") if user_dir: possible_paths.append(os.path.join(user_dir, paths)) mmf_root = get_mmf_root()", "Fix": "mmf_root = get_mmf_root() user_dir = get_mmf_env(key=\"user_dir\") if user_dir: possible_paths.append(os.path.join(user_dir, paths)) possible_paths.append(os.path.join(mmf_root, \"..\", user_dir, paths))"}
{"Type": "Non-General", "Buggy": "for t_model in getattr(self, embedding_attr): text_embedding = t_model(texts)", "Fix": "for t_model in getattr(self, embedding_attr): if isinstance(t_model, PreExtractedEmbedding): text_embedding = t_model(info['question_id']) else: text_embedding = t_model(texts)"}
{"Type": "Non-General", "Buggy": "if (batch_idx+1) % (len(train_dataloader) / 2) == 0: print(\"Epoch Average Train loss : \", total_epoch_train_loss / (batch_idx+1))", "Fix": "if ((batch_idx+1) % (len(train_dataloader) / 2) == 0) and ((batch_idx+1) < len(train_dataloader)): print(\"Half-Epoch Average Train loss : \", total_epoch_train_loss / (batch_idx+1))"}
{"Type": "Non-General", "Buggy": "if self.use_scheduler_: schedulers = [] for i in range(self.n_estimators): schedulers.append(set_module.set_scheduler(optimizers[i], self.scheduler_name, **self.scheduler_args))", "Fix": "if self.use_scheduler_: scheduler_ = set_module.set_scheduler(optimizers[0], self.scheduler_name, **self.scheduler_args)"}
{"Type": "Non-General", "Buggy": "initialize_model(model, cfg, src_padding_idx, trg_padding_idx)", "Fix": "initialize_model(model, cfg, src_padding_idx, trg_padding_idx) pretrained_enc_embed_path = cfg[\"encoder\"][\"embeddings\"].get(\"load_pretrained\", None) pretrained_dec_embed_path = cfg[\"decoder\"][\"embeddings\"].get(\"load_pretrained\", None)"}
{"Type": "Non-General", "Buggy": "def _run_act_layer_grad(act_type): x = torch.rand(10, 1000) * 10 m = MLP(act_layer=act_type)", "Fix": "def _run_act_layer_grad(act_type, inplace=True): x = torch.rand(10, 1000) * 10 m = MLP(act_layer=act_type, inplace=inplace)"}
{"Type": "Non-General", "Buggy": "for batch in data.batch_q_iter: feed_dict = {inputs: batch} results = model.infer(sess, feed_dict) bs_infer = results['output']['bs_infer'] batch_hyps = gen_text(bs_infer, ...) bs_turn = dict(question=question, answer=batch_hyps[1][i])", "Fix": "for batch in data.batch_q_iter: feed_dict = {inputs: batch} results = model.infer(sess, feed_dict) bs_infer = results['output']['bs_infer'] batch_hyps = gen_text(bs_infer, ...) if beam_width > 0: bs_turn = dict(question=question, answer=batch_hyps[1][i])"}
{"Type": "Non-General", "Buggy": "def call(self, inputs, mask=None) input_shape = K.int_shape(inputs) if input_shape[0]: def step(x, _): output = self.layer.call(x) return output, []", "Fix": "def call(self, inputs, training=None, mask=None): kwargs = {} func_args = inspect.getargspec(self.layer.call).args if 'training' in func_args: kwargs['training'] = training input_shape = K.int_shape(inputs) if input_shape[0]: def step(x, _): output = self.layer.call(x, **kwargs) return output, []"}
{"Type": "Non-General", "Buggy": "bboxlist = detect(self.face_detector, image, device=self.device)[0] keep = nms(bboxlist, 0.3) bboxlist = bboxlist[keep, :] bboxlist = [x for x in bboxlist if x[-1] > 0.5]", "Fix": "bboxlist = detect(self.face_detector, image, device=self.device)[0] bboxlist = self._filter_bboxes(bboxlist)"}
{"Type": "Non-General", "Buggy": "if args.audio_backend != \"stempeg\": torchaudio.set_audio_backend(args.audio_backend)", "Fix": "if args.audio_backend != \"stempeg\" and args.audio_backend is not None: torchaudio.set_audio_backend(args.audio_backend)"}
