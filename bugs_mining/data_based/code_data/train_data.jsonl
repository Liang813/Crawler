{"Type": "Shape mismatch", "Buggy": "return np.squeeze(self.model.predict(x, **kwargs))", "Fix": "return np.squeeze(self.model.predict(x, **kwargs), axis=-1)"}
{"Type": "Shape mismatch", "Buggy": "crossEntropy = -torch.log(torch.gather(inp, 1, target.view(-1, 1)))", "Fix": "crossEntropy = -torch.log(torch.gather(inp, 1, target.view(-1, 1)).squeeze(1))"}
{"Type": "Shape mismatch", "Buggy": "x = tf.cast(tf.squeeze(x), dtype=tf.float32) p = tf.cast(tf.squeeze(p), dtype=tf.float32)", "Fix": "x = tf.cast(x, dtype=tf.float32) p = tf.cast(p, dtype=tf.float32)"}
{"Type": "Shape mismatch", "Buggy": "outputs = self.head(lstm_out) mu = outputs[:, :self._action_size] log_std = outputs[:, self._action_size:-1] v = outputs[:, -1].squeeze(-1)", "Fix": "outputs = self.head(lstm_out.view(T * B, -1)) mu = outputs[:, :self._action_size] log_std = outputs[:, self._action_size:-1] v = outputs[:, -1]"}
{"Type": "Shape mismatch", "Buggy": "softmax_output = self.crit(pred_hid.view(-1, pred_hid.size(-1)), labels)", "Fix": "softmax_output = self.crit(pred_hid, labels)"}
{"Type": "Shape mismatch", "Buggy": "input_shape = input_ids.shape", "Fix": "input_shape = tf.shape(input_ids)"}
{"Type": "Shape mismatch", "Buggy": "output_shape = [input_layer.shape[0], out_rows, out_cols, depth] y = tf.nn.conv2d_transpose(input_layer, params, output_shape, stride, edges)", "Fix": "output_shape = [tf.shape(input_layer.tensor)[0], out_rows, out_cols,depth] y = tf.nn.conv2d_transpose(input_layer, params, output_shape, stride, edges) y.set_shape([input_layer.shape[0], out_rows, out_cols, depth])"}
{"Type": "Shape mismatch", "Buggy": "self.masked_v = tf.matmul(tf.squeeze(self.caps2), tf.reshape(self.Y, (-1, 10, 1)), transpose_a=True)", "Fix": "self.masked_v = tf.multply(tf.squeeze(self.caps2), tf.reshape(self.Y, (-1, 10, 1)))"}
{"Type": "Shape mismatch", "Buggy": "ybar = model(xadv) yshape = ybar.get_shape().as_list() n, ydim = yshape[0], yshape[1]", "Fix": "ybar = model(xadv) ydim = ybar.get_shape().as_list()[1] n = tf.shape(ybar)[0]"}
{"Type": "Shape mismatch", "Buggy": "edges = np.asarray(buckets[i, j])", "Fix": "edges = np.array(buckets[i, j], dtype=np.int64).reshape((-1, 3))"}
{"Type": "Shape mismatch", "Buggy": "target = Variable(torch.arange(1, 11))", "Fix": "target = Variable(torch.arange(1, 11)) target = target.view(1, -1)"}
{"Type": "Shape mismatch", "Buggy": "x = op.inputs[0] d0 = x.get_shape().as_list()[0] d = tf.convert_to_tensor([d0], dtype=tf.int32)", "Fix": "x = op.inputs[0] d = tf.shape(x) d = tf.reshape(d[0], [1])"}
{"Type": "Shape mismatch", "Buggy": "box_regression = permute_and_flatten(box_regression, N, A, 4, H, W) box_regression = box_regression.reshape(N, -1, 4)", "Fix": "box_regression = permute_and_flatten(box_regression, N, A, 4, H, W)"}
{"Type": "Shape mismatch", "Buggy": "return np.expand_dims(in_[..., 1:-1, 1:-1], -3)", "Fix": "return in_[..., 1:-1, 1:-1]"}
{"Type": "Shape mismatch", "Buggy": "return torch.unsqueeze(in_[..., 1:-1, 1:-1], -3)", "Fix": "return in_[..., 1:-1, 1:-1]"}
{"Type": "Shape mismatch", "Buggy": "y = tf.placeholder(tf.float32, [None,n_classes])", "Fix": "y = tf.placeholder(tf.float32, [None]) y_one_hot = tf.one_hot( y , 10 )"}
{"Type": "Shape mismatch", "Buggy": "y = tf.placeholder(tf.float32, [None, n_classes])", "Fix": "y = tf.placeholder(tf.float32, [None])"}
{"Type": "Shape mismatch", "Buggy": "data = np.array([0.1, 0.2]) x = tf.placeholder(\"float\", shape=[2]) T1 = tf.Variable(tf.ones([2,2])) l1 = tf.matmul(T1, x)", "Fix": "data = np.array([[0.1], [0.2]]) x = tf.placeholder(tf.float32, shape=[2, 1]) T1 = tf.Variable(tf.ones([2, 2])) l1 = tf.matmul(T1, x)"}
{"Type": "Shape mismatch", "Buggy": "vertex_textures = meshes.textures.verts_rgb_padded()", "Fix": "vertex_textures = meshes.textures.verts_rgb_padded().reshape(-1, 3)"}
{"Type": "Shape mismatch", "Buggy": "v_ts = ms.tex.verts_rgb_padded()", "Fix": "v_ts = ms.tex.verts_rgb_padded().view(-1, 3)"}
{"Type": "Shape mismatch", "Buggy": "tf.register_gradient(Mygrad) def Mygrad(op,grad): return cust_grad*grad", "Fix": "tf.register_gradient(Mygrad) def Mygrad(op,grad): return tf.matmul(tf.reshape(cust_grad,[calculated_shape]),tf.reshape(grad,expeced_shape))"}
{"Type": "Shape mismatch", "Buggy": "data_set = tf.data.Dataset.from_tensor_slices((data.values, target))", "Fix": "data_set = tf.data.Dataset.from_tensor_slices((data.values, target)).batch(8)"}
{"Type": "Shape mismatch", "Buggy": "a1 = np.array((a,b)) a2 = np.array((x,y)) results = np.dot(a1, a2)", "Fix": "a1 = np.array((a,b)) a2 = np.array((x,y)) results = np.dot(a1.T, a2)"}
{"Type": "Shape mismatch", "Buggy": "a1 = np.array((a,b)) a2 = np.array((x,y)) results = np.dot(a1, a2)", "Fix": "a1 = np.array((a,b)) a2 = np.array((x,y)) results = np.einsum('ij,ij->j', a1, a2).item()"}
{"Type": "Shape mismatch", "Buggy": "def func(x,y,z): return x+y+z y = linspace(0,1,100) z = linspace(0,1,100) x0 = zeros((y.size,z.size)) + 0.5 yz = (y[:,newaxis],z[newaxis,:]) x, info, iterations, message = fsolve(func,x0,yz)", "Fix": "def func(x, y, z): x = x.reshape(y.size, z.size) return (x + y + z).ravel() y = linspace(0,1,100) z = linspace(0,1,100) x0 = zeros((y.size,z.size)) + 0.5 yz = (y[:,newaxis],z[newaxis,:]) sol, info, iterations, message  = fsolve(func, x0.ravel(), args=yz, full_output=True) x = sol.reshape(y.size, z.size)"}
{"Type": "Shape mismatch", "Buggy": "obs = tf.zeros([8]) q_network = Sequential([Dense(40, input_dim=observation_dimension, activation='relu'),Dense(40, activation='relu'),Dense(number_of_actions, activation='linear')]) q_network.predict(obs)", "Fix": "obs = tf.zeros([8]) q_network = Sequential([Dense(40, input_dim=observation_dimension, activation='relu'),Dense(40, activation='relu'),Dense(number_of_actions, activation='linear')]) q_network.predict(obs.reshape(1, 8))"}
{"Type": "Shape mismatch", "Buggy": "model = Sequential() model.add(Dense(input_dim=93, units=40, activation=\"sigmoid\")) model.add(Dense(units=2, activation=\"linear\")) sgd = optimizers.SGD(lr=0.01, clipvalue=0.5) model.compile(loss=\"mse\", optimizer=sgd, learning_rate=0.01) n_batches = 10 data = np.random.randint(0,2,93*n_batches) model.predict(data)", "Fix": "model = Sequential() model.add(Dense(input_dim=93, units=40, activation=\"sigmoid\")) model.add(Dense(units=2, activation=\"linear\")) sgd = optimizers.SGD(lr=0.01, clipvalue=0.5) model.compile(loss=\"mse\", optimizer=sgd, learning_rate=0.01) n_batches = 10 data = np.random.randint(0,2,93*n_batches) data = data.reshape(-1,93) model.predict(data)"}
{"Type": "Shape mismatch", "Buggy": "def doit(): json_file = open('model.json', 'r') loaded_model_json = json_file.read() json_file.close() loaded_model = model_from_json(loaded_model_json) loaded_model.load_weights(\"model.h5\") x = [1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] z = np.array(x) prediction = loaded_model.predict(z) return prediction", "Fix": "def doit(): json_file = open('model.json', 'r') loaded_model_json = json_file.read() json_file.close() loaded_model = model_from_json(loaded_model_json) loaded_model.load_weights(\"model.h5\") x = [1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] z = np.array(x) z = z[:, np.newaxis].T prediction = loaded_model.predict(z) return prediction"}
{"Type": "Shape mismatch", "Buggy": "def doit(): json_file = open('model.json', 'r') loaded_model_json = json_file.read() json_file.close() loaded_model = model_from_json(loaded_model_json) loaded_model.load_weights(\"model.h5\") x = [1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] z = np.array(x) prediction = loaded_model.predict(z) return prediction", "Fix": "def doit(): json_file = open('model.json', 'r') loaded_model_json = json_file.read() json_file.close() loaded_model = model_from_json(loaded_model_json) loaded_model.load_weights(\"model.h5\") x = [1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] z = np.array(x) z = z.reshape(1,-1) prediction = loaded_model.predict(z) return prediction"}
{"Type": "Shape mismatch", "Buggy": "tf.register_gradient(Mygrad) def Mygrad(op,grad): cust_grad = 2 return cust_grad*grad", "Fix": "tf.register_gradient(Mygrad) def Mygrad(op,grad): cust_grad = 2 return tf.matmul(tf.reshape(cust_grad,[calculated_shape]),tf.reshape(grad,expeced_shape))"}
{"Type": "Shape mismatch", "Buggy": "poly_features = PolynomialFeatures(degree=2, include_bias=False) linear_reg = LinearRegression(fit_intercept = True) X = df_copy[[\"open\",\"volume\", \"base volume\", \"RSI_14\"]] X_poly = poly_features.fit_transform(X)[1]", "Fix": "poly_features = PolynomialFeatures(degree=2, include_bias=False) linear_reg = LinearRegression(fit_intercept = True) X = df_copy[[\"open\",\"volume\", \"base volume\", \"RSI_14\"]] X_poly = poly_features.fit_transform(X.values.reshape(1,-1))[1]"}
{"Type": "Shape mismatch", "Buggy": "best_model = models[cv_scores.index(max(cv_scores))] best_model.save_model() best_model.predict_entries(X[0])", "Fix": "best_model = models[cv_scores.index(max(cv_scores))] best_model.save_model() X_reshape = X[0].reshape(1, 2139) best_model.predict_entries(X_reshape)"}
{"Type": "Shape mismatch", "Buggy": "train_X = np.ones((2,3,4,5,6))", "Fix": "train_X = np.ones((2,3,4,5,6)) train_X_ = np.swapaxes(train_X, 3, 1)"}
{"Type": "Shape mismatch", "Buggy": "def set_shapes(img, label, img_shape=(128,128,3)): img.set_shape(img_shape) label.set_shape([]) return img, label", "Fix": "def set_shapes(img, label, img_shape=(128,128,3)): img.set_shape(img_shape) label.set_shape([1,]) return img, label"}
{"Type": "Shape mismatch", "Buggy": "a = torch.tensor([[2.4]]) torch.squeeze(a, 1) print(a)", "Fix": "a = torch.tensor([[2.4]]) a = a.squeeze(1) print(a)"}
{"Type": "Shape mismatch", "Buggy": "a = torch.tensor([[2.4]]) torch.squeeze(a, 1) print(a)", "Fix": "a = torch.tensor([[2.4]]) a.squeeze_(1) print(a)"}
{"Type": "Shape mismatch", "Buggy": "x = np.array([[[0], [1], [2]]]) a = np.squeeze(x)", "Fix": "x = np.array([[[0], [1], [2]]]) a = np.squeeze(x, axis=0)"}
{"Type": "Shape mismatch", "Buggy": "a_S = tf.reshape(a_S, [n_C, n_H*n_W]) a_G = tf.reshape(a_G, [n_C, n_H*n_W]) GS = gram_matrix(a_S) GG = gram_matrix(a_G)", "Fix": "a_S = tf.reshape(a_S, [n_H*n_W, n_C]) a_G = tf.reshape(a_G, [n_H*n_W, n_C]) GS = gram_matrix(tf.transpose(a_S)) GG = gram_matrix(tf.transpose(a_G))"}
{"Type": "Shape mismatch", "Buggy": "import tensorflow as tf import random import numpy as np x = tf.placeholder('float') x = tf.reshape(x, [-1,28,28,1]) with tf.Session() as sess: x1 = np.asarray([random.uniform(0,1) for i in range(784)]) result = sess.run(x, feed_dict={x: x1}) print(result)", "Fix": "import tensorflow as tf import random import numpy as np x = tf.placeholder('float') y = tf.reshape(x, [-1,28,28,1]) with tf.Session() as sess: x1 = np.asarray([random.uniform(0,1) for i in range(784)]) result = sess.run(y, feed_dict={x: x1}) print(result)"}
{"Type": "Shape mismatch", "Buggy": "sess = tf.InteractiveSession() m = 100 n = 300 x = 123 y = 456 a = tf.get_variable(dtype=tf.int32, shape=[m, n, x], name=\"a\") b = tf.get_variable(dtype=tf.int32, shape=[m, n, y], name=\"b\") c = tf.concat([a, b], axis=-1) s = tf.shape(c) cc = tf.reshape(c, [s[0]*s[1], -1])", "Fix": "sess = tf.InteractiveSession() m = 100 n = 300 x = 123 y = 456 a = tf.get_variable(dtype=tf.int32, shape=[m, n, x], name=\"a\") b = tf.get_variable(dtype=tf.int32, shape=[m, n, y], name=\"b\") c = tf.concat([a, b], axis=-1) s = c.get_shape().as_list() cc = tf.reshape(c, [s[0]*s[1], -1])"}
{"Type": "Shape mismatch", "Buggy": "sess = tf.InteractiveSession() m = 100 n = 300 x = 123 y = 456 a = tf.get_variable(dtype=tf.int32, shape=[m, n, x], name=\"a\") b = tf.get_variable(dtype=tf.int32, shape=[m, n, y], name=\"b\") c = tf.concat([a, b], axis=-1) s = tf.shape(c) cc = tf.reshape(c, [s[0]*s[1], -1])", "Fix": "sess = tf.InteractiveSession() m = 100 n = 300 x = 123 y = 456 a = tf.get_variable(dtype=tf.int32, shape=[m, n, x], name=\"a\") b = tf.get_variable(dtype=tf.int32, shape=[m, n, y], name=\"b\") c = tf.concat([a, b], axis=-1) s = c.shape.as_list() cc = tf.reshape(c, [s[0]*s[1], -1])"}
{"Type": "Shape mismatch", "Buggy": "height = tf.cast(features['height'], tf.int32) width = tf.cast(features['width'], tf.int32) image = tf.reshape(image,[height, width, 3])", "Fix": "height = tf.cast(features['height'], tf.int32) width = tf.cast(features['width'], tf.int32) image = tf.reshape(image, tf.stack([height, width, 3]))"}
{"Type": "Shape mismatch", "Buggy": "one_hot_label = tf.nn.embedding_lookup(np.eye(vocab_size), Y[labels_i]) print(one_hot_label)", "Fix": "one_hot_label = tf.nn.embedding_lookup(np.eye(vocab_size), Y[labels_i]) one_hot_label = tf.expand_dims(one_hot_label, axis=2) print(one_hot_label)"}
{"Type": "Shape mismatch", "Buggy": "dist_vector = np.arange(5000) print(dist_vector.shape)", "Fix": "dist_vector = np.arange(5000) dist_vector = dist_vector[:, np.newaxis] print(dist_vector.shape)"}
{"Type": "Shape mismatch", "Buggy": "X = tf.placeholder(tf.float32, (36)) Y = tf.placeholder(tf.float32) W = tf.Variable(tf.zeros([36], name=\"weight\")) b = tf.Variable(tf.zeros([1]), name=\"bias\") activation = tf.add(tf.matmul(X, W), b)", "Fix": "X = tf.placeholder(tf.float32, (36)) Y = tf.placeholder(tf.float32) W = tf.Variable(tf.zeros([36], name=\"weight\")) b = tf.Variable(tf.zeros([1]), name=\"bias\") activation = tf.add(tf.multiply(X, W), b)"}
{"Type": "Shape mismatch", "Buggy": "my_array = np.array([[[ 0.,  0.,  0.,  0.], [ 0.,  0.,  0.,  0.]], [[ 0.,  0.,  0.,  0.], [ 0.,  0.,  0.,  0.]]]) new_array = np.expand_dims(my_array, axis=3) print(new_array)", "Fix": "my_array = np.array([[[ 0.,  0.,  0.,  0.], [ 0.,  0.,  0.,  0.]], [[ 0.,  0.,  0.,  0.], [ 0.,  0.,  0.,  0.]]]) new_array = my_array.reshape(2,2,2,2) print(new_array)"}
{"Type": "Shape mismatch", "Buggy": "a = torch.Tensor([[1, 2, 3], [4, 5, 6]])", "Fix": "a = torch.Tensor([[1, 2, 3], [4, 5, 6]]) a = a.view(-1)"}
{"Type": "Shape mismatch", "Buggy": "data_1 = torch.randn(2, 1, 3)", "Fix": "data_1 = torch.randn(2, 1, 3) data_1 = data_1.transpose(0, 1)"}
{"Type": "Shape mismatch", "Buggy": "import numpy as np d = 764 n_neurons = 100 W = np.random.rand(d, n_neurons) X = np.random.rand(d, n_neurons) Y = W @ X", "Fix": "import numpy as np d = 764 n_neurons = 100 W = np.random.rand(d, n_neurons) X = np.random.rand(d, n_neurons) Y = W @ X.T"}
{"Type": "Shape mismatch", "Buggy": "W = np.array([[1, 2], [3, 4]]) b = np.array([9, 10]).reshape(2, 1) x = np.array([4, 5]).reshape(2, 1) h = np.array([1,2]) W @ np.dot(b, b) + np.eye(2,2)@x", "Fix": "W = np.array([[1, 2], [3, 4]]) b = np.array([9, 10]).reshape(2, 1) x = np.array([4, 5]).reshape(2, 1) h = np.array([1,2]) W @ np.dot(b, b.T) + np.eye(2,2)@x"}
{"Type": "Shape mismatch", "Buggy": "W = tf.constant([[1, 2], [3, 4]]) b = tf.constant([[9, 10]]) res = tf.matmul(W, b)", "Fix": "W = tf.constant([[1, 2], [3, 4]]) b = tf.reshape(tf.constant([[9, 10]]), (2, 1)) res = tf.matmul(W, b)"}
{"Type": "Shape mismatch", "Buggy": "W = np.array([[1, 2], [3, 4]]) b = np.array([9, 10]).reshape(2, 1) x = np.array([4, 5]) res = W @ np.dot(b,x)", "Fix": "W = np.array([[1, 2], [3, 4]]) b = np.array([9, 10]).reshape(2, 1) x = np.array([4, 5]).reshape(1, 2) res = W @ np.dot(b,x)"}
{"Type": "Shape mismatch", "Buggy": "a = np.array([[5, 1, 3], [1, 1, 1], [1, 2, 1]]) b = np.array([1, 2, 3]) W = a*b", "Fix": "a = np.array([[5, 1, 3], [1, 1, 1], [1, 2, 1]]) b = np.array([1, 2, 3]) W = a.dot(b)"}
{"Type": "Shape mismatch", "Buggy": "a = np.array([[5, 1, 3], [1, 1, 1], [1, 2, 1]]) b = np.array([1, 2, 3]) W = a*b", "Fix": "a = np.array([[5, 1, 3], [1, 1, 1], [1, 2, 1]]) b = np.array([1, 2, 3]) W = np.einsum('ji,i->j', a, b)"}
{"Type": "Shape mismatch", "Buggy": "res = np.multiply([1,0,0,1,0,0], [[0,1],[1,1],[1,0],[1,0],[1,1],[0,1]])", "Fix": "res = np.matmul([1,0,0,1,0,0], [[0,1],[1,1],[1,0],[1,0],[1,1],[0,1]])"}
{"Type": "Shape mismatch", "Buggy": "a = torch.rand(4,4) b = torch.rand(4) res = a.mm(b)", "Fix": "a = torch.rand(4,4) b = torch.rand(4) res = torch.mv(a,b)"}
{"Type": "Shape mismatch", "Buggy": "c = torch.arange(0, 6)", "Fix": "c = torch.arange(0, 6) c = c.view(2, 3).unsqueeze(1)"}
{"Type": "Shape mismatch", "Buggy": "A = np.array([[1, 2, 3, 4, 5], [6, 7, 8, 9, 10], [11,12,13,14,15]]) B = np.array([[1,0,0],[0,2,0],[0,0,3]]) C = np.zeros(5) for i in range(5): C[i] = np.linalg.multi_dot([A[:,i].T, B, A[:,i]]) res = C", "Fix": "A = np.array([[1, 2, 3, 4, 5], [6, 7, 8, 9, 10], [11,12,13,14,15]]) B = np.array([[1,0,0],[0,2,0],[0,0,3]]) C = np.einsum('ji,jk,ki->i',A,B,A) res = C"}
{"Type": "Shape mismatch", "Buggy": "A = np.array([[1, 2, 3, 4, 5], [6, 7, 8, 9, 10], [11,12,13,14,15]]) B = np.array([[1,0,0],[0,2,0],[0,0,3]]) C = np.zeros(5) for i in range(5): C[i] = np.linalg.multi_dot([A[:,i].T, B, A[:,i]]) res = C", "Fix": "A = np.array([[1, 2, 3, 4, 5], [6, 7, 8, 9, 10], [11,12,13,14,15]]) B = np.array([[1,0,0],[0,2,0],[0,0,3]]) C = (A.T[:,None,:]@B@A.T[:,:,None]).squeeze() res = C"}
{"Type": "Numeric error", "Buggy": "denominator = K.sqrt(K.batch_dot(l1, l1, self.dot_axes) * K.batch_dot(l2, l2, self.dot_axes)) a = dje/denominator", "Fix": "denominator = K.sqrt(K.batch_dot(l1, l1, self.dot_axes) * K.batch_dot(l2, l2, self.dot_axes)) denominator = K.maximum(denominator, 1e-15) a = dje/denominator"}
{"Type": "Numeric error", "Buggy": "orig_embeddings = span_embeddings_sum / span_embeddings_len", "Fix": "orig_embeddings = span_embeddings_sum / torch.clamp_min(span_embeddings_len, 1)"}
{"Type": "Numeric error", "Buggy": "self.val_check_batch = int(self.nb_tng_batches * self.val_check_interval) res = val_sum / self.val_check_batch", "Fix": "self.val_check_batch = int(self.nb_tng_batches * self.val_check_interval) self.val_check_batch = max(1, self.val_check_batch) res = val_sum / self.val_check_batch"}
{"Type": "Numeric error", "Buggy": "numerator = self._weighted_product(X, X2) theta = tf.acos(tf.clip_by_value(numerator / X_denominator[:, None] / X2_denominator[None, :],-1., 1.))", "Fix": "numerator = self._weighted_product(X, X2) cos_theta = numerator / X_denominator[:, None] / X2_denominator[None, :] jitter = settings.numerics.jitter_level theta = tf.acos(jitter + (1 - 2 * jitter) * cos_theta)"}
{"Type": "Numeric error", "Buggy": "cost = - tf.reduce_mean(ref_input * tf.log(model_output) + (1 - ref_input) * tf.log(1 - model_output))", "Fix": "cost = - tf.reduce_mean(ref_input * tf.log(tf.clip_by_value(model_output, 1e-10, float('inf'))) + (1 - ref_input) * tf.log(tf.clip_by_value(1 - model_output, 1e-10, float('inf'))))"}
{"Type": "Numeric error", "Buggy": "W_fc2 = weight_variable([n_fc, 10]) b_fc2 = bias_variable([10]) y_pred = tf.nn.softmax(tf.matmul(h_fc1_drop, W_fc2) + b_fc2) cross_entropy = -tf.reduce_sum(y * tf.log(y_pred))", "Fix": "W_fc2 = weight_variable([n_fc, 10]) b_fc2 = bias_variable([10]) y_logits = tf.matmul(h_fc1_drop, W_fc2) + b_fc2 cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(y_logits, y))"}
{"Type": "Numeric error", "Buggy": "cost = -tf.reduce_sum(y*tf.log(activation))", "Fix": "cost = tf.reduce_mean(-tf.reduce_sum(y*tf.log(activation), reduction_indices=1))"}
{"Type": "Numeric error", "Buggy": "self.d_loss_real = tf.nn.sigmoid_cross_entropy_with_logits(self.D_logits, tf.ones_like(self.D)) self.d_loss_fake = tf.nn.sigmoid_cross_entropy_with_logits(self.D_logits_, tf.zeros_like(self.D_))", "Fix": "self.d_loss_real = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(self.D_logits, tf.ones_like(self.D))) self.d_loss_fake = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(self.D_logits_, tf.zeros_like(self.D_)))"}
{"Type": "Numeric error", "Buggy": "v = 0.5 + (1 - 1e-10) * (samples - 0.5) samples_tf = torch.erfinv(2 * v - 1) * math.sqrt(2)", "Fix": "v = 0.5 + (1 - torch.finfo(samples.dtype).eps) * (samples - 0.5) samples_tf = torch.erfinv(2 * v - 1) * math.sqrt(2)"}
{"Type": "Numeric error", "Buggy": "self.softmax_out = tf.matmul(last_layer, self.softmax_W) + self.softmax_b self.layer_nodes.append(self.softmax_out)", "Fix": "self.softmax_out = tf.nn.softmax(tf.matmul(last_layer, self.softmax_W) + self.softmax_b) self.layer_nodes.append(self.softmax_out)"}
{"Type": "Numeric error", "Buggy": "tot = tf.reduce_sum(y_true) wmape = tf.realdiv(tf.reduce_sum(tf.abs(tf.subtract(y_true,y_pred))),tot)*100", "Fix": "tot = tf.reduce_sum(y_true) tot = tf.clip_by_value(tot, clip_value_min=1,clip_value_max=1000) wmape = tf.realdiv(tf.reduce_sum(tf.abs(tf.subtract(y_true,y_pred))),tot)*100#/tot"}
{"Type": "Numeric error", "Buggy": "opt = tf.train.AdamOptimizer(1e-3)", "Fix": "opt = tf.train.AdamOptimizer(1e-3, epsilon=1e-4)"}
{"Type": "Numeric error", "Buggy": "cross_entropy = -tf.reduce_sum(y_*tf.log(y))", "Fix": "cross_entropy = -tf.reduce_mean(y_*tf.log(y))"}
{"Type": "Numeric error", "Buggy": "train_step = tf.train.GradientDescentOptimizer(0.01).minimize(cross_entropy)", "Fix": "train_step = tf.train.GradientDescentOptimizer(0.005).minimize(cross_entropy)"}
{"Type": "Numeric error", "Buggy": "cross_entropy = -tf.reduce_sum(y_*tf.log(y))", "Fix": "cross_entropy = -tf.reduce_sum(y_*tf.log(y + 1e-10))"}
{"Type": "Numeric error", "Buggy": "self.w_upd8 = self.W.assign_add(self.learning_rate * (positive - negative))", "Fix": "self.w_upd8 = self.W.assign_add(self.learning_rate * (positive - negative)/tf.shape(self.input_data)[0])"}
{"Type": "Numeric error", "Buggy": "cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y_conv), reduction_indices=[1]))", "Fix": "with tf.name_scope('cross_entropy'): diff = y_ * (tf.nn.log_softmax(y_conv)) with tf.name_scope('total'): cross_entropy = tf.reduce_mean(-tf.reduce_sum(diff, reduction_indices=[1])) tf.scalar_summary('cross entropy', cross_entropy)"}
{"Type": "Numeric error", "Buggy": "with tf.name_scope(\"cost\"): if loss_func == 'cross_entropy': cost = -tf.reduce\\_mean(ref\\_input * tf.log(model_output)", "Fix": "with tf.name_scope(\"cost\"): if loss_func == 'cross_entropy': cost = -tf.reduce\\_mean(ref\\_input * tf.log(tf.clip_by_value(model_output, 1e-10, float('inf')))"}
{"Type": "Numeric error", "Buggy": "optimizer = tf.train.AdamOptimizer(learning_rate=1e-4, epsilon=1e-5)", "Fix": "optimizer = tf.train.AdamOptimizer(learning_rate=5e-5, epsilon=1e-5)"}
{"Type": "Numeric error", "Buggy": "loss = -tf.reduce_mean(tf.reduce_sum(y_label*tf.log(y_pred),reduction_indices=[1]))", "Fix": "eps = 1e-10 y_clip = tf.clip_by_value(y_pred, eps, 1.0-eps) loss = -tf.reduce_mean(tf.reduce_sum(y_label*tf.log(y_clip),reduction_indices=[1]))"}
{"Type": "Numeric error", "Buggy": "loss_A = y * log(y_pred)", "Fix": "loss_A = y * log(tf.clip_by_value(y_pred, 1e-10, 1e100))"}
{"Type": "Numeric error", "Buggy": "cross_entropy = -tf.reduce_sum(y_*tf.log(y_conv))", "Fix": "y_clip = tf.clip_by_value(y_conv,1e-10,1.0) cross_entropy = -tf.reduce_sum(y_*tf.log(y_clip))"}
{"Type": "Numeric error", "Buggy": "hypothesis = tf.sigmoid(tf.matmul(X, W) + b) cost = -tf.reduce_mean(Y * tf.log(hypothesis) + (1 - Y) * tf.log(1 - hypothesis))", "Fix": "hypothesis = tf.sigmoid(tf.matmul(X, W) + b) cost = -tf.reduce_mean(Y*(tf.log(hypothesis+1e-4))+(1-Y)*(tf.log(1-hypothesis+1e-4)))"}
{"Type": "Numeric error", "Buggy": "tempX = x tempW = W tempMult = tf.matmul(tempX, W) s = tempMult + b p = tf.exp(s) / tf.reduce_sum(tf.exp(s), axis=1)", "Fix": "tempX = x tempW = W tempMult = tf.matmul(tempX, W) s = tempMult + b p = tf.exp(s) / tf.reshape( tf.reduce_sum(tf.exp(s), axis=1), [-1,1] )"}
{"Type": "Numeric error", "Buggy": "starter_learning_rate = 0.5 learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step, 10000, 0.96)", "Fix": "starter_learning_rate = 0.01 learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step, 10000, 0.96)"}
{"Type": "Numeric error", "Buggy": "learning_rate = 0.01 optimizer = tf.train.GradientDescentOptimizer(learning_rate)", "Fix": "learning_rate = 0.001 optimizer = tf.train.GradientDescentOptimizer(learning_rate)"}
{"Type": "Numeric error", "Buggy": "def model(X,w,b): return tf.multiply(X,w) + b pred = model(X,w,b) cost = tf.square(Y-pred)", "Fix": "def model(X,w,b): return tf.multiply(X,w) + b pred = model(X,w,b) cost = -tf.reduce_sum(Y*tf.log(tf.clip_by_value(pred,1e-10,1.0)))"}
{"Type": "Numeric error", "Buggy": "W2 = tf.get_variable('W2', shape=[10 * 10 * 32, 1], initializer=tf.contrib.layers.xavier_initializer()) b = tf.Variable(tf.random_normal([1])) hypothesis = tf.matmul(L1, W2) + b cost = -tf.reduce_mean(Y * tf.log(hypothesis) + (1 - Y) * tf.log(1 - hypothesis))", "Fix": "W2 = tf.get_variable('W2', shape=[10 * 10 * 32, 1], initializer=tf.contrib.layers.xavier_initializer()) b = tf.Variable(tf.random_normal([1])) hypothesis = tf.nn.softmax(tf.add(tf.matmul(L1, W2), b)) cost = -tf.reduce_mean(Y * tf.log(hypothesis) + (1 - Y) * tf.log(1 - hypothesis))"}
{"Type": "Numeric error", "Buggy": "W2 = tf.get_variable('W2', shape=[10 * 10 * 32, 1], initializer=tf.contrib.layers.xavier_initializer()) b = tf.Variable(tf.random_normal([1])) hypothesis = tf.matmul(L1, W2) + b cost = -tf.reduce_mean(Y * tf.log(hypothesis) + (1 - Y) * tf.log(1 - hypothesis))", "Fix": "W2 = tf.get_variable('W2', shape=[10 * 10 * 32, 1], initializer=tf.contrib.layers.xavier_initializer()) b = tf.Variable(tf.random_normal([1])) hypothesis = tf.matmul(L1, W2) + b cost = tf.nn.softmax_cross_entropy_with_logits(labels=Y, logits=hypothesis)"}
{"Type": "Numeric error", "Buggy": "learning_rate = 0.001 optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)", "Fix": "learning_rate = 0.0001 optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)"}
{"Type": "Numeric error", "Buggy": "with tf.name_scope(\"loss\"): loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels))", "Fix": "with tf.name_scope(\"loss\"): loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits, labels))"}
{"Type": "Numeric error", "Buggy": "train_step = tf.train.GradientDescentOptimizer(learning_rate=0.003).minimize(cost_function)", "Fix": "train_step = tf.train.GradientDescentOptimizer(learning_rate=0.0003).minimize(cost_function)"}
{"Type": "Numeric error", "Buggy": "lr = tf.train.exponential_decay(get_initial_learning_rate(), global_step, decay_steps, get_learning_rate_decay_factor(), staircase=True) opt = tf.train.GradientDescentOptimizer(lr)", "Fix": "opt = tf.train.AdamOptimizer(get_initial_learning_rate(), beta1=0.9, beta2=0.999, epsilon=1e-08, use_locking=False)"}
{"Type": "Numeric error", "Buggy": "cost = tf.reduce_mean(-tf.reduce_sum(y * tf.log(pred), reduction_indices=[1]))", "Fix": "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(pred, lables))"}
{"Type": "Numeric error", "Buggy": "cost = tf.reduce_mean(-tf.reduce_sum(y*tf.log(pred), reduction_indices=1))", "Fix": "cost = tf.reduce_mean(-tf.reduce_sum(y*tf.log(tf.maximum(pred, 1e-15)), reduction_indices=1))"}
{"Type": "Numeric error", "Buggy": "cost = tf.reduce_mean(-tf.reduce_sum(y*tf.log(pred), reduction_indices=1))", "Fix": "cost = tf.reduce_mean(-tf.reduce_sum(y*tf.log(tf.clip_by_value(pred, 1e-15, 1.0)), reduction_indices=1))"}
{"Type": "Numeric error", "Buggy": "criterion = torch.nn.MSELoss() loss = criterion(y, pred)", "Fix": "criterion = nn.CrossEntropyLoss() loss = criterion(zs, ts)"}
{"Type": "Numeric error", "Buggy": "def focal_loss(y_real, y_pred, gamma = 2): y_pred = torch.sigmoid(y_pred) return -torch.sum((1 - y_pred)**gamma * y_real * torch.log(y_pred) + y_pred**gamma * (1 - y_real) * torch.log(1 - y_pred))", "Fix": "def focal_loss(y_real, y_pred, eps = 1e-8, gamma = 0): probabilities = torch.clamp(torch.sigmoid(y_pred), min=eps, max=1-eps) return torch.mean((1 - probabilities)**gamma * (y_pred - y_real * y_pred + torch.log(1 + torch.exp(-y_pred))))"}
{"Type": "Numeric error", "Buggy": "def forward(self, x): x = torch.sigmoid(self.fc1(x)) x = torch.sigmoid(self.fc2(x)) x = self.fc3(x) return F.softmax(x, dim=-1) model = Net() optimizer = torch.optim.SGD(model.parameters(), lr=0.0001, momentum=0.9) loss_fn = nn.NLLLoss()", "Fix": "def forward(self, x): x = torch.sigmoid(self.fc1(x)) x = torch.sigmoid(self.fc2(x)) x = self.fc3(x) return F.log_softmax(x, dim=-1) model = Net() optimizer = torch.optim.SGD(model.parameters(), lr=0.0001, momentum=0.9) loss_fn = nn.NLLLoss()"}
{"Type": "Numeric error", "Buggy": "def backward(ctx, grad_output): y_pred, y = ctx.saved_tensors grad_input = torch.mean(-2.0 * (y - y_pred)).repeat(y_pred.shape[0]) return grad_input, None", "Fix": "def backward(ctx, grad_output): y_pred, y = ctx.saved_tensors grad_input = 2 * (y_pred - y) / y_pred.shape[0] return grad_input, None"}
{"Type": "Numeric error", "Buggy": "prediction = tf.nn.softmax(tf.matmul(h_fc1_drop, W_fc2) + b_fc2) cross_entropy = tf.reduce_mean(-tf.reduce_sum(ys * tf.log(prediction), reduction_indices=[1]))", "Fix": "prediction = tf.nn.softmax(tf.matmul(h_fc1_drop, W_fc2) + b_fc2) cross_entropy = tf.reduce_mean(-tf.reduce_sum(ys * tf.log(prediction + 1e-10), reduction_indices=[1]))"}
{"Type": "Numeric error", "Buggy": "prediction = tf.nn.softmax(tf.matmul(h_fc1_drop, W_fc2) + b_fc2) cross_entropy = tf.reduce_mean(-tf.reduce_sum(ys * tf.log(prediction), reduction_indices=[1]))", "Fix": "prediction = tf.nn.softmax(tf.matmul(h_fc1_drop, W_fc2) + b_fc2) cross_entropy = tf.reduce_mean(-tf.reduce_sum(ys * tf.log(tf.clip_by_value(prediction, 1e-10,1.0)), reduction_indices=[1]))"}
{"Type": "Numeric error", "Buggy": "optimizer=tf.train.GradientDescentOptimizer(learning_rate=0.05).minimize(loss)", "Fix": "optimizer=tf.train.GradientDescentOptimizer(learning_rate=0.000001).minimize(loss)"}
{"Type": "Numeric error", "Buggy": "optimizer=tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(loss)", "Fix": "optimizer=tf.train.GradientDescentOptimizer(learning_rate=0.001).minimize(loss)"}
{"Type": "Numeric error", "Buggy": "min_n = sum_xy / xy", "Fix": "min_n = sum_xy / torch.clamp_min(xy, 1)"}
{"Type": "Numeric error", "Buggy": "num_a = sum / a_len", "Fix": "num_a = sum / max(a_len, 1)"}
{"Type": "Numeric error", "Buggy": "res = A / B", "Fix": "res = A / (B + 0.00001)"}
{"Type": "Numeric error", "Buggy": "soft_out = tf.matmul(last_layer, soft_W) + soft_b", "Fix": "soft_out = tf.nn.softmax(tf.matmul(last_layer, soft_W) + soft_b)"}
{"Type": "Type mismatch", "Buggy": "tp = torch.sum(gt == pr) score = tp / gt.view(-1).shape[0]", "Fix": "tp = torch.sum(gt == pr, dtype=pr.dtype) score = tp / gt.view(-1).shape[0]"}
{"Type": "Type mismatch", "Buggy": "padded = np.full((len(examples), pad_length), pad_value, dtype=np.long)", "Fix": "padded = np.full((len(examples), pad_length), pad_value, dtype=np.int64)"}
{"Type": "Type mismatch", "Buggy": "self.dtype = np.int32 if is_cat else np.float32", "Fix": "self.dtype = np.int64 if is_cat else np.float32"}
{"Type": "Type mismatch", "Buggy": "a = [1, 4, 6]", "Fix": "a = [1, 4, 6] a = tf.to_double(a)"}
{"Type": "Type mismatch", "Buggy": "mask &= input_tensor != pad_id", "Fix": "mask &= torch.as_tensor(input_tensor != pad_id, dtype=torch.uint8)"}
{"Type": "Type mismatch", "Buggy": "@tf.function(experimental_relax_shapes=True,input_signature=[tf.TensorSpec([None, None], dtype=tf.int32),tf.TensorSpec([None, None], dtype=tf.int32),tf.TensorSpec([None, None], dtype=tf.int32),tf.TensorSpec([None, None], dtype=tf.int32),tf.TensorSpec([None, None, 80], dtype=tf.float32)])", "Fix": "@tf.function(experimental_relax_shapes=True,input_signature=[tf.TensorSpec([None, None], dtype=tf.int32),tf.TensorSpec([None, None], dtype=tf.int32),tf.TensorSpec([None, None], dtype=tf.float32),tf.TensorSpec([None, None], dtype=tf.float32),tf.TensorSpec([None, None, 80], dtype=tf.float32)])"}
{"Type": "Type mismatch", "Buggy": "not_dones = 1. - dones", "Fix": "not_dones = 1. - tf.cast(dones, dtype=tf.float32)"}
{"Type": "Type mismatch", "Buggy": "return math.ceil(float(size) / float(stride))", "Fix": "return int(math.ceil(float(size) / float(stride)))"}
{"Type": "Type mismatch", "Buggy": "return slim.l1_regularizer(scale=regularizer.l1_regularizer.weight)", "Fix": "return slim.l1_regularizer(scale=float(regularizer.l1_regularizer.weight))"}
{"Type": "Type mismatch", "Buggy": "all_toks = all_toks + (attr_value if isinstance(attr_value, (list, tuple)) else [attr_value])", "Fix": "all_toks = all_toks + (list(attr_value) if isinstance(attr_value, (list, tuple)) else [attr_value])"}
{"Type": "Type mismatch", "Buggy": "p_init = tfd.Categorical(probs=np.ones(nstates) / nstates) pswitch = 0.05 pt = pswitch / (nstates - 1) * np.ones([nstates, nstates], dtype=np.float32)", "Fix": "p_init = tfd.Categorical(probs=np.float32(np.ones(nstates) / nstates)) pswitch = 0.05 pt = pswitch / (nstates - 1) * np.ones([nstates, nstates])"}
{"Type": "Type mismatch", "Buggy": "rgb = self.to_hwc_tensor().numpy()", "Fix": "rgb = self.to_hwc_tensor().numpy().astype(np.float32)"}
{"Type": "Type mismatch", "Buggy": "K.set_value(self.states[i], states[i])", "Fix": "K.set_value(self.states[i], K.eval(states[i]))"}
{"Type": "Type mismatch", "Buggy": "input_mask = 1.0 - attention_mask", "Fix": "input_mask = 1.0 - tf.cast(attention_mask, dtype=dtype_float)"}
{"Type": "Type mismatch", "Buggy": "if _s == pred_s[i] and _e == pred_e[i]]): em.update(1)", "Fix": "if _s == torch.from_numpy(pred_s[i]) and _e == torch.from_numpy(pred_e[i])]): em.update(1)"}
{"Type": "Type mismatch", "Buggy": "attn = torch.matmul(selected_attn_probs.transpose(1, 2), selected_v.transpose(1, 2).type_as(selected_attn_probs)).transpose(1, 2)", "Fix": "attn = torch.matmul(selected_attn_probs.transpose(1, 2), selected_v.transpose(1, 2)).transpose(1, 2)"}
{"Type": "Type mismatch", "Buggy": "ds[\"segmentation\"] = np.empty(1, object) ds[\"area\"] = np.empty(1, object) ds[\"iscrowd\"] = np.empty(1, object)", "Fix": "ds[\"segmentation\"] = np.empty(1, np.uint32) ds[\"area\"] = np.empty(1, np.uint32) ds[\"iscrowd\"] = np.empty(1, np.uint8)"}
{"Type": "Type mismatch", "Buggy": "batch_size = cdna_input.get_shape()[0] height = prev_image.get_shape()[1] width = prev_image.get_shape()[2]", "Fix": "batch_size = int(cdna_input.get_shape()[0]) height = int(prev_image.get_shape()[1]) width = int(prev_image.get_shape()[2])"}
{"Type": "Type mismatch", "Buggy": "actions = tf.split(axis=1, num_or_size_splits=actions.get_shape()[1], value=actions)", "Fix": "actions = tf.split(axis=1, num_or_size_splits=int(actions.get_shape()[1]), value=actions)"}
{"Type": "Type mismatch", "Buggy": "x = tf.zeros_like(m.input_data)", "Fix": "x = tf.zeros_like(m.input_data).eval()"}
{"Type": "Type mismatch", "Buggy": "some_test = tf.constant(tf.random_normal([2, 2], mean=0.0, stddev=1.0, dtype=tf.float32))", "Fix": "some_test = tf.constant(np.random.normal(loc=0.0, scale=1.0, size=(2, 2)).astype(np.float32))"}
{"Type": "Type mismatch", "Buggy": "original_dist = 2. - 2 * original_dist original_dist = np.power(original_dist, 2)", "Fix": "original_dist = 2. - 2 * original_dist original_dist = np.power(original_dist, 2).astype(np.float32)"}
{"Type": "Type mismatch", "Buggy": "y[t, ...] = torch.tensor(0.0, device=x.device)", "Fix": "y[t, ...] = torch.tensor(0.0, device=x.device, dtype=x.dtype)"}
{"Type": "Type mismatch", "Buggy": "y = tf.placeholder(tf.float32, [None, n_input, n_input, n_classes], name=\"ground_truth\") sess.run([cost, accuracy], feed_dict={x: batch_x, y: batch_y, keep_prob: 1.0})", "Fix": "y = tf.placeholder(tf.float32, [None, n_input, n_input, n_classes], name=\"ground_truth\") batch_y = convert_to_2_channel(batch_y, batch_size) sess.run([cost, accuracy], feed_dict={x: batch_x, y: batch_y, keep_prob: 1.0})"}
{"Type": "Type mismatch", "Buggy": "mask = get_mask() mask = mask.astype('int') mask[mask == 0] = 255 mask[mask == 1] = 0 fg_masked = cv2.bitwise_and(img, img, mask=mask)", "Fix": "mask = get_mask() mask = mask.astype('np.int8') mask[mask == 0] = 255 mask[mask == 1] = 0 fg_masked = cv2.bitwise_and(img, img, mask=mask)"}
{"Type": "Type mismatch", "Buggy": "python_list = [] nparray = python_list", "Fix": "python_list = [] nparray = np.array(python_list)"}
{"Type": "Type mismatch", "Buggy": "arr = np.array([1, 2, 3])", "Fix": "arr = np.array([1, 2, 3]) list1 = arr.tolist()"}
{"Type": "Type mismatch", "Buggy": "flipped_images = tf.image.random_flip_left_right(images)", "Fix": "flipped_images = tf.image.random_flip_left_right(tf.convert_to_tensor(images))"}
{"Type": "Type mismatch", "Buggy": "sum=tf.add(sum,overate)", "Fix": "sum=tf.add(tf.cast(sum, overate.dtype),overate)"}
{"Type": "Type mismatch", "Buggy": "est = build_estimator(model_dir) tf.cast(training_set, tf.float32)", "Fix": "est = build_estimator(model_dir) training_set = tf.cast(training_set, tf.float32)"}
{"Type": "Type mismatch", "Buggy": "ten = tf.Variable(tf.random_normal([2, 2])) print(ten)", "Fix": "ten = tf.Variable(tf.random_normal([2, 2],dtype=tf.float64)) print(ten)"}
{"Type": "Type mismatch", "Buggy": "tensor_a = tf.constant([0, 2, 5])", "Fix": "tensor_a = tf.constant([0, 2, 5]) boolean_a = tf.cast(tensor_a,dtype=tf.bool)"}
{"Type": "Type mismatch", "Buggy": "tensor_a = tf.range(5)", "Fix": "tensor_a = tf.range(5) b = tf.Variable(tensor_a)"}
{"Type": "Type mismatch", "Buggy": "a = np.ones((1,2))", "Fix": "a = np.ones((1,2)) a = tf.convert_to_tensor(a, dtype='float32')"}
{"Type": "Type mismatch", "Buggy": "arr_a = tf.constant([1, 2, 3])", "Fix": "arr_a = tf.constant([1, 2, 3]) sess = tf.Session() arr_a = sess.run(arr_a)"}
{"Type": "Type mismatch", "Buggy": "arr_a = tf.constant([1, 2, 3])", "Fix": "arr_a = tf.constant([1, 2, 3]) sess = tf.InteractiveSession() arr_a = arr_a.eval()"}
{"Type": "Type mismatch", "Buggy": "npa = np.ones([3, 3])", "Fix": "npa = np.ones([3, 3]) npa = torch.from_numpy(npa)"}
{"Type": "Type mismatch", "Buggy": "npa = np.ones([3, 3]) npa = torch.from_numpy(npa, dtype = torch.float32)", "Fix": "npa = np.ones([3, 3]) npa = torch.from_numpy(npa)"}
{"Type": "Type mismatch", "Buggy": "npa = np.ones([3, 3]) npa = torch.from_numpy(npa, dtype = torch.int32)", "Fix": "npa = np.ones([3, 3]) npa = torch.from_numpy(npa).int()"}
{"Type": "Type mismatch", "Buggy": "arr = torch.zeros(3)", "Fix": "arr = torch.zeros(3) arr = arr.numpy()"}
{"Type": "Type mismatch", "Buggy": "ten = torch.tensor([1.3, 1.5, 1.7])", "Fix": "ten = torch.tensor([1.3, 1.5, 1.7]) ten = ten.long()"}
{"Type": "Type mismatch", "Buggy": "ten = torch.tensor([-1, 2, 256])", "Fix": "ten = torch.tensor([-1, 2, 256]) ten = ten.type(torch.uint8)"}
{"Type": "Type mismatch", "Buggy": "a = torch.tensor([1, 2, 3]) b = torch.tensor([1.1, 2.2, 3.3])", "Fix": "a = torch.tensor([1, 2, 3]) b = torch.tensor([1.1, 2.2, 3.3]) b = b.type_as(a)"}
{"Type": "Type mismatch", "Buggy": "ndarray = np.array([1, 2, 3, 4])", "Fix": "ndarray = np.array([1, 2, 3, 4]) ndarray = ndarray.tolist()"}
{"Type": "Type mismatch", "Buggy": "a = torch.tensor([1, 2, 3])", "Fix": "a = torch.tensor([1, 2, 3]) a = a.numpy()"}
{"Type": "Type mismatch", "Buggy": "tensor_a = torch.tensor([1, 2, 3])", "Fix": "tensor_a = torch.tensor([1, 2, 3]) tensor_a = tensor_a.numpy().tolist()"}
{"Type": "Type mismatch", "Buggy": "data = MyData", "Fix": "data = MyData data = data.cuda()"}
{"Type": "Type mismatch", "Buggy": "data = MyData", "Fix": "data = MyData data = data.cpu()"}
{"Type": "Type mismatch", "Buggy": "a = logs", "Fix": "a = logs a = a.cpu()"}
{"Type": "Type mismatch", "Buggy": "data = MyData", "Fix": "data = MyData data = data.to('cuda:1')"}
{"Type": "Type mismatch", "Buggy": "a = list(range(1, 6))", "Fix": "a = list(range(1, 6)) a = torch.tensor(a)"}
{"Type": "Type mismatch", "Buggy": "res = (1, 2, 3)", "Fix": "res = (1, 2, 3) res = str(res)"}
{"Type": "Type mismatch", "Buggy": "result = '123'", "Fix": "result = '123' result = list(result)"}
{"Type": "Type mismatch", "Buggy": "d = 'abc'", "Fix": "d = 'abc' d = tuple(d)"}
{"Type": "Type mismatch", "Buggy": "a1 = tf.constant([1, 2])", "Fix": "a1 = tf.constant([1, 2], dtype = tf.float32)"}
{"Type": "Type mismatch", "Buggy": "aTensor = np.arange(5)", "Fix": "aTensor = np.arange(5) aTensor = tf.convert_to_tensor(aTensor)"}
{"Type": "Type mismatch", "Buggy": "var_tf = tf.range(5)", "Fix": "var_tf = tf.range(5) var_tf = tf.Variable(var_tf)"}
{"Type": "Type mismatch", "Buggy": "floata=torch.tensor([[1,2,3],[4,5,6]])", "Fix": "floata=torch.tensor([[1,2,3],[4,5,6]]) floata = floata.float()"}
{"Type": "Type mismatch", "Buggy": "floatb=torch.tensor([[1,2,3],[4,5,6]])", "Fix": "floatb=torch.tensor([[1,2,3],[4,5,6]]) floatb = floatb.to(dtype = torch.double)"}
{"Type": "Type mismatch", "Buggy": "t1=torch.tensor([[1,2,3],[4,5,6]],dtype=torch.float64) t2=torch.tensor([[1,2],[3,4],[5,6]]) torch.mm(t1, t2)", "Fix": "t1=torch.tensor([[1,2,3],[4,5,6]],dtype=torch.float64) t2=torch.tensor([[1,2],[3,4],[5,6]], dtype=torch.float64) torch.mm(t1, t2)"}
{"Type": "Type mismatch", "Buggy": "x = ['1', '2', '7']", "Fix": "x = ['1', '2', '7'] with tf.Session() as sess: x = tf.string_to_number(x, out_type = tf.int32)"}
{"Type": "API misuse", "Buggy": "cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits, labels, name='cross_entropy_per_example')", "Fix": "cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=labels, name='cross_entropy_per_example')"}
{"Type": "API misuse", "Buggy": "Loss = tf.nn.sampled_softmax_loss(w_t, v, inputs, labels, hps.num_softmax_samples, vsize)", "Fix": "Loss = tf.nn.sampled_softmax_loss(weights=w_t, biases=v, labels=labels, inputs=inputs, num_sampled=hps.num_softmax_samples, num_classes=vsize)"}
{"Type": "API misuse", "Buggy": "mixed = tf.concat(3, [tower_conv, tower_conv1_1, tower_conv2_2])", "Fix": "mixed = tf.concat(axis=3, values=[tower_conv, tower_conv1_1, tower_conv2_2])"}
{"Type": "API misuse", "Buggy": "losses = tf.nn.sparse_softmax_cross_entropy_with_logits(decoder_output.logits, labels)", "Fix": "losses = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=decoder_output.logits,labels=labels)"}
{"Type": "API misuse", "Buggy": "outputs = tf.split(axis=tf.nn.log_softmax(output), num_or_size_splits=beam_size, value=0)", "Fix": "outputs = tf.split(axis=0, num_or_size_splits=beam_size, value=tf.nn.log_softmax(output))"}
{"Type": "API misuse", "Buggy": "res = tf.split(axis=tf.nn.log_softmax(output), num_or_size_splits=beam_size, value=0)", "Fix": "res = tf.split(axis=0, num_or_size_splits=beam_size, value=tf.nn.log_softmax(output))"}
{"Type": "API misuse", "Buggy": "self.saver = tf.train.Saver() self.saver.save(self.sess, os.path.join(checkpoint_dir, model_name), global_step=step, max_to_keep=1)", "Fix": "self.saver = tf.train.Saver(max_to_keep=1) self.saver.save(self.sess, os.path.join(checkpoint_dir, model_name), global_step=step)"}
{"Type": "API misuse", "Buggy": "storage['session'] = tf.Session()", "Fix": "storage['session'] = tf.Session() storage['session'].run(tf.initialize_all_variables())"}
{"Type": "API misuse", "Buggy": "init_op = tf.initialize_all_variables()", "Fix": "init_op = tf.group(tf.initialize_all_variables(), tf.initialize_local_variables())"}
{"Type": "API misuse", "Buggy": "sess = tf.Session(target=self.target, config=self.config) sess.run(tf.global_variables_initializer()) sess.run(tf.local_variables_initializer())", "Fix": "sess = tf.Session(target=self.target, config=self.config) sess.run(tf.global_variables_initializer()) sess.run(tf.local_variables_initializer()) sess.run(tf.tables_initializer())"}
{"Type": "API misuse", "Buggy": "tf.initialize_all_variables().run()", "Fix": "tf.global_variables_initializer().run()"}
{"Type": "API misuse", "Buggy": "self.W = Param(np.ones((state_dim, state_dim)), trainable=False)", "Fix": "self.W = Param(np.eye(state_dim, state_dim), trainable=False)"}
{"Type": "API misuse", "Buggy": "model = models.__dict__[network_data['arch']](network_data).cuda() model.eval()", "Fix": "model = models.__dict__[network_data['arch']](network_data).to(device) model.eval()"}
{"Type": "API misuse", "Buggy": "input_var = torch.tensor(torch.cat([img1, img2]).cuda()).unsqueeze(0)", "Fix": "input_var = torch.cat([img1, img2]).unsqueeze(0)"}
{"Type": "API misuse", "Buggy": "args.cuda = not args.no_cuda and torch.cuda.is_available()", "Fix": "use_cuda = not args.no_cuda and torch.cuda.is_available() device = torch.device('cuda' if use_cuda else 'cpu')"}
{"Type": "API misuse", "Buggy": "if args.cuda: model.cuda()", "Fix": "model = model.to(device)"}
{"Type": "API misuse", "Buggy": "if gpu: torch.set_default_tensor_type(\"torch.cuda.FloatTensor\")", "Fix": "if gpu and torch.cuda.is_available(): torch.cuda.set_device(device_id)"}
{"Type": "API misuse", "Buggy": "def _initialize(self): self.backend_module.init_distributed()", "Fix": "def _initialize(self): self.backend_module.init_distributed() if torch.cuda.is_available(): torch.cuda.set_device(self._get_local_rank())"}
{"Type": "API misuse", "Buggy": "state = self.cell.zero_state(1, tf.float32).eval()", "Fix": "state = sess.run(self.cell.zero_state(1, tf.float32))"}
{"Type": "API misuse", "Buggy": "distorted_image = tf.image.resize_images(distorted_image, height, width, resize_method)", "Fix": "distorted_image = tf.image.resize_images(distorted_image, [height, width], method=resize_method)"}
{"Type": "API misuse", "Buggy": "distorted_image = apply_with_random_selector(distorted_image, lambda x, method: tf.image.resize_images(x, height, width, method), num_cases=num_resize_cases)", "Fix": "distorted_image = apply_with_random_selector(distorted_image, lambda x, method: tf.image.resize_images(x, [height, width], method=method), num_cases=num_resize_cases)"}
{"Type": "API misuse", "Buggy": "self.d_bn1 = batch_norm(batch_size, name='d_bn1') self.d_bn2 = batch_norm(batch_size, name='d_bn2')", "Fix": "self.d_bn1 = batch_norm(name='d_bn1') self.d_bn2 = batch_norm(name='d_bn2')"}
{"Type": "API misuse", "Buggy": "output = net(init_input, annotation, adj_matrix) test_loss += criterion(output, target).data[0]", "Fix": "output = net(init_input, annotation, adj_matrix) test_loss += criterion(output, target).data.item()"}
{"Type": "API misuse", "Buggy": "scores = np.exp(outputs) / np.exp(outputs).sum(-1) return [{\"label\": self.model.config.id2label[item.argmax()], \"score\": item.max()} for item in scores]", "Fix": "scores = np.exp(outputs) / np.exp(outputs).sum(-1, keepdims=True) return [{\"label\": self.model.config.id2label[item.argmax()], \"score\": item.max().item()} for item in scores]"}
{"Type": "API misuse", "Buggy": "out = model(data, -1)", "Fix": "out = model(data)"}
{"Type": "API misuse", "Buggy": "self.d_loss_real = binary_cross_entropy(self.D, tf.ones_like(self.D)) self.d_loss_fake = binary_cross_entropy(self.D_, tf.zeros_like(self.D_))", "Fix": "self.d_loss_real = tf.nn.sigmoid_cross_entropy_with_logits(self.D, tf.ones_like(self.D)) self.d_loss_fake = tf.nn.sigmoid_cross_entropy_with_logits(self.D_, tf.zeros_like(self.D_))"}
{"Type": "API misuse", "Buggy": "loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits, train_labels_node))", "Fix": "loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=train_labels_node, logits=logits))"}
{"Type": "API misuse", "Buggy": "net = tf.concat(3, [branch1x1, branch7x7, branch7x7dbl, branch_pool])", "Fix": "net = tf.concat([branch1x1, branch7x7, branch7x7dbl, branch_pool], 3)"}
{"Type": "API misuse", "Buggy": "return tf.nn.sampled_softmax_loss(w_t, v, inputs, labels, hps.num_softmax_samples, vsize)", "Fix": "return tf.nn.sampled_softmax_loss(weights=w_t, biases=v, labels=labels, inputs=inputs,num_sampled=hps.num_softmax_samples, num_classes=vsize)"}
{"Type": "API misuse", "Buggy": "mixed = tf.concat(3, [tower_conv, tower_conv1_1, tower_conv2_2])", "Fix": "mixed = tf.concat(axis=3, values=[tower_conv, tower_conv1_1, tower_conv2_2])"}
{"Type": "API misuse", "Buggy": "tf.concat(axis=initial_state, values=1, name=\"initial_state\")", "Fix": "tf.concat(axis=1, values=initial_state, name=\"initial_state\")"}
{"Type": "API misuse", "Buggy": "self.d_loss_real = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=self.D_logits, labels=tf.ones_like(self.D)))", "Fix": "self.d_loss_real = tf.reduce_mean(sigmoid_cross_entropy_with_logits(self.D_logits, tf.ones_like(self.D)))"}
{"Type": "API misuse", "Buggy": "self.d_loss_real = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(self.D_logits,tf.ones_like(self.D)))", "Fix": "self.d_loss_real = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=self.D_logits, labels=tf.ones_like(self.D)))"}
{"Type": "API misuse", "Buggy": "cost = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(prediction,tf.squeeze(y)))", "Fix": "cost = tf.nn.softmax_cross_entropy_with_logits(logits=prediction, labels=y)"}
{"Type": "API misuse", "Buggy": "v_1 = tf.Variable(v) v_2 = tf.Variable(v_1)", "Fix": "v_1 = tf.Variable(v.initialized_value()) v_2 = tf.Variable(v_1.initialized_value())"}
{"Type": "API misuse", "Buggy": "v1 = tf.Variable(v) v2 = tf.Variable(v1)", "Fix": "v1 = tf.Variable(v.initialized_value()) v2 = tf.Variable(v1.initialized_value())"}
{"Type": "API misuse", "Buggy": "x = tf.Variable(35, name='x') y = tf.Variable(x + 5, name='y') model = tf.global_variables_initializer() with tf.Session() as session: session.run(model)", "Fix": "x = tf.Variable(35, name='x') model_x = tf.variables_initializer([x]) y = tf.Variable(x + 5, name='y') model_y = tf.variables_initializer([y]) with tf.Session() as session: session.run(model_x) session.run(model_y)"}
{"Type": "API misuse", "Buggy": "data = np.random.randint(1000, size=10000) x = tf.Variable(data, name='x') y = tf.Variable(5*x*x-3*x+15, name='y')", "Fix": "data = np.random.randint(1000, size=10000) x = tf.Variable(data, name='x') x0 = x.initialized_value() y = tf.Variable(5*x0*x0-3*x0+15, name='y')"}
{"Type": "API misuse", "Buggy": "x = tf.Variable(tf.random_normal([2,2], stddev=0.35)) init_op = tf.initialize_all_variables() sess = tf.Session(init_op)", "Fix": "x = tf.Variable(tf.random_normal([2,2], stddev=0.35)) init_op = tf.initialize_all_variables() sess = tf.Session() sess.run(init_op)"}
{"Type": "API misuse", "Buggy": "train_op = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)", "Fix": "train_op = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy) init_op = tf.initialize_all_variables()"}
{"Type": "API misuse", "Buggy": "tf.losses.softmax_cross_entropy(logits, one_hot_labels)", "Fix": "tf.losses.softmax_cross_entropy(logits = logits, onehot_labels = one_hot_labels)"}
{"Type": "API misuse", "Buggy": "resized_images=tf.image.resize_images(images, 224, 224)", "Fix": "resized_images = tf.image.resize_images(images, (224, 224))"}
{"Type": "API misuse", "Buggy": "xw = tf.matmul(x,w) z = tf.add(xw,b) a = tf.nn.relu(z) yhat = sess.run(a,feed_dict={x:np.random.random([100000,in_size])})", "Fix": "xw = tf.matmul(x,w) z = tf.add(xw,b) a = tf.nn.relu(z) init = tf.global_variables_initializer() with tf.Session() as sess: sess.run(init) yhat = sess.run(a,feed_dict={x:np.random.random([100000,in_size])})"}
{"Type": "API misuse", "Buggy": "some_test = tf.constant(tf.random_normal([2, 2], mean=0.0, stddev=1.0, dtype=tf.float32)) session.run(some_test)", "Fix": "some_test = tf.Variable(tf.random_normal([2, 2], mean=0.0, stddev=1.0, dtype=tf.float32) sess.run(some_test.initializer)"}
{"Type": "API misuse", "Buggy": "images = get_batch()", "Fix": "images = get_batch() tf.train.start_queue_runners(sess=sess)"}
{"Type": "API misuse", "Buggy": "sess.run(init_op, feed_dict=feed_init)", "Fix": "sess.run(init_op, feed_dict=feed_init) tf.train.start_queue_runners(sess)"}
{"Type": "API misuse", "Buggy": "train_data=train_data.eval() train_labels=train_labels.eval()", "Fix": "tf.train.start_queue_runners(sess) train_data=train_data.eval() train_labels=train_labels.eval()"}
{"Type": "API misuse", "Buggy": "res = 2", "Fix": "res = 2 device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"}
{"Type": "API misuse", "Buggy": "logits = torch.rand(2,2)", "Fix": "logits = torch.rand(2,2) device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"}
{"Type": "API misuse", "Buggy": "loss = criterion(output, target) total_loss += loss.data[0]", "Fix": "loss = criterion(output, target) total_loss += loss.item()"}
{"Type": "API misuse", "Buggy": "cb_loss = F.binary_cross_entropy_with_logits(input = logits,target = labels_one_hot)", "Fix": "cb_loss = F.binary_cross_entropy_with_logits(input = logits,target = labels_one_hot, weights = weights)"}
{"Type": "API misuse", "Buggy": "BCLoss = F.binary_cross_entropy_with_logits(input = logits, target = labels)", "Fix": "BCLoss = F.binary_cross_entropy_with_logits(input = logits, target = labels,reduction = \"none\")"}
{"Type": "API misuse", "Buggy": "loss = F.binary_cross_entropy_with_logits(labels, logits)", "Fix": "loss = F.binary_cross_entropy_with_logits(input = logits, target = labels,reduction = \"none\")"}
{"Type": "API misuse", "Buggy": "output = self.bottleneck(mixture_w) skip_connection = torch.tensor([0.0])", "Fix": "output = self.bottleneck(mixture_w) skip_connection = torch.tensor([0.0], device=output.device)"}
{"Type": "API misuse", "Buggy": "spike_record = torch.zeros(1, int(time / dt), n_neurons)", "Fix": "spike_record = torch.zeros(1, int(time / dt), n_neurons, device=device)"}
{"Type": "API misuse", "Buggy": "attention_scores=tf.concat([0, self.attention_values[1:-1]], 0)", "Fix": "attention_scores=tf.concat([[0], tf.shape(self.attention_values)[1:-1]], 0)"}
{"Type": "API misuse", "Buggy": "self.embedded_chars = tf.nn.embedding_lookup(self.W, self.input_x) //returns Tensor object v1 = tf.Variable(tf.zeros([88,77]),dtype=tf.float32) self.embedded_chars = tf.concat(1,[self.embedded_chars,v1])", "Fix": "self.embedded_chars = tf.nn.embedding_lookup(self.W, self.input_x) //returns Tensor object v1 = tf.Variable(tf.zeros([88,77]),dtype=tf.float32) self.embedded_chars = tf.concat([self.embedded_chars,v1], 1)"}
{"Type": "API misuse", "Buggy": "random_tensor = torch.rand(input.size(), generator=generator)", "Fix": "random_tensor = torch.rand(input.size(), generator=generator, device=input.device)"}
{"Type": "API misuse", "Buggy": "loss += tf.nn.softmax_cross_entropy_with_logits(self.lstm_outputs[idx], tf.squeeze(true_output))", "Fix": "loss += tf.nn.sparse_softmax_cross_entropy_with_logits(self.lstm_outputs[idx], tf.squeeze(true_output))"}
{"Type": "API misuse", "Buggy": "net = tf.concat(3, [tower_conv_1, tower_conv1_1, tower_conv2_2, tower_pool])", "Fix": "net = tf.concat(axis=3, values=[tower_conv_1, tower_conv1_1, tower_conv2_2, tower_pool])"}
{"Type": "API misuse", "Buggy": "import tensorflow as tf with tf.variable_scope('test') as scope:  x = tf.get_variable('x', shape = [3, 4], initializer = tf.constant_initializer([0,1,2])) sess = tf.InteractiveSession() print(sess.run(x))", "Fix": "import tensorflow as tf with tf.variable_scope('test') as scope: x = tf.get_variable('x', shape = [3, 4], initializer = tf.constant_initializer([0,1,2])) sess = tf.InteractiveSession() sess.run(tf.global_variables_initializer()) print(sess.run(x))"}
{"Type": "API misuse", "Buggy": "cb_loss = F.binary_cross_entropy_with_logits(labels_one_hot, logits)", "Fix": "cb_loss = F.binary_cross_entropy_with_logits(input = logits,target = labels_one_hot, weights = weights)"}
{"Type": "API misuse", "Buggy": "logits = torch.rand(2,2) pred = F.softmax(logits, dim=1) pred1 = F.log_softmax(logits)", "Fix": "logits = torch.rand(2,2) pred = F.softmax(logits, dim=1) pred1 = F.log_softmax(logits, dim=1)"}
{"Type": "Non-General", "Buggy": "if self.output_attentions: attentions = tuple(t.permute(2, 3, 0, 1).contiguous() for t in attentions)", "Fix": "if self.output_attentions: if target_mapping is not None: attentions = tuple(tuple(att_stream.permute(2, 3, 0, 1).contiguous() for att_stream in t) for t in attentions) else: attentions = tuple(t.permute(2, 3, 0, 1).contiguous() for t in attentions)"}
{"Type": "Non-General", "Buggy": "def __len__(self): raise NotImplementedError", "Fix": "def __len__(self): raise NotImplementedError def duplicate(self): return deepcopy(self)"}
{"Type": "Non-General", "Buggy": "self.total_batches = self.nb_training_batches + self.nb_val_batches", "Fix": "is_val_epoch = (self.current_epoch + 1) % self.check_val_every_n_epoch == 0 val_checks_per_epoch = self.nb_training_batches // self.val_check_batch val_checks_per_epoch = val_checks_per_epoch if is_val_epoch else 0 self.total_batches = (self.nb_training_batches + self.nb_val_batches * val_checks_per_epoch)"}
{"Type": "Non-General", "Buggy": "y = np.random.random((32, 10, 64))", "Fix": "y = np.random.random((32, 10, 64)) def test_from_config(self): with self.test_session(config=self.config) as sess: K.set_session(sess)"}
{"Type": "Non-General", "Buggy": "decoder_input_fn_infer = decoders.DynamicDecoderInputs(initial_inputs=initial_input, make_input_fn=lambda x: tf.nn.embedding_lookup(target_embedding, x.predictions))", "Fix": "decoder_input_fn_infer = decoders.DynamicDecoderInputs(initial_inputs=initial_input, make_input_fn=make_input_fn)"}
{"Type": "Non-General", "Buggy": "for i in xrange(FLAGS.num_gpus): with tf.device('/gpu:%d' % i): with tf.name_scope('%s_%d' % (cifar10.TOWER_NAME, i)) as scope: loss = tower_loss(scope) tf.get_variable_scope().reuse_variables()", "Fix": "with tf.variable_scope(tf.get_variable_scope()): for i in xrange(FLAGS.num_gpus): with tf.device('/gpu:%d' % i): with tf.name_scope('%s_%d' % (cifar10.TOWER_NAME, i)) as scope: loss = tower_loss(scope) tf.get_variable_scope().reuse_variables()"}
{"Type": "Non-General", "Buggy": "self.build_model()", "Fix": "self.is_grayscale = (self.c_dim == 1) self.build_model()"}
{"Type": "Non-General", "Buggy": "opt = parser.parse_args() print(opt)", "Fix": "logging.basicConfig(level=logging.INFO) logger = logging.getLogger(__name__) opt = parser.parse_args() logger.info(opt)"}
{"Type": "Non-General", "Buggy": "network_size = int(network_size)", "Fix": "network_size = int(network_size) pytorch_version = torch.__version__ pytorch_version = pytorch_version.rsplit('.', 2)[0]"}
{"Type": "Non-General", "Buggy": "bboxlist = bboxlists[i] keep = nms(bboxlist, 0.3) bboxlist = bboxlist[keep, :] bboxlist = [x for x in bboxlist if x[-1] > 0.5]", "Fix": "bboxlist = bboxlists[i] bboxlist = self._filter_bboxes(bboxlist)"}
{"Type": "Non-General", "Buggy": "self.model_name = model_name self.num_classes = num_classes self.pretrained = pretrained", "Fix": "self.model_name = model_name self.num_classes = num_classes self.pretrained = pretrained self.catch_output_size_exception = catch_output_size_exception"}
{"Type": "Non-General", "Buggy": "import pandas as pd self.result = pd.DataFrame(self.result) self.result.columns = self.result.iloc[0] self.result = self.result.drop(0)", "Fix": "import pandas as pd cols = self.result[0] self.result = pd.DataFrame(self.result[1:]) self.result.columns = cols"}
{"Type": "Non-General", "Buggy": "outputs = model(input_ids, start_positions=start_positions, end_positions=end_positions) loss, start_scores, end_scores = outputs[:2]", "Fix": "outputs = model(input_ids, start_positions=start_positions, end_positions=end_positions) loss, start_scores, end_scores = outputs[:3]"}
{"Type": "Non-General", "Buggy": "model = cls(*cls_args, **cls_kwargs)", "Fix": "if len(cls_spec.args) <= 1 and not cls_spec.kwonlyargs: cls_args, cls_kwargs = [], {} model = cls(*cls_args, **cls_kwargs)"}
{"Type": "Non-General", "Buggy": "opt_op = optimizer.minimize(loss, var_list=tf.trainable_variables()) opt_slots_init = tf.variables_initializer([optimizer.get_slot(v, name) for v in tf.trainable_variables() for name in optimizer.get_slot_names()])", "Fix": "with tf.variable_scope(optimizer.get_name()) as scope: opt_op = optimizer.minimize(loss, var_list=tf.trainable_variables()) opt_slots_init = tf.variables_initializer(scope.get_collection(tf.GraphKeys.GLOBAL_VARIABLES))"}
{"Type": "Non-General", "Buggy": "self.loader.reset_batch_pointer(split_idx) target = np.zeros([self.batch_size, self.seq_length, self.word_vocab_size])", "Fix": "self.loader.reset_batch_pointer(split_idx) target = np.zeros([self.batch_size, self.seq_length])"}
{"Type": "Non-General", "Buggy": "def call(self, inputs, mask=None): input_shape = K.int_shape(inputs) if input_shape[0]: def step(x, _): output = self.layer.call(x) return output, []", "Fix": "def call(self, inputs, training=None, mask=None): kwargs = {} func_args = inspect.getargspec(self.layer.call).args if 'training' in func_args: kwargs['training'] = training input_shape = K.int_shape(inputs) if input_shape[0]: def step(x, _): output = self.layer.call(x, **kwargs) return output, []"}
{"Type": "Non-General", "Buggy": "input_shape = _obtain_input_shape(input_shape, default_size=default_size, min_size=32, data_format=K.image_data_format(), require_flatten=include_top or weights, weights=weights)", "Fix": "input_shape = _obtain_input_shape(input_shape, default_size=default_size, min_size=32, data_format=K.image_data_format(), require_flatten=include_top, weights=weights)"}
{"Type": "Non-General", "Buggy": "if self.return_sequences: y_rev = K.reverse(y_rev, 1) if self.merge_mode == 'concat': return K.concatenate([y, y_rev]) elif self.merge_mode == 'sum': return y + y_rev elif self.merge_mode == 'ave': return (y + y_rev) / 2 elif self.merge_mode == 'mul': return y * y_rev elif self.merge_mode is None: return [y, y_rev]", "Fix": "if self.return_sequences: y_rev = K.reverse(y_rev, 1) if self.merge_mode == 'concat': output = K.concatenate([y, y_rev]) elif self.merge_mode == 'sum': output = y + y_rev elif self.merge_mode == 'ave': output = (y + y_rev) / 2 elif self.merge_mode == 'mul': output = y * y_rev elif self.merge_mode is None: output = [y, y_rev]"}
{"Type": "Non-General", "Buggy": "def call(self, inputs): return self._merge_function(inputs)", "Fix": "def call(self, inputs): return self._merge_function(inputs) def compute_output_shape(self, input_shape): return input_shape[0]"}
{"Type": "Non-General", "Buggy": "def build(self, input_shape): assert len(input_shape) == 5 self.input_spec = [InputSpec(shape=input_shape)]", "Fix": "def build(self, input_shape): assert len(input_shape) == 5"}
{"Type": "Non-General", "Buggy": "for padding in _convolution_paddings: for out_padding in [None, (1, 1)]: for strides in [(1, 1), (2, 2)]: if padding == 'same' and strides != (1, 1): continue", "Fix": "for padding in _convolution_paddings: for out_padding in [None, (0, 0), (1, 1)]: for strides in [(1, 1), (2, 2)]: if padding == 'same' and strides != (1, 1): continue if strides == (1, 1) and out_padding == (1, 1): continue"}
{"Type": "Non-General", "Buggy": "num_words = min(MAX_NUM_WORDS, len(word_index)) + 1 embedding_matrix = np.zeros((num_words, EMBEDDING_DIM)) for word, i in word_index.items(): if i > MAX_NUM_WORDS: continue", "Fix": "num_words = min(MAX_NUM_WORDS, len(word_index) + 1) embedding_matrix = np.zeros((num_words, EMBEDDING_DIM)) for word, i in word_index.items(): if i >= MAX_NUM_WORDS: continue"}
{"Type": "Non-General", "Buggy": "if padding == 'same': th_avg_pool_mode = 'average_inc_pad' elif padding == 'valid': th_avg_pool_mode = 'average_exc_pad' pool_out = pool.pool_2d(x, ws=pool_size, stride=strides, ignore_border=True, pad=pad, mode=th_avg_pool_mode)", "Fix": "pool_out = pool.pool_2d(x, ws=pool_size, stride=strides, ignore_border=True, pad=pad, mode='average_exc_pad')"}
{"Type": "Non-General", "Buggy": "if self._cells[0].output_size != inputs.get_shape().as_list()[0] and self._residual_combiner == \"add\": inputs = tf.contrib.layers.fully_connected(inputs=inputs, num_outputs=self._cells[0].output_size, activation_fn=None, scope=\"input_transform\")", "Fix": "if self._cells[0].output_size != inputs.get_shape().as_list()[1] and self._residual_combiner == \"add\": inputs = tf.contrib.layers.fully_connected(inputs=inputs, num_outputs=self._cells[0].output_size, activation_fn=None, scope=\"input_transform\")"}
{"Type": "Non-General", "Buggy": "with tf.variable_scope(self._architecture, reuse=self._reuse_weights) as var_scope: _, activations = self._resnet_model(preprocessed_inputs, num_classes=None, is_training=False, global_pool=False, output_stride=self._first_stage_features_stride, scope=var_scope)", "Fix": "with tf.variable_scope(self._architecture, reuse=self._reuse_weights) as var_scope: _, activations = self._resnet_model(preprocessed_inputs, num_classes=None, is_training=False, global_pool=False, output_stride=self._first_stage_features_stride, spatial_squeeze=False, scope=var_scope)"}
{"Type": "Non-General", "Buggy": "def empty_field(self): return MultiLabelField([], self._label_namespace, skip_indexing=True)", "Fix": "def empty_field(self): return MultiLabelField([], self._label_namespace, skip_indexing=True, num_labels=self._num_labels)"}
{"Type": "Non-General", "Buggy": "def apply_mv_norm(features): mean, invstddev = calc_mean_invstddev(features) res = (features - mean) * invstddev return res", "Fix": "def apply_mv_norm(features): if features.size(0) < 2: return features mean, invstddev = calc_mean_invstddev(features) res = (features - mean) * invstddev return res"}
{"Type": "Non-General", "Buggy": "reverse_sampler = np.argsort(sampler) preds[0],preds[1] = preds[0][reverse_sampler],preds[1][reverse_sampler]", "Fix": "reverse_sampler = np.argsort(sampler) preds = [p[reverse_sampler] for p in preds]"}
{"Type": "Non-General", "Buggy": "def _default_positive_minimum_factory(): try: value = _default(_Values.POSITIVE_MINIMUM) if value is not None: value = float(value) except ValueError: raise TypeError(\"Config cannot set the positive_minimum value with non float type.\") return value", "Fix": "def _default_positive_minimum_factory(): value = _default(_Values.POSITIVE_MINIMUM) try: value = float(value) except ValueError: raise TypeError(\"Config cannot set the positive_minimum value with non float type.\") return value"}
{"Type": "Non-General", "Buggy": "def get_output(self, train=False): return 1", "Fix": "def get_output(self, train=False): X = self.get_input(train) return X * T.shape_padright(T.any((1. - T.eq(X, self.mask_value)), axis=-1))"}
{"Type": "Non-General", "Buggy": "dropout_input_keep_prob=self.params[\"rnn_cell.dropout_input_keep_prob\"], dropout_output_keep_prob=self.params[\"rnn_cell.dropout_output_keep_prob\"])", "Fix": "dropout_input_keep_prob=(self.params[\"rnn_cell.dropout_input_keep_prob\"] if enable_dropout else 1.0), dropout_output_keep_prob=(self.params[\"rnn_cell.dropout_output_keep_prob\"] if enable_dropout else 1.0))"}
{"Type": "Non-General", "Buggy": "cdna_kerns = tf.tile(cdna_kerns, [1, 1, 1, color_channels, 1]) cdna_kerns = tf.split(axis=0, num_or_size_splits=batch_size, value=cdna_kerns) prev_images = tf.split(axis=0, num_or_size_splits=batch_size, value=prev_image)", "Fix": "cdna_kerns = tf.transpose(cdna_kerns, [1, 2, 0, 4, 3]) cdna_kerns = tf.reshape(cdna_kerns, [DNA_KERN_SIZE, DNA_KERN_SIZE, batch_size, num_masks]) prev_image = tf.transpose(prev_image, [3, 1, 2, 0])"}
{"Type": "Non-General", "Buggy": "labels = 2 - tf.to_int32(labels) predictions = 3 - tf.to_int32(predictions) * 2", "Fix": "labels = (2 - tf.to_int32(labels)) - 1 predictions = (3 - tf.to_int32(predictions) * 2) - 1"}
{"Type": "Non-General", "Buggy": "if edge_attr is not None: if edge_attr.dim() == 1: edge_attr = edge_attr.view(-1, 1) assert self.lin_edge is not None", "Fix": "if edge_attr is not None and self.lin_edge is not None: if edge_attr.dim() == 1: edge_attr = edge_attr.view(-1, 1)"}
{"Type": "Non-General", "Buggy": "def format_for_prediction(self, report): return generate_prediction(report)", "Fix": "def format_for_prediction(self, report): if self.is_multilabel: return generate_multilabel_prediction(report) else: return generate_binary_prediction(report)"}
{"Type": "Non-General", "Buggy": "combined_report = None num_batches_for_this_update = 1 for idx, batch in enumerate(self.train_loader): if (idx + 1) % self.training_config.update_frequency == 0: combined_report = None num_batches_for_this_update = min(self.training_config.update_frequency, num_remaining_batches) self._start_update()", "Fix": "should_start_update = True for idx, batch in enumerate(self.train_loader): if should_start_update: combined_report = None self._start_update() num_batches_for_this_update = min(self.training_config.update_frequency, num_remaining_batches) should_start_update = False self.current_iteration += 1"}
{"Type": "Non-General", "Buggy": "def __init__(self, max_features=None): self.max_features = max_features", "Fix": "def __init__(self, max_features=None): self.max_features = max_features if self.max_features: patch_dim = math.ceil(math.sqrt(self.max_features)) self.img_h = patch_dim self.img_w = patch_dim"}
{"Type": "Non-General", "Buggy": "src = self.args.zip_file self.checksum(self.args.zip_file, self.POSSIBLE_CHECKSUMS)", "Fix": "if not bypass_checksum: self.checksum(self.args.zip_file, self.POSSIBLE_CHECKSUMS) src = self.args.zip_file"}
{"Type": "Non-General", "Buggy": "def fetch_requirements(): with open(\"requirements.txt\") as f: reqs = f.read() return reqs", "Fix": "def fetch_requirements(): requirements_file = \"requirements.txt\" if platform.system() == \"Windows\": DEPENDENCY_LINKS.append(\"https://download.pytorch.org/whl/torch_stable.html\") with open(requirements_file) as f: reqs = f.read() return reqs"}
{"Type": "Non-General", "Buggy": "self.base = UnimodalBase(self.config) num_features = 100 self._is_direct_features_input = self.config.direct_features_input if not self._is_direct_features_input: num_features = self.config.modal_encoder.params.num_output_features", "Fix": "self.base = UnimodalBase(self.config) self._is_direct_features_input = self.config.direct_features_input num_features = self.config.modal_encoder.params.num_output_features"}
{"Type": "Non-General", "Buggy": "self.logger = logging.getLogger(__name__) self._file_only_logger = logging.getLogger(__name__)", "Fix": "if not name: name = __name__ self.logger = logging.getLogger(name) self._file_only_logger = logging.getLogger(name)"}
{"Type": "Non-General", "Buggy": "def build(self): encoders = self._build_encoders(self.config) self.text_encoder, self.modal_encoder = encoders[0], encoders[1] self._encoder_config = self.text_encoder.config", "Fix": "def build(self): encoders = self._build_encoders(self.config) self.text_encoder, self.modal_encoder = encoders[0], encoders[1] self._encoder_config = None if self.text_encoder: self._encoder_config = self.text_encoder.config"}
{"Type": "Non-General", "Buggy": "def _build_encoders(self, config): return (build_text_encoder(config.text_encoder), self._build_modal_encoder(config.modal_encoder),)", "Fix": "def _build_encoders(self, config): text_encoder = None if config.get(\"text_encoder\", None): text_encoder = build_text_encoder(config.text_encoder) modal_encoder = None if config.get(\"modal_encoder\", None): modal_encoder = self._build_modal_encoder(config.modal_encoder) return (text_encoder, modal_encoder)"}
{"Type": "Non-General", "Buggy": "with open(\"./list\", \"r\") as f: lines = f.readlines() for line in lines: exclude[line.strip(\"\\n\").split(os.path.sep)[-1].split(\".\")[0]] = 1", "Fix": "if os.path.exists(self.args.exclude_list): with open(self.args.exclude_list, \"r\") as f: lines = f.readlines() for line in lines: exclude[line.strip(\"\\n\").split(os.path.sep)[-1].split(\".\")[0]] = 1"}
{"Type": "Non-General", "Buggy": "def build_trainer(args, *rest, **kwargs): configuration = Configuration(args.config) configuration.override_with_cmd_config(args.config_override) configuration.override_with_cmd_opts(args.opts) configuration.update_with_args(args) configuration.freeze()", "Fix": "def build_trainer(args, *rest, **kwargs): configuration = Configuration(args) configuration.freeze()"}
{"Type": "Non-General", "Buggy": "config_path = os.path.abspath(config_path) configuration = Configuration(config_path)", "Fix": "config_path = os.path.abspath(config_path) args = dummy_args() args.opts.append(\"config={}\".format(config_path)) configuration = Configuration(args)"}
{"Type": "Non-General", "Buggy": "if self.k is not None: num_k = self.k", "Fix": "num_k = self.determine_k(label_counts[1], len(reference), embeddings_come_from_same_source)"}
{"Type": "Non-General", "Buggy": "index = faiss.IndexFlatL2(d) if faiss.get_num_gpus() > 0: index = faiss.index_cpu_to_all_gpus(index) index.add(reference_embeddings) distances, indices = index.search(test_embeddings, k + 1)", "Fix": "cpu_index = faiss.IndexFlatL2(d) distances, indices = try_gpu(cpu_index, reference_embeddings, test_embeddings, k)"}
{"Type": "Non-General", "Buggy": "logits = self.forward(input_ids=input_ids, token_type_ids=input_type_ids, attention_mask=input_mask,) mlm_loss = self.mlm_loss(log_probs=logits[0], labels=output_ids, output_mask=output_mask) if self.only_mlm_loss: loss = mlm_loss else: nsp_loss = self.nsp_loss(logits=logits[1], labels=labels) loss = self.agg_loss(loss_1=mlm_loss, loss_2=nsp_loss)", "Fix": "forward_outputs = self.forward(input_ids=input_ids, token_type_ids=input_type_ids, attention_mask=input_mask) mlm_log_probs, nsp_logits = self._parse_forward_outputs(forward_outputs) _, _, loss = self._compute_losses(mlm_log_probs, nsp_logits, output_ids, output_mask, labels)"}
{"Type": "Non-General", "Buggy": "self.setup_optimization(cfg.optim)", "Fix": "self.training_perplexity = Perplexity(dist_sync_on_step=True) self.validation_perplexity = Perplexity(compute_on_step=False) self.setup_optimization(cfg.optim)"}
{"Type": "Non-General", "Buggy": "if self.extension in (\"jpg\", \"JPG\", \"jpeg\", \"JPEG\"): image = np.array(imread(uri=image_path,grayscale=self.grayscale,expand_dims=self.expand_dims,exifrotate=not self.clear_exif)) else: image = np.array(imread(uri=image_path,grayscale=self.grayscale,expand_dims=self.expand_dims))", "Fix": "_, extension = os.path.splitext(image_path) kwargs = {\"grayscale\": self.grayscale, \"expand_dims\": self.expand_dims} if extension.lower() in {\"jpg\", \"jpeg\"}: kwargs[\"exifrotate\"] = not self.clear_exif image = np.array(imread(uri=image_path, **kwargs))"}
{"Type": "Non-General", "Buggy": "def process_all(self, pool: Pool): images = [*self.in_dir.glob(f\"**/*.{self.extension}\")]", "Fix": "def process_all(self, pool: Pool): images: List[Path] = [] for root, dirs, files in os.walk(self.in_dir): root = Path(root) images.extend([root / filename for filename in files if has_image_extension(filename)])"}
{"Type": "Non-General", "Buggy": "def prepare_df_from_dirs(in_dir, tag_column_name): if not in_dir.endswith(\"/\"): in_dir = f\"{in_dir}/\" dataset = create_dataset(f\"{in_dir}/**\", process_fn=lambda x: x.replace(f\"{in_dir}\", \"\")) df = create_dataframe(dataset, columns=[tag_column_name, \"filepath\"])", "Fix": "def prepare_df_from_dirs(in_dirs, tag_column_name): dfs = [] splitted_dirs = in_dirs.strip(',').split(',')"}
{"Type": "Non-General", "Buggy": "if self.batch_granularity: self.fieldnames = ['epoch', 'batch', 'size', 'time', 'lr'] else: self.fieldnames = ['epoch', 'time', 'lr']", "Fix": "if self.batch_granularity: self.fieldnames = ['epoch', 'batch', 'size', 'time'] else: self.fieldnames = ['epoch', 'time'] if len(self.model.optimizer.param_groups) > 1: self.fieldnames += [f'lr_group_{i}' for i in range(len(self.model.optimizer.param_groups))] else: self.fieldnames += ['lr']"}
{"Type": "Non-General", "Buggy": "def _get_current_learning_rates(self): learning_rates = [param_group['lr'] for param_group in self.model.optimizer.param_groups] return learning_rates[0] if len(learning_rates) == 1 else learning_rates", "Fix": "def _get_current_learning_rates(self): if len(self.model.optimizer.param_groups) > 1: learning_rates = {f'lr_group_{i}': param_group['lr'] for i, param_group in enumerate(self.model.optimizer.param_groups)} else: learning_rates = {'lr': self.model.optimizer.param_groups[0]['lr']} return learning_rates"}
{"Type": "Non-General", "Buggy": "lr = self._get_current_learning_rates() if isinstance(lr, (list, )): self.writer.add_scalars('lr', {str(i): v for i, v in enumerate(lr)}, epoch_number) else: self.writer.add_scalars('lr', {'lr': lr}, epoch_number)", "Fix": "lr = self._get_current_learning_rates() self.writer.add_scalars('lr', lr, epoch_number)"}
{"Type": "Non-General", "Buggy": "calls.append(call('lr', {'lr': self.lr}, h['epoch']))", "Fix": "if len(lrs) == 1: calls.append(call('lr', {'lr': self.lr}, h['epoch'])) else: calls.append(call('lr', {f'lr_group_{i}': lr for i, lr in enumerate(lrs)}, h['epoch']))"}
{"Type": "Non-General", "Buggy": "nb_subplot_cols = 3 gs = mpl.gridspec.GridSpec(1, nb_subplot_cols)", "Fix": "nb_subplot_cols = 3 if is_debug: nb_subplot_cols += dataset_info.include_ir + dataset_info.include_depth + dataset_info.include_ndvi gs = mpl.gridspec.GridSpec(1, nb_subplot_cols)"}
{"Type": "Non-General", "Buggy": "a = fig.add_subplot(gs[subplot_index]) a.axes.get_xaxis().set_visible(False) a.axes.get_yaxis().set_visible(False) a.imshow(im.astype(np.uint8))", "Fix": "a = fig.add_subplot(gs[subplot_index]) a.axes.get_xaxis().set_visible(False) a.axes.get_yaxis().set_visible(False) if is_rgb: a.imshow(im.astype(np.uint8)) else: a.imshow(im, cmap='gray', vmin=0, vmax=255)"}
{"Type": "Non-General", "Buggy": "predictions_path = join(run_path, 'predictions', '{}.pdf'.format(sample_index))", "Fix": "file_name = '{}_debug.pdf' if is_debug else '{}.pdf' file_name = file_name.format(sample_index) predictions_path = join(run_path, 'predictions', file_name)"}
{"Type": "Non-General", "Buggy": "def make_split_generator(dataset_info, split, batch_size=32, shuffle=False, augment=False, scale=False, eval_mode=False): path = dataset_info.dataset_path split_path = join(path, split)", "Fix": "def make_split_generator(dataset_info, split, batch_size=32, shuffle=False,reset_interval=None, augment=False, scale=False,eval_mode=False): path = dataset_info.dataset_path split_path = join(path, split)"}
{"Type": "Non-General", "Buggy": "a = 0 if anchors_mask is not None: prune_anchor_fn = lambda _: np.where(anchors_mask)[0] else: prune_anchor_fn = None", "Fix": "a = 0"}
{"Type": "Non-General", "Buggy": "unmix = model.OpenUnmix(input_mean=scaler_mean,input_scale=scaler_std,nb_bins=args.nfft // 2 + 1,nb_channels=args.nb_channels,hidden_size=args.hidden_size,max_bin=max_bin,).to(device)", "Fix": "if args.model: unmix = utils.load_target_models(args.target, model_str_or_path=args.model, device=device, pretrained=True)[args.target] unmix = unmix.to(device) else: unmix = model.OpenUnmix(input_mean=scaler_mean,input_scale=scaler_std,nb_bins=args.nfft // 2 + 1,nb_channels=args.nb_channels,hidden_size=args.hidden_size,max_bin=max_bin,).to(device)"}
{"Type": "Non-General", "Buggy": "if self.reduction_method == 'random_forrest': pass if self.reduction_method == 'extra_trees': pass", "Fix": "if self.reduction_method == 'forrest': self = forrest(self) if self.reduction_method == 'trees': self = trees(self)"}
{"Type": "Non-General", "Buggy": "loss.backward()", "Fix": "if IS_AMP_AVAILABLE and hasattr(self.optimizer, '_amp_stash'): with amp.scale_loss(loss, self.optimizer) as scaled_loss: scaled_loss.backward() else: loss.backward()"}
{"Type": "Non-General", "Buggy": "if self.separable: lh, hl, hh = torch.unbind(h, dim=2) filts = (self.g0_col, self.g1_col, self.g0_row, self.g1_row) ll = lowlevel.sfb2d(ll, lh, hl, hh, filts, mode=self.mode) else: c = torch.cat((ll[:,:,None], h), dim=2) ll = lowlevel.sfb2d_nonsep(c, self.h, mode=self.mode)", "Fix": "ll = lowlevel.SFB2D.apply(ll, h, self.g0_col, self.g1_col, self.g0_row, self.g1_row, mode)"}
{"Type": "Non-General", "Buggy": "max_length = int(least_used_mem[0, 0].data.cpu().numpy())", "Fix": "max_length = int(least_used_mem[0, 0].data.cpu().numpy()) if not self.mem_limit_reached else (m-1)"}
{"Type": "Non-General", "Buggy": "if chx is None: chx = cuda(T.zeros(self.num_layers, batch_size, self.output_size), gpu_id=self.gpu_id) if self.rnn_type.lower() == 'lstm': chx = (chx, chx)", "Fix": "if chx is None: chx = cuda(T.zeros(batch_size, self.output_size), gpu_id=self.gpu_id) if self.rnn_type.lower() == 'lstm': chx = [ [ (chx.clone(), chx.clone()) for h in range(self.num_hidden_layers) ] for l in range(self.num_layers) ] else: chx = [ [ chx.clone() for h in range(self.num_hidden_layers) ] for l in range(self.num_layers) ]"}