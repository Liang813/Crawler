Shape mismatch ||| return np.squeeze(self.model.predict(x, **kwargs)) ||| return np.squeeze(self.model.predict(x, **kwargs), axis=-1)
Shape mismatch ||| crossEntropy = -torch.log(torch.gather(inp, 1, target.view(-1, 1))) ||| crossEntropy = -torch.log(torch.gather(inp, 1, target.view(-1, 1)).squeeze(1))
Shape mismatch ||| x = tf.cast(tf.squeeze(x), dtype=tf.float32) p = tf.cast(tf.squeeze(p), dtype=tf.float32) ||| x = tf.cast(x, dtype=tf.float32) p = tf.cast(p, dtype=tf.float32)
Shape mismatch ||| discrim_A_grads = tf.gradients(discrim_A_interp, [A_interp]) ||| discrim_A_grads = tf.gradients(discrim_A_interp, [A_interp]) discrim_A_grads = tf.squeeze(discrim_A_grads)
Shape mismatch ||| outputs = self.head(lstm_out) mu = outputs[:, :self._action_size] log_std = outputs[:, self._action_size:-1] v = outputs[:, -1].squeeze(-1) ||| outputs = self.head(lstm_out.view(T * B, -1)) mu = outputs[:, :self._action_size] log_std = outputs[:, self._action_size:-1] v = outputs[:, -1]
Shape mismatch ||| softmax_output = self.crit(pred_hid.view(-1, pred_hid.size(-1)), labels) ||| softmax_output = self.crit(pred_hid, labels)
Shape mismatch ||| input_shape = input_ids.shape ||| input_shape = tf.shape(input_ids)
Shape mismatch ||| output_shape = [input_layer.shape[0], out_rows, out_cols, depth] y = tf.nn.conv2d_transpose(input_layer, params, output_shape, stride, edges) ||| output_shape = [tf.shape(input_layer.tensor)[0], out_rows, out_cols,depth] y = tf.nn.conv2d_transpose(input_layer, params, output_shape, stride, edges) y.set_shape([input_layer.shape[0], out_rows, out_cols, depth])
Shape mismatch ||| self.masked_v = tf.matmul(tf.squeeze(self.caps2), tf.reshape(self.Y, (-1, 10, 1)), transpose_a=True) ||| self.masked_v = tf.multply(tf.squeeze(self.caps2), tf.reshape(self.Y, (-1, 10, 1)))
Shape mismatch ||| ybar = model(xadv) yshape = ybar.get_shape().as_list() n, ydim = yshape[0], yshape[1] ||| ybar = model(xadv) ydim = ybar.get_shape().as_list()[1] n = tf.shape(ybar)[0]
Shape mismatch ||| edges = np.asarray(buckets[i, j]) ||| edges = np.array(buckets[i, j], dtype=np.int64).reshape((-1, 3))
Shape mismatch ||| target = Variable(torch.arange(1, 11)) ||| target = Variable(torch.arange(1, 11)) target = target.view(1, -1)
Shape mismatch ||| edges = np.asarray(buckets[i, j]) ||| edges = np.array(buckets[i, j], dtype=np.int64).reshape((-1, 2))
Shape mismatch ||| x = op.inputs[0] d0 = x.get_shape().as_list()[0] d = tf.convert_to_tensor([d0], dtype=tf.int32) ||| x = op.inputs[0] d = tf.shape(x) d = tf.reshape(d[0], [1])
Shape mismatch ||| box_regression = permute_and_flatten(box_regression, N, A, 4, H, W) box_regression = box_regression.reshape(N, -1, 4) ||| box_regression = permute_and_flatten(box_regression, N, A, 4, H, W)
Shape mismatch ||| return np.expand_dims(in_[..., 1:-1, 1:-1], -3) ||| return in_[..., 1:-1, 1:-1]
Shape mismatch ||| return torch.unsqueeze(in_[..., 1:-1, 1:-1], -3) ||| return in_[..., 1:-1, 1:-1]
Shape mismatch ||| y = tf.placeholder(tf.float32, [None,n_classes]) ||| y = tf.placeholder(tf.float32, [None]) y_one_hot = tf.one_hot( y , 10 )
Shape mismatch ||| y = tf.placeholder(tf.float32, [None, n_classes]) ||| y = tf.placeholder(tf.float32, [None])
Shape mismatch ||| normal_dist = tf.truncated_normal(w_shape, stddev=stddev, name='normaldist') ||| normal_dist = tf.truncated_normal(w_shape, stddev=stddev, name='normaldist') normal_dist.set_shape([input_data.get_shape()[1], labels.get_shape()[1]])
Shape mismatch ||| data = np.array([0.1, 0.2]) x = tf.placeholder("float", shape=[2]) T1 = tf.Variable(tf.ones([2,2])) l1 = tf.matmul(T1, x) ||| data = np.array([[0.1], [0.2]]) x = tf.placeholder(tf.float32, shape=[2, 1]) T1 = tf.Variable(tf.ones([2, 2])) l1 = tf.matmul(T1, x)
Shape mismatch ||| data = np.array([0.1, 0.2]) x = tf.placeholder("float", shape=[2]) T1 = tf.Variable(tf.ones([2,2])) l1 = tf.matmul(T1, x) ||| data = np.array([0.1, 0.2]) x = tf.placeholder(tf.float32, shape=[2]) T1 = tf.Variable(tf.ones([2, 2])) l1 = tf.matmul(T1, tf.expand_dims(x, 1))
Shape mismatch ||| vertex_textures = meshes.textures.verts_rgb_padded() ||| vertex_textures = meshes.textures.verts_rgb_padded().reshape(-1, 3)
Shape mismatch ||| v_ts = ms.tex.verts_rgb_padded() ||| v_ts = ms.tex.verts_rgb_padded().view(-1, 3)
Shape mismatch ||| tf.register_gradient(Mygrad) def Mygrad(op,grad): return cust_grad*grad ||| tf.register_gradient(Mygrad) def Mygrad(op,grad): return tf.matmul(tf.reshape(cust_grad,[calculated_shape]),tf.reshape(grad,expeced_shape))
Shape mismatch ||| data_set = tf.data.Dataset.from_tensor_slices((data.values, target)) ||| data_set = tf.data.Dataset.from_tensor_slices((data.values, target)).batch(8)
Shape mismatch ||| vs = ms.tex.verts_rgb_padded() ||| vs = ms.tex.verts_rgb_padded().view(-1, 3)
Shape mismatch ||| prop_1 = np.array((a,b)) prop_2 = np.array((x,y)) results = np.dot(prop_1, prop_2) ||| prop_1 = np.array((a,b)) prop_2 = np.array((x,y)) results = np.dot(prop_1.reshape(-1), prop_2.reshape(-1))
Shape mismatch ||| a1 = np.array((a,b)) a2 = np.array((x,y)) results = np.dot(a1, a2) ||| a1 = np.array((a,b)) a2 = np.array((x,y)) results = np.dot(a1.T, a2)
Shape mismatch ||| a1 = np.array((a,b)) a2 = np.array((x,y)) results = np.dot(a1, a2) ||| a1 = np.array((a,b)) a2 = np.array((x,y)) results = np.einsum('ij,ij->j', a1, a2).item()
Shape mismatch ||| def func(x,y,z): return x+y+z y = linspace(0,1,100) z = linspace(0,1,100) x0 = zeros((y.size,z.size)) + 0.5 yz = (y[:,newaxis],z[newaxis,:]) x, info, iterations, message = fsolve(func,x0,yz) ||| def func(x, y, z): x = x.reshape(y.size, z.size) return (x + y + z).ravel() y = linspace(0,1,100) z = linspace(0,1,100) x0 = zeros((y.size,z.size)) + 0.5 yz = (y[:,newaxis],z[newaxis,:]) sol, info, iterations, message  = fsolve(func, x0.ravel(), args=yz, full_output=True) x = sol.reshape(y.size, z.size)
Shape mismatch ||| import numpy as np a = np.array([1,2,3,4]) b = np.array([10,20,30,40]) from sklearn.linear_model import LinearRegression regr = LinearRegression() regr.fit(a,b) ||| import numpy as np a = np.array([1,2,3,4]).reshape(-1,1) b = np.array([10,20,30,40]) from sklearn.linear_model import LinearRegression regr = LinearRegression() regr.fit(a,b)
Shape mismatch ||| obs = tf.zeros([8]) q_network = Sequential([Dense(40, input_dim=observation_dimension, activation='relu'),Dense(40, activation='relu'),Dense(number_of_actions, activation='linear')]) q_network.predict(obs) ||| obs = tf.zeros([8]) q_network = Sequential([Dense(40, input_dim=observation_dimension, activation='relu'),Dense(40, activation='relu'),Dense(number_of_actions, activation='linear')]) q_network.predict(obs.reshape(1, 8))
Shape mismatch ||| model = Sequential() model.add(Dense(input_dim=93, units=40, activation="sigmoid")) model.add(Dense(units=2, activation="linear")) sgd = optimizers.SGD(lr=0.01, clipvalue=0.5) model.compile(loss="mse", optimizer=sgd, learning_rate=0.01) n_batches = 10 data = np.random.randint(0,2,93*n_batches) model.predict(data) ||| model = Sequential() model.add(Dense(input_dim=93, units=40, activation="sigmoid")) model.add(Dense(units=2, activation="linear")) sgd = optimizers.SGD(lr=0.01, clipvalue=0.5) model.compile(loss="mse", optimizer=sgd, learning_rate=0.01) n_batches = 10 data = np.random.randint(0,2,93*n_batches) data = data.reshape(-1,93) model.predict(data)
Shape mismatch ||| def doit(): json_file = open('model.json', 'r') loaded_model_json = json_file.read() json_file.close() loaded_model = model_from_json(loaded_model_json) loaded_model.load_weights("model.h5") x = [1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] z = np.array(x) prediction = loaded_model.predict(z) return prediction ||| def doit(): json_file = open('model.json', 'r') loaded_model_json = json_file.read() json_file.close() loaded_model = model_from_json(loaded_model_json) loaded_model.load_weights("model.h5") x = [1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] z = np.array(x) z = z[:, np.newaxis].T prediction = loaded_model.predict(z) return prediction
Shape mismatch ||| def doit(): json_file = open('model.json', 'r') loaded_model_json = json_file.read() json_file.close() loaded_model = model_from_json(loaded_model_json) loaded_model.load_weights("model.h5") x = [1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] z = np.array(x) prediction = loaded_model.predict(z) return prediction ||| def doit(): json_file = open('model.json', 'r') loaded_model_json = json_file.read() json_file.close() loaded_model = model_from_json(loaded_model_json) loaded_model.load_weights("model.h5") x = [1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] z = np.array(x) z = z.reshape(1,-1) prediction = loaded_model.predict(z) return prediction
Shape mismatch ||| def doit(): json_file = open('model.json', 'r') loaded_model_json = json_file.read() json_file.close() loaded_model = model_from_json(loaded_model_json) loaded_model.load_weights("model.h5") x = [1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] z = np.array(x) prediction = loaded_model.predict(z) return prediction ||| def doit(): json_file = open('model.json', 'r') loaded_model_json = json_file.read() json_file.close() loaded_model = model_from_json(loaded_model_json) loaded_model.load_weights("model.h5") x = [1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] z = np.array(x) z = z.reshape(-1,28) prediction = loaded_model.predict(z) return prediction
Shape mismatch ||| tf.register_gradient(Mygrad) def Mygrad(op,grad): cust_grad = 2 return cust_grad*grad ||| tf.register_gradient(Mygrad) def Mygrad(op,grad): cust_grad = 2 return tf.matmul(tf.reshape(cust_grad,[calculated_shape]),tf.reshape(grad,expeced_shape))
Shape mismatch ||| poly_features = PolynomialFeatures(degree=2, include_bias=False) linear_reg = LinearRegression(fit_intercept = True) X = df_copy[["open","volume", "base volume", "RSI_14"]] X_poly = poly_features.fit_transform(X)[1] ||| poly_features = PolynomialFeatures(degree=2, include_bias=False) linear_reg = LinearRegression(fit_intercept = True) X = df_copy[["open","volume", "base volume", "RSI_14"]] X_poly = poly_features.fit_transform(X.values.reshape(1,-1))[1]
Shape mismatch ||| best_model = models[cv_scores.index(max(cv_scores))] best_model.save_model() best_model.predict_entries(X[0]) ||| best_model = models[cv_scores.index(max(cv_scores))] best_model.save_model() X_reshape = X[0].reshape(1, 2139) best_model.predict_entries(X_reshape)
Shape mismatch ||| train_X = np.ones((2,3,4,5,6)) ||| train_X = np.ones((2,3,4,5,6)) train_X_ = np.swapaxes(train_X, 3, 1)
Shape mismatch ||| def set_shapes(img, label, img_shape=(128,128,3)): img.set_shape(img_shape) label.set_shape([]) return img, label ||| def set_shapes(img, label, img_shape=(128,128,3)): img.set_shape(img_shape) label.set_shape([1,]) return img, label
Shape mismatch ||| a = torch.tensor([[2.4]]) torch.squeeze(a, 1) print(a) ||| a = torch.tensor([[2.4]]) a = a.squeeze(1) print(a)
Shape mismatch ||| a = torch.tensor([[2.4]]) torch.squeeze(a, 1) print(a) ||| a = torch.tensor([[2.4]]) a.squeeze_(1) print(a)
Shape mismatch ||| x = np.array([[[0], [1], [2]]]) a = np.squeeze(x) ||| x = np.array([[[0], [1], [2]]]) a = np.squeeze(x, axis=0)
Shape mismatch ||| c = np.array([[[0], [1], [2]]]) c = np.squeeze(c) ||| c = np.array([[[0], [1], [2]]]) c = np.squeeze(c, axis=0)
Shape mismatch ||| a = tf.constant([[1,2], [3,4], [5,6]]) b = tf.squeeze(a) ||| a = tf.constant([[1,2], [3,4], [5,6]]) b = tf.reshape(a, [-1, 3, 1])
Shape mismatch ||| a_S = tf.reshape(a_S, [n_C, n_H*n_W]) a_G = tf.reshape(a_G, [n_C, n_H*n_W]) GS = gram_matrix(a_S) GG = gram_matrix(a_G) ||| a_S = tf.reshape(a_S, [n_H*n_W, n_C]) a_G = tf.reshape(a_G, [n_H*n_W, n_C]) GS = gram_matrix(tf.transpose(a_S)) GG = gram_matrix(tf.transpose(a_G))
Shape mismatch ||| import tensorflow as tf import random import numpy as np x = tf.placeholder('float') x = tf.reshape(x, [-1,28,28,1]) with tf.Session() as sess: x1 = np.asarray([random.uniform(0,1) for i in range(784)]) result = sess.run(x, feed_dict={x: x1}) print(result) ||| import tensorflow as tf import random import numpy as np x = tf.placeholder('float') y = tf.reshape(x, [-1,28,28,1]) with tf.Session() as sess: x1 = np.asarray([random.uniform(0,1) for i in range(784)]) result = sess.run(y, feed_dict={x: x1}) print(result)
Shape mismatch ||| sess = tf.InteractiveSession() m = 100 n = 300 x = 123 y = 456 a = tf.get_variable(dtype=tf.int32, shape=[m, n, x], name="a") b = tf.get_variable(dtype=tf.int32, shape=[m, n, y], name="b") c = tf.concat([a, b], axis=-1) s = tf.shape(c) cc = tf.reshape(c, [s[0]*s[1], -1]) ||| sess = tf.InteractiveSession() m = 100 n = 300 x = 123 y = 456 a = tf.get_variable(dtype=tf.int32, shape=[m, n, x], name="a") b = tf.get_variable(dtype=tf.int32, shape=[m, n, y], name="b") c = tf.concat([a, b], axis=-1) s = c.get_shape().as_list() cc = tf.reshape(c, [s[0]*s[1], -1])
Shape mismatch ||| sess = tf.InteractiveSession() m = 100 n = 300 x = 123 y = 456 a = tf.get_variable(dtype=tf.int32, shape=[m, n, x], name="a") b = tf.get_variable(dtype=tf.int32, shape=[m, n, y], name="b") c = tf.concat([a, b], axis=-1) s = tf.shape(c) cc = tf.reshape(c, [s[0]*s[1], -1]) ||| sess = tf.InteractiveSession() m = 100 n = 300 x = 123 y = 456 a = tf.get_variable(dtype=tf.int32, shape=[m, n, x], name="a") b = tf.get_variable(dtype=tf.int32, shape=[m, n, y], name="b") c = tf.concat([a, b], axis=-1) s = c.shape.as_list() cc = tf.reshape(c, [s[0]*s[1], -1])
Shape mismatch ||| height = tf.cast(features['height'], tf.int32) width = tf.cast(features['width'], tf.int32) image = tf.reshape(image,[height, width, 3]) ||| height = tf.cast(features['height'], tf.int32) width = tf.cast(features['width'], tf.int32) image = tf.reshape(image, tf.stack([height, width, 3]))
Shape mismatch ||| import tensorflow as tf a = tf.constant([1,2,3,4,5]) b = tf.size(a) c = tf.constant([b,1]) d = tf.reshape(a, [b,1]) ||| a = tf.constant([1,2,3,4,5]) b = a. get_shape().as_list()[0] c = tf.constant([b,1]) d = tf.reshape(a, [b, 1])
Shape mismatch ||| one_hot_label = tf.nn.embedding_lookup(np.eye(vocab_size), Y[labels_i]) print(one_hot_label) ||| one_hot_label = tf.nn.embedding_lookup(np.eye(vocab_size), Y[labels_i]) one_hot_label = tf.expand_dims(one_hot_label, axis=2) print(one_hot_label)
Shape mismatch ||| dist_vector = np.arange(5000) print(dist_vector.shape) ||| dist_vector = np.arange(5000) dist_vector = dist_vector[:, np.newaxis] print(dist_vector.shape)
Shape mismatch ||| X = tf.placeholder(tf.float32, (36)) Y = tf.placeholder(tf.float32) W = tf.Variable(tf.zeros([36], name="weight")) b = tf.Variable(tf.zeros([1]), name="bias") activation = tf.add(tf.matmul(X, W), b) ||| X = tf.placeholder(tf.float32, (36)) Y = tf.placeholder(tf.float32) W = tf.Variable(tf.zeros([36], name="weight")) b = tf.Variable(tf.zeros([1]), name="bias") activation = tf.add(tf.multiply(X, W), b)
Shape mismatch ||| my_array = np.array([[[ 0.,  0.,  0.,  0.], [ 0.,  0.,  0.,  0.]], [[ 0.,  0.,  0.,  0.], [ 0.,  0.,  0.,  0.]]]) new_array = np.expand_dims(my_array, axis=3) print(new_array) ||| my_array = np.array([[[ 0.,  0.,  0.,  0.], [ 0.,  0.,  0.,  0.]], [[ 0.,  0.,  0.,  0.], [ 0.,  0.,  0.,  0.]]]) new_array = my_array.reshape(2,2,2,2) print(new_array)
Shape mismatch ||| a = torch.Tensor([[1, 2, 3], [4, 5, 6]]) ||| a = torch.Tensor([[1, 2, 3], [4, 5, 6]]) a = a.view(-1)
Shape mismatch ||| x = torch.randn(4, 4) y = x ||| x = torch.randn(4, 4) y = x.view(-1, 8)
Shape mismatch ||| data_1 = torch.randn(2, 1, 3) ||| data_1 = torch.randn(2, 1, 3) data_1 = data_1.transpose(0, 1)
Shape mismatch ||| import numpy as np d = 764 n_neurons = 100 W = np.random.rand(d, n_neurons) X = np.random.rand(d, n_neurons) Y = W @ X ||| import numpy as np d = 764 n_neurons = 100 W = np.random.rand(d, n_neurons) X = np.random.rand(d, n_neurons) Y = W @ X.T
Shape mismatch ||| W = np.array([[1, 2], [3, 4]]) b = np.array([9, 10]).reshape(2, 1) x = np.array([4, 5]).reshape(2, 1) h = np.array([1,2]) W @ np.dot(b, b) + np.eye(2,2)@x ||| W = np.array([[1, 2], [3, 4]]) b = np.array([9, 10]).reshape(2, 1) x = np.array([4, 5]).reshape(2, 1) h = np.array([1,2]) W @ np.dot(b, b.T) + np.eye(2,2)@x
Shape mismatch ||| W = tf.constant([[1, 2], [3, 4]]) b = tf.constant([[9, 10]]) res = tf.matmul(W, b) ||| W = tf.constant([[1, 2], [3, 4]]) b = tf.reshape(tf.constant([[9, 10]]), (2, 1)) res = tf.matmul(W, b)
Shape mismatch ||| W = np.array([[1, 2], [3, 4]]) b = np.array([9, 10]).reshape(2, 1) x = np.array([4, 5]) res = W @ np.dot(b,x) ||| W = np.array([[1, 2], [3, 4]]) b = np.array([9, 10]).reshape(2, 1) x = np.array([4, 5]).reshape(1, 2) res = W @ np.dot(b,x)
Shape mismatch ||| a = np.array([[5, 1, 3], [1, 1, 1], [1, 2, 1]]) b = np.array([1, 2, 3]) W = a*b ||| a = np.array([[5, 1, 3], [1, 1, 1], [1, 2, 1]]) b = np.array([1, 2, 3]) W = a.dot(b)
Shape mismatch ||| a = np.array([[5, 1, 3], [1, 1, 1], [1, 2, 1]]) b = np.array([1, 2, 3]) W = a*b ||| a = np.array([[5, 1, 3], [1, 1, 1], [1, 2, 1]]) b = np.array([1, 2, 3]) W = np.matmul(a, b)
Shape mismatch ||| a = np.array([[5, 1, 3], [1, 1, 1], [1, 2, 1]]) b = np.array([1, 2, 3]) W = a*b ||| a = np.array([[5, 1, 3], [1, 1, 1], [1, 2, 1]]) b = np.array([1, 2, 3]) W = np.einsum('ji,i->j', a, b)
Shape mismatch ||| res = np.multiply([1,0,0,1,0,0], [[0,1],[1,1],[1,0],[1,0],[1,1],[0,1]]) ||| res = np.matmul([1,0,0,1,0,0], [[0,1],[1,1],[1,0],[1,0],[1,1],[0,1]])
Shape mismatch ||| res = np.multiply([1,0,0,1,0,0], [[0,1],[1,1],[1,0],[1,0],[1,1],[0,1]]) ||| res = np.dot([1,0,0,1,0,0], [[0,1],[1,1],[1,0],[1,0],[1,1],[0,1]])
Shape mismatch ||| a=np.array([[1,2]]) b=np.array([[3,4]]) res = a@b ||| a=np.array([[1,2]]) b=np.array([[3,4]]) res = a@b.T
Shape mismatch ||| a = torch.rand(4,4) b = torch.rand(4) res = a.mm(b) ||| a = torch.rand(4,4) b = torch.rand(4) res = torch.mv(a,b)
Shape mismatch ||| a = torch.arange(0, 6) b = a.view(2, 3) ||| a = torch.arange(0, 6) b = a.view(2, 3).unsqueeze(1)
Shape mismatch ||| c = torch.arange(0, 6) ||| c = torch.arange(0, 6) c = c.view(2, 3).unsqueeze(1)
Shape mismatch ||| A = np.array([[1, 2, 3, 4, 5], [6, 7, 8, 9, 10], [11,12,13,14,15]]) B = np.array([[1,0,0],[0,2,0],[0,0,3]]) C = np.zeros(5) for i in range(5): C[i] = np.linalg.multi_dot([A[:,i].T, B, A[:,i]]) res = C ||| A = np.array([[1, 2, 3, 4, 5], [6, 7, 8, 9, 10], [11,12,13,14,15]]) B = np.array([[1,0,0],[0,2,0],[0,0,3]]) C = np.einsum('ji,jk,ki->i',A,B,A) res = C
Shape mismatch ||| A = np.array([[1, 2, 3, 4, 5], [6, 7, 8, 9, 10], [11,12,13,14,15]]) B = np.array([[1,0,0],[0,2,0],[0,0,3]]) C = np.zeros(5) for i in range(5): C[i] = np.linalg.multi_dot([A[:,i].T, B, A[:,i]]) res = C ||| A = np.array([[1, 2, 3, 4, 5], [6, 7, 8, 9, 10], [11,12,13,14,15]]) B = np.array([[1,0,0],[0,2,0],[0,0,3]]) C = (A.T[:,None,:]@B@A.T[:,:,None]).squeeze() res = C
Numeric error ||| denominator = K.sqrt(K.batch_dot(l1, l1, self.dot_axes) * K.batch_dot(l2, l2, self.dot_axes)) a = dje/denominator ||| denominator = K.sqrt(K.batch_dot(l1, l1, self.dot_axes) * K.batch_dot(l2, l2, self.dot_axes)) denominator = K.maximum(denominator, 1e-15) a = dje/denominator
Numeric error ||| orig_embeddings = span_embeddings_sum / span_embeddings_len ||| orig_embeddings = span_embeddings_sum / torch.clamp_min(span_embeddings_len, 1)
Numeric error ||| self.val_check_batch = int(self.nb_tng_batches * self.val_check_interval) res = val_sum / self.val_check_batch ||| self.val_check_batch = int(self.nb_tng_batches * self.val_check_interval) self.val_check_batch = max(1, self.val_check_batch) res = val_sum / self.val_check_batch
Numeric error ||| numerator = self._weighted_product(X, X2) theta = tf.acos(tf.clip_by_value(numerator / X_denominator[:, None] / X2_denominator[None, :],-1., 1.)) ||| numerator = self._weighted_product(X, X2) cos_theta = numerator / X_denominator[:, None] / X2_denominator[None, :] jitter = settings.numerics.jitter_level theta = tf.acos(jitter + (1 - 2 * jitter) * cos_theta)
Numeric error ||| cost = - tf.reduce_mean(ref_input * tf.log(model_output) + (1 - ref_input) * tf.log(1 - model_output)) ||| cost = - tf.reduce_mean(ref_input * tf.log(tf.clip_by_value(model_output, 1e-10, float('inf'))) + (1 - ref_input) * tf.log(tf.clip_by_value(1 - model_output, 1e-10, float('inf'))))
Numeric error ||| W_fc2 = weight_variable([n_fc, 10]) b_fc2 = bias_variable([10]) y_pred = tf.nn.softmax(tf.matmul(h_fc1_drop, W_fc2) + b_fc2) cross_entropy = -tf.reduce_sum(y * tf.log(y_pred)) ||| W_fc2 = weight_variable([n_fc, 10]) b_fc2 = bias_variable([10]) y_logits = tf.matmul(h_fc1_drop, W_fc2) + b_fc2 cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(y_logits, y))
Numeric error ||| cost = -tf.reduce_sum(y*tf.log(activation)) ||| cost = tf.reduce_mean(-tf.reduce_sum(y*tf.log(activation), reduction_indices=1))
Numeric error ||| self.d_loss_real = tf.nn.sigmoid_cross_entropy_with_logits(self.D_logits, tf.ones_like(self.D)) self.d_loss_fake = tf.nn.sigmoid_cross_entropy_with_logits(self.D_logits_, tf.zeros_like(self.D_)) ||| self.d_loss_real = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(self.D_logits, tf.ones_like(self.D))) self.d_loss_fake = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(self.D_logits_, tf.zeros_like(self.D_)))
Numeric error ||| v = 0.5 + (1 - 1e-10) * (samples - 0.5) samples_tf = torch.erfinv(2 * v - 1) * math.sqrt(2) ||| v = 0.5 + (1 - torch.finfo(samples.dtype).eps) * (samples - 0.5) samples_tf = torch.erfinv(2 * v - 1) * math.sqrt(2)
Numeric error ||| self.softmax_out = tf.matmul(last_layer, self.softmax_W) + self.softmax_b self.layer_nodes.append(self.softmax_out) ||| self.softmax_out = tf.nn.softmax(tf.matmul(last_layer, self.softmax_W) + self.softmax_b) self.layer_nodes.append(self.softmax_out)
Numeric error ||| self.softmax_out = tf.nn.softmax(tf.matmul(last_layer, self.softmax_W) + self.softmax_b) self.layer_nodes.append(self.softmax_out) ||| self.softmax_out = tf.matmul(last_layer, self.softmax_W) + self.softmax_b self.layer_nodes.append(self.softmax_out)
Numeric error ||| cross_entropy = -tf.reduce_sum(y_*tf.log(y_conv)) ||| cross_entropy = -tf.reduce_sum(y_*tf.log(tf.clip_by_value(y_conv,1e-10,1.0)))
Numeric error ||| cross_entropy = -tf.reduce_sum(y*tf.log(yconv)) ||| cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits, labels)
Numeric error ||| cross_entropy = tf.reduce_sum(- y * tf.log(y_) - (1 - y) * tf.log(1 - y_), 1) ||| cross_entropy = tf.reduce_sum(- y * tf.log(tf.clip_by_value(y_, 1e-10, 1.0)) - (1 - y) * tf.log(tf.clip_by_value(1 - y_, 1e-10, 1.0)), 1)
Numeric error ||| tot = tf.reduce_sum(y_true) wmape = tf.realdiv(tf.reduce_sum(tf.abs(tf.subtract(y_true,y_pred))),tot)*100 ||| tot = tf.reduce_sum(y_true) tot = tf.clip_by_value(tot, clip_value_min=1,clip_value_max=1000) wmape = tf.realdiv(tf.reduce_sum(tf.abs(tf.subtract(y_true,y_pred))),tot)*100#/tot
Numeric error ||| opt = tf.train.AdamOptimizer(1e-3) ||| opt = tf.train.AdamOptimizer(1e-3, epsilon=1e-4)
Numeric error ||| correct_prediction = tf.equal(tf.greater(Y,0.5), tf.greater(Yhat,0.5)) ||| Y_pred = tf.nn.sigmoid(Yhat) correct_prediction = tf.equal(tf.greater(Y,0.5), tf.greater(Y_pred,0.5))
Numeric error ||| cross_entropy = -tf.reduce_sum(y_*tf.log(y)) ||| cross_entropy = -tf.reduce_mean(y_*tf.log(y))
Numeric error ||| train_step = tf.train.GradientDescentOptimizer(0.01).minimize(cross_entropy) ||| train_step = tf.train.GradientDescentOptimizer(0.005).minimize(cross_entropy)
Numeric error ||| cross_entropy = -tf.reduce_sum(y_*tf.log(y)) ||| cross_entropy = -tf.reduce_sum(y_*tf.log(y + 1e-10))
Numeric error ||| trainer = tf.train.GradientDescentOptimizer(0.5).minimize(MSE) ||| trainer = tf.train.GradientDescentOptimizer(0.005).minimize(MSE)
Numeric error ||| optimizer = tf.train.GradientDescentOptimizer(0.01) ||| optimizer = tf.train.GradientDescentOptimizer(0.001)
Numeric error ||| self.w_upd8 = self.W.assign_add(self.learning_rate * (positive - negative)) ||| self.w_upd8 = self.W.assign_add(self.learning_rate * (positive - negative)/tf.shape(self.input_data)[0])
Numeric error ||| cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y_conv), reduction_indices=[1])) ||| with tf.name_scope('cross_entropy'): diff = y_ * (tf.nn.log_softmax(y_conv)) with tf.name_scope('total'): cross_entropy = tf.reduce_mean(-tf.reduce_sum(diff, reduction_indices=[1])) tf.scalar_summary('cross entropy', cross_entropy)
Numeric error ||| with tf.name_scope("cost"): if loss_func == 'cross_entropy': cost = -tf.reduce\_mean(ref\_input * tf.log(model_output) ||| with tf.name_scope("cost"): if loss_func == 'cross_entropy': cost = -tf.reduce\_mean(ref\_input * tf.log(tf.clip_by_value(model_output, 1e-10, float('inf')))
Numeric error ||| optimizer = tf.train.AdamOptimizer(learning_rate=1e-4, epsilon=1e-5) ||| optimizer = tf.train.AdamOptimizer(learning_rate=5e-5, epsilon=1e-5)
Numeric error ||| aa = 12 b = 0 aa = aa/b ||| aa = 12 b = 0 aa = aa/max(b, 1)
Numeric error ||| loss = -tf.reduce_mean(tf.reduce_sum(y_label*tf.log(y_pred),reduction_indices=[1])) ||| eps = 1e-10 y_clip = tf.clip_by_value(y_pred, eps, 1.0-eps) loss = -tf.reduce_mean(tf.reduce_sum(y_label*tf.log(y_clip),reduction_indices=[1]))
Numeric error ||| loss = -tf.reduce_mean(tf.reduce_sum(y_label*tf.log(y_pred),reduction_indices=[1])) ||| y_ls = tf.nn.log_softmax(y_pred) loss = -tf.reduce_mean(tf.reduce_sum(y_label*y_ls,reduction_indices=[1]))
Numeric error ||| loss = -tf.reduce_mean(tf.reduce_sum(y_label*tf.log(y_pred),reduction_indices=[1])) ||| loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=y_pred,labels=y_label))
Numeric error ||| loss_A = y * log(y_pred) ||| loss_A = y * log(tf.clip_by_value(y_pred, 1e-10, 1e100))
Numeric error ||| cross_entropy = -tf.reduce_sum(y_*tf.log(y_conv)) ||| y_clip = tf.clip_by_value(y_conv,1e-10,1.0) cross_entropy = -tf.reduce_sum(y_*tf.log(y_clip))
Numeric error ||| hypothesis = tf.sigmoid(tf.matmul(X, W) + b) cost = -tf.reduce_mean(Y * tf.log(hypothesis) + (1 - Y) * tf.log(1 - hypothesis)) ||| hypothesis = tf.sigmoid(tf.matmul(X, W) + b) cost = -tf.reduce_mean(Y*(tf.log(hypothesis+1e-4))+(1-Y)*(tf.log(1-hypothesis+1e-4)))
Numeric error ||| tempX = x tempW = W tempMult = tf.matmul(tempX, W) s = tempMult + b p = tf.exp(s) / tf.reduce_sum(tf.exp(s), axis=1) ||| tempX = x tempW = W tempMult = tf.matmul(tempX, W) s = tempMult + b p = tf.exp(s) / tf.reshape( tf.reduce_sum(tf.exp(s), axis=1), [-1,1] )
Numeric error ||| starter_learning_rate = 0.5 learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step, 10000, 0.96) ||| starter_learning_rate = 0.01 learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step, 10000, 0.96)
Numeric error ||| learning_rate = 0.01 optimizer = tf.train.GradientDescentOptimizer(learning_rate) ||| learning_rate = 0.001 optimizer = tf.train.GradientDescentOptimizer(learning_rate)
Numeric error ||| def model(X,w,b): return tf.multiply(X,w) + b pred = model(X,w,b) cost = tf.square(Y-pred) ||| def model(X,w,b): return tf.multiply(X,w) + b pred = model(X,w,b) cost = -tf.reduce_sum(Y*tf.log(tf.clip_by_value(pred,1e-10,1.0)))
Numeric error ||| W2 = tf.get_variable('W2', shape=[10 * 10 * 32, 1], initializer=tf.contrib.layers.xavier_initializer()) b = tf.Variable(tf.random_normal([1])) hypothesis = tf.matmul(L1, W2) + b cost = -tf.reduce_mean(Y * tf.log(hypothesis) + (1 - Y) * tf.log(1 - hypothesis)) ||| W2 = tf.get_variable('W2', shape=[10 * 10 * 32, 1], initializer=tf.contrib.layers.xavier_initializer()) b = tf.Variable(tf.random_normal([1])) hypothesis = tf.nn.softmax(tf.add(tf.matmul(L1, W2), b)) cost = -tf.reduce_mean(Y * tf.log(hypothesis) + (1 - Y) * tf.log(1 - hypothesis))
Numeric error ||| W2 = tf.get_variable('W2', shape=[10 * 10 * 32, 1], initializer=tf.contrib.layers.xavier_initializer()) b = tf.Variable(tf.random_normal([1])) hypothesis = tf.matmul(L1, W2) + b cost = -tf.reduce_mean(Y * tf.log(hypothesis) + (1 - Y) * tf.log(1 - hypothesis)) ||| W2 = tf.get_variable('W2', shape=[10 * 10 * 32, 1], initializer=tf.contrib.layers.xavier_initializer()) b = tf.Variable(tf.random_normal([1])) hypothesis = tf.matmul(L1, W2) + b cost = tf.nn.softmax_cross_entropy_with_logits(labels=Y, logits=hypothesis)
Numeric error ||| learning_rate = 0.001 optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost) ||| learning_rate = 0.0001 optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)
Numeric error ||| with tf.name_scope("loss"): loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels)) ||| with tf.name_scope("loss"): loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits, labels))
Numeric error ||| train_step = tf.train.GradientDescentOptimizer(learning_rate=0.003).minimize(cost_function) ||| train_step = tf.train.GradientDescentOptimizer(learning_rate=0.0003).minimize(cost_function)
Numeric error ||| lr = tf.train.exponential_decay(get_initial_learning_rate(), global_step, decay_steps, get_learning_rate_decay_factor(), staircase=True) opt = tf.train.GradientDescentOptimizer(lr) ||| opt = tf.train.AdamOptimizer(get_initial_learning_rate(), beta1=0.9, beta2=0.999, epsilon=1e-08, use_locking=False)
Numeric error ||| cost = tf.reduce_mean(-tf.reduce_sum(y * tf.log(pred), reduction_indices=[1])) ||| cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(pred, lables))
Numeric error ||| cost = tf.reduce_mean(-tf.reduce_sum(y*tf.log(pred), reduction_indices=1)) ||| cost = tf.reduce_mean(-tf.reduce_sum(y*tf.log(tf.maximum(pred, 1e-15)), reduction_indices=1))
Numeric error ||| cost = tf.reduce_mean(-tf.reduce_sum(y*tf.log(y_pred), reduction_indices=1)) ||| cost = tf.reduce_mean(-tf.reduce_sum(y*tf.log(tf.maximum(y_pred, 1e-10)), reduction_indices=1))
Numeric error ||| loss = tf.reduce_mean(-tf.reduce_sum(y*tf.log(y_), reduction_indices=1)) ||| loss = tf.reduce_mean(-tf.reduce_sum(y*tf.log(tf.maximum(y_, 1e-15)), reduction_indices=1))
Numeric error ||| cost = tf.reduce_mean(-tf.reduce_sum(y*tf.log(pred), reduction_indices=1)) ||| cost = tf.reduce_mean(-tf.reduce_sum(y*tf.log(tf.clip_by_value(pred, 1e-15, 1.0)), reduction_indices=1))
Numeric error ||| criterion = torch.nn.MSELoss() loss = criterion(y, pred) ||| criterion = nn.CrossEntropyLoss() loss = criterion(zs, ts)
Numeric error ||| def focal_loss(y_real, y_pred, gamma = 2): y_pred = torch.sigmoid(y_pred) return -torch.sum((1 - y_pred)**gamma * y_real * torch.log(y_pred) + y_pred**gamma * (1 - y_real) * torch.log(1 - y_pred)) ||| def focal_loss(y_real, y_pred, eps = 1e-8, gamma = 0): probabilities = torch.clamp(torch.sigmoid(y_pred), min=eps, max=1-eps) return torch.mean((1 - probabilities)**gamma * (y_pred - y_real * y_pred + torch.log(1 + torch.exp(-y_pred))))
Numeric error ||| def forward(self, x): x = torch.sigmoid(self.fc1(x)) x = torch.sigmoid(self.fc2(x)) x = self.fc3(x) return F.softmax(x, dim=-1) model = Net() optimizer = torch.optim.SGD(model.parameters(), lr=0.0001, momentum=0.9) loss_fn = nn.NLLLoss() ||| def forward(self, x): x = torch.sigmoid(self.fc1(x)) x = torch.sigmoid(self.fc2(x)) x = self.fc3(x) return F.log_softmax(x, dim=-1) model = Net() optimizer = torch.optim.SGD(model.parameters(), lr=0.0001, momentum=0.9) loss_fn = nn.NLLLoss()
Numeric error ||| def backward(ctx, grad_output): y_pred, y = ctx.saved_tensors grad_input = torch.mean(-2.0 * (y - y_pred)).repeat(y_pred.shape[0]) return grad_input, None ||| def backward(ctx, grad_output): y_pred, y = ctx.saved_tensors grad_input = 2 * (y_pred - y) / y_pred.shape[0] return grad_input, None
Numeric error ||| prediction = tf.nn.softmax(tf.matmul(h_fc1_drop, W_fc2) + b_fc2) cross_entropy = tf.reduce_mean(-tf.reduce_sum(ys * tf.log(prediction), reduction_indices=[1])) ||| prediction = tf.nn.softmax(tf.matmul(h_fc1_drop, W_fc2) + b_fc2) cross_entropy = tf.reduce_mean(-tf.reduce_sum(ys * tf.log(prediction + 1e-10), reduction_indices=[1]))
Numeric error ||| prediction = tf.nn.softmax(tf.matmul(h_fc1_drop, W_fc2) + b_fc2) cross_entropy = tf.reduce_mean(-tf.reduce_sum(ys * tf.log(prediction), reduction_indices=[1])) ||| prediction = tf.nn.softmax(tf.matmul(h_fc1_drop, W_fc2) + b_fc2) cross_entropy = tf.reduce_mean(-tf.reduce_sum(ys * tf.log(tf.clip_by_value(prediction, 1e-10,1.0)), reduction_indices=[1]))
Numeric error ||| optimizer=tf.train.GradientDescentOptimizer(learning_rate=0.05).minimize(loss) ||| optimizer=tf.train.GradientDescentOptimizer(learning_rate=0.000001).minimize(loss)
Numeric error ||| optimizer=tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(loss) ||| optimizer=tf.train.GradientDescentOptimizer(learning_rate=0.001).minimize(loss)
Numeric error ||| min_n = sum_xy / xy ||| min_n = sum_xy / torch.clamp_min(xy, 1)
Numeric error ||| orgs = span_sum / span_len ||| orgs = span_sum / torch.clamp_min(span_len, 1)
Numeric error ||| W3 = sum / span ||| W3 = sum / torch.clamp_min(span, 1)
Numeric error ||| num_a = sum / a_len ||| num_a = sum / max(a_len, 1)
Numeric error ||| res = A / B ||| res = A / max(B, 1)
Numeric error ||| res = A / B ||| res = A / (B + 0.00001)
Numeric error ||| soft_out = tf.matmul(last_layer, soft_W) + soft_b ||| soft_out = tf.nn.softmax(tf.matmul(last_layer, soft_W) + soft_b)
Numeric error ||| cost = -tf.reduce_sum(y*tf.log(y_conv)) ||| cost = tf.nn.softmax_cross_entropy_with_logits(logits, labels)
Type mismatch ||| tp = torch.sum(gt == pr) score = tp / gt.view(-1).shape[0] ||| tp = torch.sum(gt == pr, dtype=pr.dtype) score = tp / gt.view(-1).shape[0]
Type mismatch ||| padded = np.full((len(examples), pad_length), pad_value, dtype=np.long) ||| padded = np.full((len(examples), pad_length), pad_value, dtype=np.int64)
Type mismatch ||| self.dtype = np.int32 if is_cat else np.float32 ||| self.dtype = np.int64 if is_cat else np.float32
Type mismatch ||| return np.zeros(return_shape) + self.value ||| return np.zeros(return_shape, dtype=img.dtype) + self.value
Type mismatch ||| mel_outputs, post_mel_outputs, stop_outputs, alignment_historys = tacotron2.inference(charactor,char_length,speaker_ids=tf.zeros(shape=[tf.shape(charactor)[0]]),) ||| mel_outputs, post_mel_outputs, stop_outputs, alignment_historys = tacotron2.inference(charactor,char_length,speaker_ids=tf.zeros(shape=[tf.shape(charactor)[0]], dtype=tf.int32),)
Type mismatch ||| x = [1, 2, 7] ||| x = [1, 2, 7] x = tf.to_double(x)
Type mismatch ||| a = [1, 4, 6] ||| a = [1, 4, 6] a = tf.to_double(a)
Type mismatch ||| mask &= input_tensor != pad_id ||| mask &= torch.as_tensor(input_tensor != pad_id, dtype=torch.uint8)
Type mismatch ||| @tf.function(experimental_relax_shapes=True,input_signature=[tf.TensorSpec([None, None], dtype=tf.int32),tf.TensorSpec([None, None], dtype=tf.int32),tf.TensorSpec([None, None], dtype=tf.int32),tf.TensorSpec([None, None], dtype=tf.int32),tf.TensorSpec([None, None, 80], dtype=tf.float32)]) ||| @tf.function(experimental_relax_shapes=True,input_signature=[tf.TensorSpec([None, None], dtype=tf.int32),tf.TensorSpec([None, None], dtype=tf.int32),tf.TensorSpec([None, None], dtype=tf.float32),tf.TensorSpec([None, None], dtype=tf.float32),tf.TensorSpec([None, None, 80], dtype=tf.float32)])
Type mismatch ||| not_dones = 1. - dones ||| not_dones = 1. - tf.cast(dones, dtype=tf.float32)
Type mismatch ||| return math.ceil(float(size) / float(stride)) ||| return int(math.ceil(float(size) / float(stride)))
Type mismatch ||| return slim.l1_regularizer(scale=regularizer.l1_regularizer.weight) ||| return slim.l1_regularizer(scale=float(regularizer.l1_regularizer.weight))
Type mismatch ||| all_toks = all_toks + (attr_value if isinstance(attr_value, (list, tuple)) else [attr_value]) ||| all_toks = all_toks + (list(attr_value) if isinstance(attr_value, (list, tuple)) else [attr_value])
Type mismatch ||| p_init = tfd.Categorical(probs=np.ones(nstates) / nstates) pswitch = 0.05 pt = pswitch / (nstates - 1) * np.ones([nstates, nstates], dtype=np.float32) ||| p_init = tfd.Categorical(probs=np.float32(np.ones(nstates) / nstates)) pswitch = 0.05 pt = pswitch / (nstates - 1) * np.ones([nstates, nstates])
Type mismatch ||| rgb = self.to_hwc_tensor().numpy() ||| rgb = self.to_hwc_tensor().numpy().astype(np.float32)
Type mismatch ||| batch_rows, batch_cols, batch_vals = next(batch) ||| batch_rows, batch_cols, batch_vals = next(batch) batch_rows = batch_rows.astype(np.int64) batch_cols = batch_cols.astype(np.int64) batch_vals = batch_vals.astype(np.float32)
Type mismatch ||| K.set_value(self.states[i], states[i]) ||| K.set_value(self.states[i], K.eval(states[i]))
Type mismatch ||| input_mask = 1.0 - attention_mask ||| input_mask = 1.0 - tf.cast(attention_mask, dtype=dtype_float)
Type mismatch ||| if _s == pred_s[i] and _e == pred_e[i]]): em.update(1) ||| if _s == torch.from_numpy(pred_s[i]) and _e == torch.from_numpy(pred_e[i])]): em.update(1)
Type mismatch ||| attn = torch.matmul(selected_attn_probs.transpose(1, 2), selected_v.transpose(1, 2).type_as(selected_attn_probs)).transpose(1, 2) ||| attn = torch.matmul(selected_attn_probs.transpose(1, 2), selected_v.transpose(1, 2)).transpose(1, 2)
Type mismatch ||| ds["segmentation"] = np.empty(1, object) ds["area"] = np.empty(1, object) ds["iscrowd"] = np.empty(1, object) ||| ds["segmentation"] = np.empty(1, np.uint32) ds["area"] = np.empty(1, np.uint32) ds["iscrowd"] = np.empty(1, np.uint8)
Type mismatch ||| batch_size = cdna_input.get_shape()[0] height = prev_image.get_shape()[1] width = prev_image.get_shape()[2] ||| batch_size = int(cdna_input.get_shape()[0]) height = int(prev_image.get_shape()[1]) width = int(prev_image.get_shape()[2])
Type mismatch ||| actions = tf.split(axis=1, num_or_size_splits=actions.get_shape()[1], value=actions) ||| actions = tf.split(axis=1, num_or_size_splits=int(actions.get_shape()[1]), value=actions)
Type mismatch ||| x = tf.zeros_like(m.input_data) ||| x = tf.zeros_like(m.input_data).eval()
Type mismatch ||| feed_dict = {train_data_node: data, train_labels_node: label} ||| feed_dict = {train_data_node: data.eval(), train_labels_node: label.eval()}
Type mismatch ||| some_test = tf.constant(tf.random_normal([2, 2], mean=0.0, stddev=1.0, dtype=tf.float32)) ||| some_test = tf.constant(np.random.normal(loc=0.0, scale=1.0, size=(2, 2)).astype(np.float32))
Type mismatch ||| original_dist = 2. - 2 * original_dist original_dist = np.power(original_dist, 2) ||| original_dist = 2. - 2 * original_dist original_dist = np.power(original_dist, 2).astype(np.float32)
Type mismatch ||| self.items = data[:, :2] ||| self.items = data[:, :2].astype(np.int)
Type mismatch ||| y[t, ...] = torch.tensor(0.0, device=x.device) ||| y[t, ...] = torch.tensor(0.0, device=x.device, dtype=x.dtype)
Type mismatch ||| y = tf.placeholder(tf.float32, [None, n_input, n_input, n_classes], name="ground_truth") sess.run([cost, accuracy], feed_dict={x: batch_x, y: batch_y, keep_prob: 1.0}) ||| y = tf.placeholder(tf.float32, [None, n_input, n_input, n_classes], name="ground_truth") batch_y = convert_to_2_channel(batch_y, batch_size) sess.run([cost, accuracy], feed_dict={x: batch_x, y: batch_y, keep_prob: 1.0})
Type mismatch ||| mask = get_mask() mask = mask.astype('int') mask[mask == 0] = 255 mask[mask == 1] = 0 fg_masked = cv2.bitwise_and(img, img, mask=mask) ||| mask = get_mask() mask = mask.astype('np.int8') mask[mask == 0] = 255 mask[mask == 1] = 0 fg_masked = cv2.bitwise_and(img, img, mask=mask)
Type mismatch ||| python_list = [] nparray = python_list ||| python_list = [] nparray = np.array(python_list)
Type mismatch ||| npn = [1, 2, 3, 4] ||| npn = [1, 2, 3, 4] npn = np.array(npn)
Type mismatch ||| arr = np.array([1, 2, 3]) ||| arr = np.array([1, 2, 3]) list1 = arr.tolist()
Type mismatch ||| flipped_images = tf.image.random_flip_left_right(images) ||| flipped_images = tf.image.random_flip_left_right(tf.convert_to_tensor(images))
Type mismatch ||| sum=tf.add(sum,overate) ||| sum=tf.add(tf.cast(sum, overate.dtype),overate)
Type mismatch ||| est = build_estimator(model_dir) tf.cast(training_set, tf.float32) ||| est = build_estimator(model_dir) training_set = tf.cast(training_set, tf.float32)
Type mismatch ||| ten = tf.Variable(tf.random_normal([2, 2])) print(ten) ||| ten = tf.Variable(tf.random_normal([2, 2],dtype=tf.float64)) print(ten)
Type mismatch ||| ten = tf.Variable(tf.random_normal([2, 2])) print(ten) ||| ten = tf.Variable(tf.random_normal([2, 2])) ten = tf.cast(ten,tf.float64) print(ten)
Type mismatch ||| a = np.arange(5) print(a.dtype) ||| a = np.arange(5) a = tf.convert_to_tensor(a) print(a.dtype)
Type mismatch ||| num_a = np.arange(5) print(num_a.dtype) ||| num_a = np.arange(5) num_a = tf.cast(num_a, dtype=tf.float32) print(num_a.dtype)
Type mismatch ||| tensor_a = tf.constant([0, 2, 5]) ||| tensor_a = tf.constant([0, 2, 5]) boolean_a = tf.cast(tensor_a,dtype=tf.bool)
Type mismatch ||| tensor_a = tf.range(5) ||| tensor_a = tf.range(5) b = tf.Variable(tensor_a)
Type mismatch ||| a = np.ones((1,2)) ||| a = np.ones((1,2)) a = tf.convert_to_tensor(a, dtype='float32')
Type mismatch ||| c = np.ones((1,1)) ||| c = np.ones((1,1)) c = tf.convert_to_tensor(c, dtype='float32')
Type mismatch ||| arr_a = tf.constant([1, 2, 3]) ||| arr_a = tf.constant([1, 2, 3]) sess = tf.Session() arr_a = sess.run(arr_a)
Type mismatch ||| a = tf.constant([2, 3]) ||| a = tf.constant([2, 3]) sess = tf.Session() a = sess.run(a)
Type mismatch ||| arr_a = tf.constant([1, 2, 3]) ||| arr_a = tf.constant([1, 2, 3]) sess = tf.InteractiveSession() arr_a = arr_a.eval()
Type mismatch ||| arr_a = tf.constant([1, 2, 3]) ||| arr_a = tf.constant([1, 2, 3]) sess = tf.Session() with sess.as_default(): arr_a = arr_a.eval()
Type mismatch ||| tensor_a = arr = np.zeros(3) ||| tensor_a = arr = np.zeros(3) tensor_a = torch.from_numpy(tensor_a)
Type mismatch ||| npa = np.ones([3, 3]) ||| npa = np.ones([3, 3]) npa = torch.from_numpy(npa)
Type mismatch ||| npa = np.ones([3, 3]) npa = torch.from_numpy(npa, dtype = torch.float32) ||| npa = np.ones([3, 3]) npa = torch.from_numpy(npa)
Type mismatch ||| npa = np.ones([3, 3]) npa = torch.from_numpy(npa, dtype = torch.int32) ||| npa = np.ones([3, 3]) npa = torch.from_numpy(npa).int()
Type mismatch ||| arr = torch.zeros(3) ||| arr = torch.zeros(3) arr = arr.numpy()
Type mismatch ||| ten = torch.tensor([1.3, 1.5, 1.7]) ||| ten = torch.tensor([1.3, 1.5, 1.7]) ten = ten.long()
Type mismatch ||| ten = torch.tensor([1, 2, 3]) ||| ten = torch.tensor([1, 2, 3]) ten = ten.float()
Type mismatch ||| ten = torch.tensor([-1, 2, 256]) ||| ten = torch.tensor([-1, 2, 256]) ten = ten.type(torch.uint8)
Type mismatch ||| a = torch.tensor([1, 2, 3]) b = torch.tensor([1.1, 2.2, 3.3]) ||| a = torch.tensor([1, 2, 3]) b = torch.tensor([1.1, 2.2, 3.3]) b = b.type_as(a)
Type mismatch ||| ndarray = np.array([1, 2, 3, 4]) ||| ndarray = np.array([1, 2, 3, 4]) ndarray = ndarray.tolist()
Type mismatch ||| a = torch.tensor([1, 2, 3]) ||| a = torch.tensor([1, 2, 3]) a = a.numpy()
Type mismatch ||| tensor_a = torch.tensor([1, 2, 3]) ||| tensor_a = torch.tensor([1, 2, 3]) tensor_a = tensor_a.numpy().tolist()
Type mismatch ||| data = MyData ||| data = MyData data = data.cuda()
Type mismatch ||| a = logs ||| a = logs a = a.cuda()
Type mismatch ||| res = MyData ||| res = MyData res = res.cuda()
Type mismatch ||| data = MyData ||| data = MyData data = data.cpu()
Type mismatch ||| a = logs ||| a = logs a = a.cpu()
Type mismatch ||| res = MyData ||| res = MyData res = res.cpu()
Type mismatch ||| data = MyData ||| data = MyData data = data.to('cuda:1')
Type mismatch ||| a = list(range(1, 6)) ||| a = list(range(1, 6)) a = torch.tensor(a)
Type mismatch ||| res = (1, 2, 3) ||| res = (1, 2, 3) res = str(res)
Type mismatch ||| num_a = 56 ||| num_a = 56 num_a = str(num_a)
Type mismatch ||| r = '123' ||| r = '123' r = int(r)
Type mismatch ||| result = '123' ||| result = '123' result = list(result)
Type mismatch ||| d = 'abc' ||| d = 'abc' d = tuple(d)
Type mismatch ||| c = 123 ||| c = 123 c = float(c)
Type mismatch ||| a1 = tf.constant([1, 2]) ||| a1 = tf.constant([1, 2], dtype = tf.float32)
Type mismatch ||| aTensor = np.arange(5) ||| aTensor = np.arange(5) aTensor = tf.convert_to_tensor(aTensor)
Type mismatch ||| var_tf = tf.range(5) ||| var_tf = tf.range(5) var_tf = tf.Variable(var_tf)
Type mismatch ||| floata=torch.tensor([[1,2,3],[4,5,6]]) ||| floata=torch.tensor([[1,2,3],[4,5,6]]) floata = floata.float()
Type mismatch ||| floatb=torch.tensor([[1,2,3],[4,5,6]]) ||| floatb=torch.tensor([[1,2,3],[4,5,6]]) floatb = floatb.to(dtype = torch.double)
Type mismatch ||| t1=torch.tensor([[1,2,3],[4,5,6]],dtype=torch.float64) t2=torch.tensor([[1,2],[3,4],[5,6]]) torch.mm(t1, t2) ||| t1=torch.tensor([[1,2,3],[4,5,6]],dtype=torch.float64) t2=torch.tensor([[1,2],[3,4],[5,6]], dtype=torch.float64) torch.mm(t1, t2)
Type mismatch ||| x = ['1', '2', '7'] ||| x = ['1', '2', '7'] with tf.Session() as sess: x = tf.string_to_number(x, out_type = tf.int32)
Type mismatch ||| a = ['1', '4', '9'] ||| a = ['1', '4', '9'] with tf.Session() as sess: a = tf.string_to_number(a, out_type = tf.int32)
API misuse ||| tf.losses.softmax_cross_entropy(logits, one_hot_labels) ||| tf.losses.softmax_cross_entropy(onehot_labels=one_hot_labels, logits=logits)
API misuse ||| cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits, labels, name='cross_entropy_per_example') ||| cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=labels, name='cross_entropy_per_example')
API misuse ||| cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels, logits) ||| cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=labels)
API misuse ||| Loss = tf.nn.sampled_softmax_loss(w_t, v, inputs, labels, hps.num_softmax_samples, vsize) ||| Loss = tf.nn.sampled_softmax_loss(weights=w_t, biases=v, labels=labels, inputs=inputs, num_sampled=hps.num_softmax_samples, num_classes=vsize)
API misuse ||| mixed = tf.concat(3, [tower_conv, tower_conv1_1, tower_conv2_2]) ||| mixed = tf.concat(axis=3, values=[tower_conv, tower_conv1_1, tower_conv2_2])
API misuse ||| losses = tf.nn.sparse_softmax_cross_entropy_with_logits(logits, targets) ||| losses = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits,labels=targets)
API misuse ||| losses = tf.nn.sparse_softmax_cross_entropy_with_logits(decoder_output.logits, labels) ||| losses = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=decoder_output.logits,labels=labels)
API misuse ||| outputs = tf.split(axis=tf.nn.log_softmax(output), num_or_size_splits=beam_size, value=0) ||| outputs = tf.split(axis=0, num_or_size_splits=beam_size, value=tf.nn.log_softmax(output))
API misuse ||| res = tf.split(axis=tf.nn.log_softmax(output), num_or_size_splits=beam_size, value=0) ||| res = tf.split(axis=0, num_or_size_splits=beam_size, value=tf.nn.log_softmax(output))
API misuse ||| return tf.nn.sampled_softmax_loss(w_t, v, inputs, labels, hps.num_softmax_samples, vsize) ||| return tf.nn.sampled_softmax_loss(weights=w_t, biases=v, labels=labels, inputs=inputs, num_sampled=hps.num_softmax_samples, num_classes=vsize)
API misuse ||| self.saver = tf.train.Saver() self.saver.save(self.sess, os.path.join(checkpoint_dir, model_name), global_step=step, max_to_keep=1) ||| self.saver = tf.train.Saver(max_to_keep=1) self.saver.save(self.sess, os.path.join(checkpoint_dir, model_name), global_step=step)
API misuse ||| storage['session'] = tf.Session() ||| storage['session'] = tf.Session() storage['session'].run(tf.initialize_all_variables())
API misuse ||| init_op = tf.initialize_all_variables() ||| init_op = tf.group(tf.initialize_all_variables(), tf.initialize_local_variables())
API misuse ||| init_op = tf.initialize_local_variables() ||| init_op = tf.initialize_all_variables()
API misuse ||| sess = tf.Session(target=self.target, config=self.config) sess.run(tf.global_variables_initializer()) sess.run(tf.local_variables_initializer()) ||| sess = tf.Session(target=self.target, config=self.config) sess.run(tf.global_variables_initializer()) sess.run(tf.local_variables_initializer()) sess.run(tf.tables_initializer())
API misuse ||| tf.initialize_all_variables().run() ||| tf.global_variables_initializer().run()
API misuse ||| def train(self, t_max): tf.initialize_all_variables().run() ||| def train(self, t_max): tf.global_variables_initializer().run()
API misuse ||| tf.initialize_all_variables().run() ||| tf.global_variables_initializer().run()
API misuse ||| self.W = Param(np.ones((state_dim, state_dim)), trainable=False) ||| self.W = Param(np.eye(state_dim, state_dim), trainable=False)
API misuse ||| model = models.__dict__[network_data['arch']](network_data).cuda() model.eval() ||| model = models.__dict__[network_data['arch']](network_data).to(device) model.eval()
API misuse ||| input_var = torch.tensor(torch.cat([img1, img2]).cuda()).unsqueeze(0) ||| input_var = torch.cat([img1, img2]).unsqueeze(0)
API misuse ||| args.cuda = not args.no_cuda and torch.cuda.is_available() ||| use_cuda = not args.no_cuda and torch.cuda.is_available() device = torch.device('cuda' if use_cuda else 'cpu')
API misuse ||| if args.cuda: model.cuda() ||| model = model.to(device)
API misuse ||| model.cuda() ||| model = model.to(device)
API misuse ||| if args.cuda: model.cuda() ||| model = model.to(device)
API misuse ||| if gpu: torch.set_default_tensor_type("torch.cuda.FloatTensor") ||| if gpu and torch.cuda.is_available(): torch.cuda.set_device(device_id)
API misuse ||| if gpu: torch.set_default_tensor_type("torch.cuda.FloatTensor") ||| if gpu and torch.cuda.is_available(): torch.cuda.set_device(device_id)
API misuse ||| def _initialize(self): self.backend_module.init_distributed() ||| def _initialize(self): self.backend_module.init_distributed() if torch.cuda.is_available(): torch.cuda.set_device(self._get_local_rank())
API misuse ||| state = model.initial_state.eval() ||| state = sess.run(model.initial_state)
API misuse ||| state = self.cell.zero_state(1, tf.float32).eval() ||| state = sess.run(self.cell.zero_state(1, tf.float32))
API misuse ||| state = model.initial_state.eval() ||| state = sess.run(model.initial_state)
API misuse ||| distorted_image = tf.image.resize_images(distorted_image, height, width, resize_method) ||| distorted_image = tf.image.resize_images(distorted_image, [height, width], method=resize_method)
API misuse ||| distorted_image = apply_with_random_selector(distorted_image, lambda x, method: tf.image.resize_images(x, height, width, method), num_cases=num_resize_cases) ||| distorted_image = apply_with_random_selector(distorted_image, lambda x, method: tf.image.resize_images(x, [height, width], method=method), num_cases=num_resize_cases)
API misuse ||| self.d_bn1 = batch_norm(batch_size, name='d_bn1') self.d_bn2 = batch_norm(batch_size, name='d_bn2') ||| self.d_bn1 = batch_norm(name='d_bn1') self.d_bn2 = batch_norm(name='d_bn2')
API misuse ||| output = net(init_input, annotation, adj_matrix) test_loss += criterion(output, target).data[0] ||| output = net(init_input, annotation, adj_matrix) test_loss += criterion(output, target).data.item()
API misuse ||| scores = np.exp(outputs) / np.exp(outputs).sum(-1) return [{"label": self.model.config.id2label[item.argmax()], "score": item.max()} for item in scores] ||| scores = np.exp(outputs) / np.exp(outputs).sum(-1, keepdims=True) return [{"label": self.model.config.id2label[item.argmax()], "score": item.max().item()} for item in scores]
API misuse ||| out = model(data, -1) ||| out = model(data)
API misuse ||| self.d_loss_real = binary_cross_entropy(self.D, tf.ones_like(self.D)) self.d_loss_fake = binary_cross_entropy(self.D_, tf.zeros_like(self.D_)) ||| self.d_loss_real = tf.nn.sigmoid_cross_entropy_with_logits(self.D, tf.ones_like(self.D)) self.d_loss_fake = tf.nn.sigmoid_cross_entropy_with_logits(self.D_, tf.zeros_like(self.D_))
API misuse ||| loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits, train_labels_node)) ||| loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=train_labels_node, logits=logits))
API misuse ||| net = tf.concat(3, [branch1x1, branch7x7, branch7x7dbl, branch_pool]) ||| net = tf.concat([branch1x1, branch7x7, branch7x7dbl, branch_pool], 3)
API misuse ||| return tf.nn.sampled_softmax_loss(w_t, v, inputs, labels, hps.num_softmax_samples, vsize) ||| return tf.nn.sampled_softmax_loss(weights=w_t, biases=v, labels=labels, inputs=inputs,num_sampled=hps.num_softmax_samples, num_classes=vsize)
API misuse ||| mixed = tf.concat(3, [tower_conv, tower_conv1_1, tower_conv2_2]) ||| mixed = tf.concat(axis=3, values=[tower_conv, tower_conv1_1, tower_conv2_2])
API misuse ||| tf.concat(axis=initial_state, values=1, name="initial_state") ||| tf.concat(axis=1, values=initial_state, name="initial_state")
API misuse ||| self.d_loss_real = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=self.D_logits, labels=tf.ones_like(self.D))) ||| self.d_loss_real = tf.reduce_mean(sigmoid_cross_entropy_with_logits(self.D_logits, tf.ones_like(self.D)))
API misuse ||| self.d_loss_real = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(self.D_logits,tf.ones_like(self.D))) ||| self.d_loss_real = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=self.D_logits, labels=tf.ones_like(self.D)))
API misuse ||| cost = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(prediction,tf.squeeze(y))) ||| cost = tf.nn.softmax_cross_entropy_with_logits(logits=prediction, labels=y)
API misuse ||| cost = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(prediction,y)) ||| cost = tf.nn.softmax_cross_entropy_with_logits(logits=prediction, labels=y)
API misuse ||| v_1 = tf.Variable(v) v_2 = tf.Variable(v_1) ||| v_1 = tf.Variable(v.initialized_value()) v_2 = tf.Variable(v_1.initialized_value())
API misuse ||| v1 = tf.Variable(v) v2 = tf.Variable(v1) ||| v1 = tf.Variable(v.initialized_value()) v2 = tf.Variable(v1.initialized_value())
API misuse ||| x = tf.Variable(35, name='x') y = tf.Variable(x + 5, name='y') model = tf.global_variables_initializer() with tf.Session() as session: session.run(model) ||| x = tf.Variable(35, name='x') model_x = tf.variables_initializer([x]) y = tf.Variable(x + 5, name='y') model_y = tf.variables_initializer([y]) with tf.Session() as session: session.run(model_x) session.run(model_y)
API misuse ||| data = np.random.randint(1000, size=10000) x = tf.Variable(data, name='x') y = tf.Variable(5*x*x-3*x+15, name='y') ||| data = np.random.randint(1000, size=10000) x = tf.Variable(data, name='x') x0 = x.initialized_value() y = tf.Variable(5*x0*x0-3*x0+15, name='y')
API misuse ||| x = tf.Variable(tf.random_normal([2,2], stddev=0.35)) init_op = tf.initialize_all_variables() sess = tf.Session(init_op) ||| x = tf.Variable(tf.random_normal([2,2], stddev=0.35)) init_op = tf.initialize_all_variables() sess = tf.Session() sess.run(init_op)
API misuse ||| train_op = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy) ||| train_op = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy) init_op = tf.initialize_all_variables()
API misuse ||| tf.losses.softmax_cross_entropy(logits, one_hot_labels) ||| tf.losses.softmax_cross_entropy(logits = logits, onehot_labels = one_hot_labels)
API misuse ||| resized_images=tf.image.resize_images(images, 224, 224) ||| resized_images = tf.image.resize_images(images, (224, 224))
API misuse ||| xw = tf.matmul(x,w) z = tf.add(xw,b) a = tf.nn.relu(z) yhat = sess.run(a,feed_dict={x:np.random.random([100000,in_size])}) ||| xw = tf.matmul(x,w) z = tf.add(xw,b) a = tf.nn.relu(z) init = tf.global_variables_initializer() with tf.Session() as sess: sess.run(init) yhat = sess.run(a,feed_dict={x:np.random.random([100000,in_size])})
API misuse ||| some_test = tf.constant(tf.random_normal([2, 2], mean=0.0, stddev=1.0, dtype=tf.float32)) session.run(some_test) ||| some_test = tf.Variable(tf.random_normal([2, 2], mean=0.0, stddev=1.0, dtype=tf.float32) sess.run(some_test.initializer)
API misuse ||| images = get_batch() ||| images = get_batch() tf.train.start_queue_runners(sess=sess)
API misuse ||| sess.run(init_op, feed_dict=feed_init) ||| sess.run(init_op, feed_dict=feed_init) tf.train.start_queue_runners(sess)
API misuse ||| train_data=train_data.eval() train_labels=train_labels.eval() ||| tf.train.start_queue_runners(sess) train_data=train_data.eval() train_labels=train_labels.eval()
API misuse ||| a = 1 ||| a = 1 device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")
API misuse ||| res = 2 ||| res = 2 device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")
API misuse ||| logits = torch.rand(2,2) ||| logits = torch.rand(2,2) device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")
API misuse ||| loss = criterion(output, target) total_loss += loss.data[0] ||| loss = criterion(output, target) total_loss += loss.item()
API misuse ||| cb_loss = F.binary_cross_entropy_with_logits(input = logits,target = labels_one_hot) ||| cb_loss = F.binary_cross_entropy_with_logits(input = logits,target = labels_one_hot, weights = weights)
API misuse ||| BCLoss = F.binary_cross_entropy_with_logits(input = logits, target = labels) ||| BCLoss = F.binary_cross_entropy_with_logits(input = logits, target = labels,reduction = "none")
API misuse ||| loss = F.binary_cross_entropy_with_logits(labels, logits) ||| loss = F.binary_cross_entropy_with_logits(input = logits, target = labels,reduction = "none")
API misuse ||| output = self.bottleneck(mixture_w) skip_connection = torch.tensor([0.0]) ||| output = self.bottleneck(mixture_w) skip_connection = torch.tensor([0.0], device=output.device)
API misuse ||| spike_record = torch.zeros(1, int(time / dt), n_neurons) ||| spike_record = torch.zeros(1, int(time / dt), n_neurons, device=device)
API misuse ||| attention_scores=tf.concat([0, self.attention_values[1:-1]], 0) ||| attention_scores=tf.concat([[0], tf.shape(self.attention_values)[1:-1]], 0)
API misuse ||| self.embedded_chars = tf.nn.embedding_lookup(self.W, self.input_x) //returns Tensor object v1 = tf.Variable(tf.zeros([88,77]),dtype=tf.float32) self.embedded_chars = tf.concat(1,[self.embedded_chars,v1]) ||| self.embedded_chars = tf.nn.embedding_lookup(self.W, self.input_x) //returns Tensor object v1 = tf.Variable(tf.zeros([88,77]),dtype=tf.float32) self.embedded_chars = tf.concat([self.embedded_chars,v1], 1)
API misuse ||| random_tensor = torch.rand(input.size(), generator=generator) ||| random_tensor = torch.rand(input.size(), generator=generator, device=input.device)
API misuse ||| loss += tf.nn.softmax_cross_entropy_with_logits(self.lstm_outputs[idx], tf.squeeze(true_output)) ||| loss += tf.nn.sparse_softmax_cross_entropy_with_logits(self.lstm_outputs[idx], tf.squeeze(true_output))
API misuse ||| net = tf.concat(3, [tower_conv_1, tower_conv1_1, tower_conv2_2, tower_pool]) ||| net = tf.concat(axis=3, values=[tower_conv_1, tower_conv1_1, tower_conv2_2, tower_pool])
API misuse ||| import tensorflow as tf with tf.variable_scope('test') as scope:  x = tf.get_variable('x', shape = [3, 4], initializer = tf.constant_initializer([0,1,2])) sess = tf.InteractiveSession() print(sess.run(x)) ||| import tensorflow as tf with tf.variable_scope('test') as scope: x = tf.get_variable('x', shape = [3, 4], initializer = tf.constant_initializer([0,1,2])) sess = tf.InteractiveSession() sess.run(tf.global_variables_initializer()) print(sess.run(x))
API misuse ||| cb_loss = F.binary_cross_entropy_with_logits(labels_one_hot, logits) ||| cb_loss = F.binary_cross_entropy_with_logits(input = logits,target = labels_one_hot, weights = weights)
API misuse ||| logits = torch.rand(2,2) pred = F.softmax(logits, dim=1) pred1 = F.log_softmax(logits) ||| logits = torch.rand(2,2) pred = F.softmax(logits, dim=1) pred1 = F.log_softmax(logits, dim=1)
API misuse ||| logits = torch.rand(2,2) pred = F.softmax(logits, dim=0) pred1 = F.log_softmax(logits) ||| logits = torch.rand(2,2) pred = F.softmax(logits, dim=0) pred1 = F.log_softmax(logits, dim=0)
API misuse ||| score = torch.rand(2,2) pred = F.log_softmax(score) ||| score = torch.rand(2,2) score = F.log_softmax(logits, dim=1)
Non-General ||| if self.output_attentions: attentions = tuple(t.permute(2, 3, 0, 1).contiguous() for t in attentions) ||| if self.output_attentions: if target_mapping is not None: attentions = tuple(tuple(att_stream.permute(2, 3, 0, 1).contiguous() for att_stream in t) for t in attentions) else: attentions = tuple(t.permute(2, 3, 0, 1).contiguous() for t in attentions)
Non-General ||| def __len__(self): raise NotImplementedError ||| def __len__(self): raise NotImplementedError def duplicate(self): return deepcopy(self)
Non-General ||| for param in self.get_model().parameters(): param.requires_grad = False ||| if len(self.optimizers) > 1: for param in self.get_model().parameters(): param.requires_grad = False
Non-General ||| self.total_batches = self.nb_training_batches + self.nb_val_batches ||| is_val_epoch = (self.current_epoch + 1) % self.check_val_every_n_epoch == 0 val_checks_per_epoch = self.nb_training_batches // self.val_check_batch val_checks_per_epoch = val_checks_per_epoch if is_val_epoch else 0 self.total_batches = (self.nb_training_batches + self.nb_val_batches * val_checks_per_epoch)
Non-General ||| y = np.random.random((32, 10, 64)) ||| y = np.random.random((32, 10, 64)) def test_from_config(self): with self.test_session(config=self.config) as sess: K.set_session(sess)
Non-General ||| decoder_input_fn_infer = decoders.DynamicDecoderInputs(initial_inputs=initial_input, make_input_fn=lambda x: tf.nn.embedding_lookup(target_embedding, x.predictions)) ||| decoder_input_fn_infer = decoders.DynamicDecoderInputs(initial_inputs=initial_input, make_input_fn=make_input_fn)
Non-General ||| for i in xrange(FLAGS.num_gpus): with tf.device('/gpu:%d' % i): with tf.name_scope('%s_%d' % (cifar10.TOWER_NAME, i)) as scope: loss = tower_loss(scope) tf.get_variable_scope().reuse_variables() ||| with tf.variable_scope(tf.get_variable_scope()): for i in xrange(FLAGS.num_gpus): with tf.device('/gpu:%d' % i): with tf.name_scope('%s_%d' % (cifar10.TOWER_NAME, i)) as scope: loss = tower_loss(scope) tf.get_variable_scope().reuse_variables()
Non-General ||| samples = sess.run(dcgan.sampler, feed_dict={dcgan.z: z_sample}) ||| if config.dataset == "mnist": y = np.random.choice(10, config.batch_size) y_one_hot = np.zeros((config.batch_size, 10)) y_one_hot[np.arange(config.batch_size), y] = 1 samples = sess.run(dcgan.sampler, feed_dict={dcgan.z: z_sample, dcgan.y: y_one_hot}) else: samples = sess.run(dcgan.sampler, feed_dict={dcgan.z: z_sample})
Non-General ||| self.build_model() ||| self.is_grayscale = (self.c_dim == 1) self.build_model()
Non-General ||| def __init__(self, n_input, n_hidden, optimizer = tf.train.AdamOptimizer(), gaussian_sample_size = 128): self.n_input = n_input self.n_hidden = n_hidden self.gaussian_sample_size = gaussian_sample_size ||| def __init__(self, n_input, n_hidden, optimizer = tf.train.AdamOptimizer()): self.n_input = n_input self.n_hidden = n_hidden
Non-General ||| opt = parser.parse_args() print(opt) ||| logging.basicConfig(level=logging.INFO) logger = logging.getLogger(__name__) opt = parser.parse_args() logger.info(opt)
Non-General ||| network_size = int(network_size) ||| network_size = int(network_size) pytorch_version = torch.__version__ pytorch_version = pytorch_version.rsplit('.', 2)[0]
Non-General ||| bboxlist = bboxlists[i] keep = nms(bboxlist, 0.3) bboxlist = bboxlist[keep, :] bboxlist = [x for x in bboxlist if x[-1] > 0.5] ||| bboxlist = bboxlists[i] bboxlist = self._filter_bboxes(bboxlist)
Non-General ||| task = config.task feature_config = task.features featurizer = create_featurizer(task.featurizer, feature_config) ||| supportedInputTensorizers = [FloatListTensorizer, GazetteerTensorizer, TokenTensorizer,] new_task = NewTask.from_config(config.task) input_tensorizers = { name: tensorizer for name, tensorizer in new_task.data.tensorizers.items() if any(isinstance(tensorizer, t) for t in supportedInputTensorizers) }
Non-General ||| import pandas as pd self.result = pd.DataFrame(self.result) self.result.columns = self.result.iloc[0] self.result = self.result.drop(0) ||| import pandas as pd cols = self.result[0] self.result = pd.DataFrame(self.result[1:]) self.result.columns = cols
Non-General ||| norm_locs = normalize_points((H,W), batch_locs.view(num_pts, 2).transpose(1,0)) norm_locs = torch.cat((norm_locs, torch.ones(1, num_pts)), dim=0) transtheta = transthetas[:2,:] norm_locs = torch.mm(transtheta, norm_locs) real_locs = denormalize_points(shape.tolist(), norm_locs) ||| norm_locs = torch.cat((batch_locs[0].transpose(1,0), torch.ones(1, num_pts)), dim=0) norm_locs = torch.mm(transthetas[:2, :], norm_locs) real_locs = denormalize_points(shape.tolist(), norm_locs)
Non-General ||| read_weights = hidden['read_weights'].gather(1, hidden['read_positions']) write_weights = hidden['write_weights'].gather(1, hidden['read_positions']) ||| read_weights = hidden['read_weights'].gather(1, hidden['read_positions']) if self.timestep == 1: read_weights = read_weights + 1 write_weights = hidden['write_weights'].gather(1, hidden['read_positions'])
Non-General ||| for time in range(max_length): chx = self.rnns[layer](input[time], chx) ||| for time in range(max_length): layer_input = input[time] hchx = [] for hlayer in range(self.num_hidden_layers): h = self.rnns[layer][hlayer](layer_input, chx[hlayer]) layer_input = h[0] if self.rnn_type.lower() == 'lstm' else h hchx.append(h) chx = hchx
Non-General ||| model = models.resnet50().cuda(gpu) model.fc = nn.Identity() state_dict = torch.load(args.pretrained, map_location='cpu') model.load_state_dict(state_dict) ||| model = models.resnet50().cuda(gpu) state_dict = torch.load(args.pretrained, map_location='cpu') missing_keys, unexpected_keys = model.load_state_dict(state_dict, strict=False) assert missing_keys == ['fc.weight', 'fc.bias'] and unexpected_keys == [] model.fc.weight.data.normal_(mean=0.0, std=0.01) model.fc.bias.data.zero_()
Non-General ||| mask = np.array([c == class_name for c in gt_names], dtype=np.bool_) ||| mask = np.array([c == class_name for c in gt_names], dtype=np.bool_) anchors = anchor_dict["anchors"].reshape(-1, self.box_coder.code_size) num_anchors = anchors.shape[0] anchors_mask_class = anchors_mask[anchor_start_idx:anchor_start_idx+num_anchors] if anchors_mask is not None: prune_anchor_fn = lambda _: np.where(anchors_mask_class)[0] else: prune_anchor_fn = None
Non-General ||| samples = get_samples(gen, batch_size) ||| samples = get_samples(gen, batch_size) sample_count += batch_size if reset_interval is not None and sample_count >= reset_interval: gen = make_gen()
Non-General ||| if line.startswith("obj_info"): items = line.split(" ") if len(items) != 3: raise ValueError("Invalid line: %s" % line) self.obj_info[items[1]] = items[2] ||| if line.startswith("obj_info "): self.obj_info.append(line[9:])
Non-General ||| best_valid_metrics = checkpoints[0][1] ||| all_epochs_metrics = [(f"epoch_{order_index}", valid_metric) for (order_index, valid_metric) in enumerate(self.epochs_metrics)] best_valid_metrics = top_best_checkpoints[0][1]
Non-General ||| loss_func = NTXentLoss(temperature=0.1) ||| loss_func = NTXentLoss(temperature=0.1) all_embedding_angles = [[0], [0, 10, 20]] all_labels = [torch.LongTensor([0]), torch.LongTensor([0, 0, 0])]
Non-General ||| self.writer = Logger(self.config) registry.register("writer", self.writer) ||| writer = registry.get("writer", no_warning=True) if writer: self.writer = writer else: self.writer = Logger(self.config) registry.register("writer", self.writer)
Non-General ||| def __call__(self, item): return {"text": self.tokenizer(item["text"])} ||| def __call__(self, item, *args, **kwargs): return {"text": self.tokenizer(item["text"], *args, **kwargs)}
Non-General ||| user_dir = get_mmf_env(key="user_dir") if user_dir: possible_paths.append(os.path.join(user_dir, paths)) mmf_root = get_mmf_root() ||| mmf_root = get_mmf_root() user_dir = get_mmf_env(key="user_dir") if user_dir: possible_paths.append(os.path.join(user_dir, paths)) possible_paths.append(os.path.join(mmf_root, "..", user_dir, paths))
Non-General ||| for t_model in getattr(self, embedding_attr): text_embedding = t_model(texts) ||| for t_model in getattr(self, embedding_attr): if isinstance(t_model, PreExtractedEmbedding): text_embedding = t_model(info['question_id']) else: text_embedding = t_model(texts)
Non-General ||| if (batch_idx+1) % (len(train_dataloader) / 2) == 0: print("Epoch Average Train loss : ", total_epoch_train_loss / (batch_idx+1)) ||| if ((batch_idx+1) % (len(train_dataloader) / 2) == 0) and ((batch_idx+1) < len(train_dataloader)): print("Half-Epoch Average Train loss : ", total_epoch_train_loss / (batch_idx+1))
Non-General ||| if self.use_scheduler_: schedulers = [] for i in range(self.n_estimators): schedulers.append(set_module.set_scheduler(optimizers[i], self.scheduler_name, **self.scheduler_args)) ||| if self.use_scheduler_: scheduler_ = set_module.set_scheduler(optimizers[0], self.scheduler_name, **self.scheduler_args)
Non-General ||| initialize_model(model, cfg, src_padding_idx, trg_padding_idx) ||| initialize_model(model, cfg, src_padding_idx, trg_padding_idx) pretrained_enc_embed_path = cfg["encoder"]["embeddings"].get("load_pretrained", None) pretrained_dec_embed_path = cfg["decoder"]["embeddings"].get("load_pretrained", None)
Non-General ||| def _run_act_layer_grad(act_type): x = torch.rand(10, 1000) * 10 m = MLP(act_layer=act_type) ||| def _run_act_layer_grad(act_type, inplace=True): x = torch.rand(10, 1000) * 10 m = MLP(act_layer=act_type, inplace=inplace)
Non-General ||| def create_stem(in_chs, out_chs, stem_type='', preact=True, conv_layer=None, norm_layer=None): stem = OrderedDict() ||| def create_resnetv2_stem(in_chs, out_chs=64, stem_type='', preact=True,conv_layer=StdConv2d, norm_layer=partial(GroupNormAct, num_groups=32)): stem = OrderedDict()
Non-General ||| for batch in data.batch_q_iter: feed_dict = {inputs: batch} results = model.infer(sess, feed_dict) bs_infer = results['output']['bs_infer'] batch_hyps = gen_text(bs_infer, ...) bs_turn = dict(question=question, answer=batch_hyps[1][i]) ||| for batch in data.batch_q_iter: feed_dict = {inputs: batch} results = model.infer(sess, feed_dict) bs_infer = results['output']['bs_infer'] batch_hyps = gen_text(bs_infer, ...) if beam_width > 0: bs_turn = dict(question=question, answer=batch_hyps[1][i])
Non-General ||| def call(self, inputs, mask=None) input_shape = K.int_shape(inputs) if input_shape[0]: def step(x, _): output = self.layer.call(x) return output, [] ||| def call(self, inputs, training=None, mask=None): kwargs = {} func_args = inspect.getargspec(self.layer.call).args if 'training' in func_args: kwargs['training'] = training input_shape = K.int_shape(inputs) if input_shape[0]: def step(x, _): output = self.layer.call(x, **kwargs) return output, []
Non-General ||| def test_batched_loss_is_correct(self): seq_length, batch_size, num_tags = 3, 10, 5 emissions = torch.autograd.Variable(torch.randn(seq_length, batch_size, num_tags), requires_grad=True) tags = torch.autograd.Variable(torch.LongTensor([[random.randrange(num_tags) for b in range(batch_size)] for _ in range(seq_length)])) crf = CRF(num_tags) initialize(crf) ||| def test_batched_loss_is_correct(self): crf = make_crf() batch_size = 10 emissions = make_emissions(batch_size=batch_size, num_tags=crf.num_tags) tags = make_tags(batch_size=batch_size, num_tags=crf.num_tags)
Non-General ||| def test_works_with_mask(self): seq_length, batch_size, num_tags = 3, 2, 5 emissions = torch.autograd.Variable(torch.randn(seq_length, batch_size, num_tags)) ||| def test_works_with_mask(self): crf = make_crf() seq_length, batch_size = 3, 2 emissions = make_emissions(seq_length, batch_size, crf.num_tags)
Non-General ||| bboxlist = detect(self.face_detector, image, device=self.device)[0] keep = nms(bboxlist, 0.3) bboxlist = bboxlist[keep, :] bboxlist = [x for x in bboxlist if x[-1] > 0.5] ||| bboxlist = detect(self.face_detector, image, device=self.device)[0] bboxlist = self._filter_bboxes(bboxlist)
Non-General ||| if args.audio_backend != "stempeg": torchaudio.set_audio_backend(args.audio_backend) ||| if args.audio_backend != "stempeg" and args.audio_backend is not None: torchaudio.set_audio_backend(args.audio_backend)
Non-General ||| self.model_name = model_name self.num_classes = num_classes self.pretrained = pretrained ||| self.model_name = model_name self.num_classes = num_classes self.pretrained = pretrained self.catch_output_size_exception = catch_output_size_exception
Non-General ||| import pandas as pd self.result = pd.DataFrame(self.result) self.result.columns = self.result.iloc[0] self.result = self.result.drop(0) ||| import pandas as pd cols = self.result[0] self.result = pd.DataFrame(self.result[1:]) self.result.columns = cols
Non-General ||| outputs = model(input_ids, start_positions=start_positions, end_positions=end_positions) loss, start_scores, end_scores = outputs[:2] ||| outputs = model(input_ids, start_positions=start_positions, end_positions=end_positions) loss, start_scores, end_scores = outputs[:3]
Non-General ||| model = cls(*cls_args, **cls_kwargs) ||| if len(cls_spec.args) <= 1 and not cls_spec.kwonlyargs: cls_args, cls_kwargs = [], {} model = cls(*cls_args, **cls_kwargs)
Non-General ||| opt_op = optimizer.minimize(loss, var_list=tf.trainable_variables()) opt_slots_init = tf.variables_initializer([optimizer.get_slot(v, name) for v in tf.trainable_variables() for name in optimizer.get_slot_names()]) ||| with tf.variable_scope(optimizer.get_name()) as scope: opt_op = optimizer.minimize(loss, var_list=tf.trainable_variables()) opt_slots_init = tf.variables_initializer(scope.get_collection(tf.GraphKeys.GLOBAL_VARIABLES))
Non-General ||| self.loader.reset_batch_pointer(split_idx) target = np.zeros([self.batch_size, self.seq_length, self.word_vocab_size]) ||| self.loader.reset_batch_pointer(split_idx) target = np.zeros([self.batch_size, self.seq_length])
Non-General ||| def call(self, inputs, mask=None): input_shape = K.int_shape(inputs) if input_shape[0]: def step(x, _): output = self.layer.call(x) return output, [] ||| def call(self, inputs, training=None, mask=None): kwargs = {} func_args = inspect.getargspec(self.layer.call).args if 'training' in func_args: kwargs['training'] = training input_shape = K.int_shape(inputs) if input_shape[0]: def step(x, _): output = self.layer.call(x, **kwargs) return output, []
Non-General ||| input_shape = _obtain_input_shape(input_shape, default_size=default_size, min_size=32, data_format=K.image_data_format(), require_flatten=include_top or weights, weights=weights) ||| input_shape = _obtain_input_shape(input_shape, default_size=default_size, min_size=32, data_format=K.image_data_format(), require_flatten=include_top, weights=weights)
Non-General ||| if self.return_sequences: y_rev = K.reverse(y_rev, 1) if self.merge_mode == 'concat': return K.concatenate([y, y_rev]) elif self.merge_mode == 'sum': return y + y_rev elif self.merge_mode == 'ave': return (y + y_rev) / 2 elif self.merge_mode == 'mul': return y * y_rev elif self.merge_mode is None: return [y, y_rev] ||| if self.return_sequences: y_rev = K.reverse(y_rev, 1) if self.merge_mode == 'concat': output = K.concatenate([y, y_rev]) elif self.merge_mode == 'sum': output = y + y_rev elif self.merge_mode == 'ave': output = (y + y_rev) / 2 elif self.merge_mode == 'mul': output = y * y_rev elif self.merge_mode is None: output = [y, y_rev]
Non-General ||| def call(self, inputs): return self._merge_function(inputs) ||| def call(self, inputs): return self._merge_function(inputs) def compute_output_shape(self, input_shape): return input_shape[0]
Non-General ||| def build(self, input_shape): assert len(input_shape) == 5 self.input_spec = [InputSpec(shape=input_shape)] ||| def build(self, input_shape): assert len(input_shape) == 5
Non-General ||| for padding in _convolution_paddings: for out_padding in [None, (1, 1)]: for strides in [(1, 1), (2, 2)]: if padding == 'same' and strides != (1, 1): continue ||| for padding in _convolution_paddings: for out_padding in [None, (0, 0), (1, 1)]: for strides in [(1, 1), (2, 2)]: if padding == 'same' and strides != (1, 1): continue if strides == (1, 1) and out_padding == (1, 1): continue
Non-General ||| num_words = min(MAX_NUM_WORDS, len(word_index)) + 1 embedding_matrix = np.zeros((num_words, EMBEDDING_DIM)) for word, i in word_index.items(): if i > MAX_NUM_WORDS: continue ||| num_words = min(MAX_NUM_WORDS, len(word_index) + 1) embedding_matrix = np.zeros((num_words, EMBEDDING_DIM)) for word, i in word_index.items(): if i >= MAX_NUM_WORDS: continue
Non-General ||| if padding == 'same': th_avg_pool_mode = 'average_inc_pad' elif padding == 'valid': th_avg_pool_mode = 'average_exc_pad' pool_out = pool.pool_2d(x, ws=pool_size, stride=strides, ignore_border=True, pad=pad, mode=th_avg_pool_mode) ||| pool_out = pool.pool_2d(x, ws=pool_size, stride=strides, ignore_border=True, pad=pad, mode='average_exc_pad')
Non-General ||| if self._cells[0].output_size != inputs.get_shape().as_list()[0] and self._residual_combiner == "add": inputs = tf.contrib.layers.fully_connected(inputs=inputs, num_outputs=self._cells[0].output_size, activation_fn=None, scope="input_transform") ||| if self._cells[0].output_size != inputs.get_shape().as_list()[1] and self._residual_combiner == "add": inputs = tf.contrib.layers.fully_connected(inputs=inputs, num_outputs=self._cells[0].output_size, activation_fn=None, scope="input_transform")
Non-General ||| with tf.variable_scope(self._architecture, reuse=self._reuse_weights) as var_scope: _, activations = self._resnet_model(preprocessed_inputs, num_classes=None, is_training=False, global_pool=False, output_stride=self._first_stage_features_stride, scope=var_scope) ||| with tf.variable_scope(self._architecture, reuse=self._reuse_weights) as var_scope: _, activations = self._resnet_model(preprocessed_inputs, num_classes=None, is_training=False, global_pool=False, output_stride=self._first_stage_features_stride, spatial_squeeze=False, scope=var_scope)
Non-General ||| def empty_field(self): return MultiLabelField([], self._label_namespace, skip_indexing=True) ||| def empty_field(self): return MultiLabelField([], self._label_namespace, skip_indexing=True, num_labels=self._num_labels)
Non-General ||| def apply_mv_norm(features): mean, invstddev = calc_mean_invstddev(features) res = (features - mean) * invstddev return res ||| def apply_mv_norm(features): if features.size(0) < 2: return features mean, invstddev = calc_mean_invstddev(features) res = (features - mean) * invstddev return res
Non-General ||| reverse_sampler = np.argsort(sampler) preds[0],preds[1] = preds[0][reverse_sampler],preds[1][reverse_sampler] ||| reverse_sampler = np.argsort(sampler) preds = [p[reverse_sampler] for p in preds]
Non-General ||| def _default_positive_minimum_factory(): try: value = _default(_Values.POSITIVE_MINIMUM) if value is not None: value = float(value) except ValueError: raise TypeError("Config cannot set the positive_minimum value with non float type.") return value ||| def _default_positive_minimum_factory(): value = _default(_Values.POSITIVE_MINIMUM) try: value = float(value) except ValueError: raise TypeError("Config cannot set the positive_minimum value with non float type.") return value
Non-General ||| return T.nnet.bn.batch_normalization(x, gamma, beta, mean, sqrt(var) + epsilon, mode='high_mem') ||| return T.nnet.bn.batch_normalization(x, gamma, beta, mean, sqrt(var + epsilon), mode='high_mem')
Non-General ||| def get_output(self, train=False): return 1 ||| def get_output(self, train=False): X = self.get_input(train) return X * T.shape_padright(T.any((1. - T.eq(X, self.mask_value)), axis=-1))
Non-General ||| dropout_input_keep_prob=self.params["rnn_cell.dropout_input_keep_prob"], dropout_output_keep_prob=self.params["rnn_cell.dropout_output_keep_prob"]) ||| dropout_input_keep_prob=(self.params["rnn_cell.dropout_input_keep_prob"] if enable_dropout else 1.0), dropout_output_keep_prob=(self.params["rnn_cell.dropout_output_keep_prob"] if enable_dropout else 1.0))
Non-General ||| cdna_kerns = tf.tile(cdna_kerns, [1, 1, 1, color_channels, 1]) cdna_kerns = tf.split(axis=0, num_or_size_splits=batch_size, value=cdna_kerns) prev_images = tf.split(axis=0, num_or_size_splits=batch_size, value=prev_image) ||| cdna_kerns = tf.transpose(cdna_kerns, [1, 2, 0, 4, 3]) cdna_kerns = tf.reshape(cdna_kerns, [DNA_KERN_SIZE, DNA_KERN_SIZE, batch_size, num_masks]) prev_image = tf.transpose(prev_image, [3, 1, 2, 0])
Non-General ||| labels = 2 - tf.to_int32(labels) predictions = 3 - tf.to_int32(predictions) * 2 ||| labels = (2 - tf.to_int32(labels)) - 1 predictions = (3 - tf.to_int32(predictions) * 2) - 1
Non-General ||| if edge_attr is not None: if edge_attr.dim() == 1: edge_attr = edge_attr.view(-1, 1) assert self.lin_edge is not None ||| if edge_attr is not None and self.lin_edge is not None: if edge_attr.dim() == 1: edge_attr = edge_attr.view(-1, 1)
Non-General ||| def format_for_prediction(self, report): return generate_prediction(report) ||| def format_for_prediction(self, report): if self.is_multilabel: return generate_multilabel_prediction(report) else: return generate_binary_prediction(report)
Non-General ||| combined_report = None num_batches_for_this_update = 1 for idx, batch in enumerate(self.train_loader): if (idx + 1) % self.training_config.update_frequency == 0: combined_report = None num_batches_for_this_update = min(self.training_config.update_frequency, num_remaining_batches) self._start_update() ||| should_start_update = True for idx, batch in enumerate(self.train_loader): if should_start_update: combined_report = None self._start_update() num_batches_for_this_update = min(self.training_config.update_frequency, num_remaining_batches) should_start_update = False self.current_iteration += 1
Non-General ||| def __init__(self, max_features=None): self.max_features = max_features ||| def __init__(self, max_features=None): self.max_features = max_features if self.max_features: patch_dim = math.ceil(math.sqrt(self.max_features)) self.img_h = patch_dim self.img_w = patch_dim
Non-General ||| current_sample.text_len = torch.tensor(len(sample_info["question_tokens"]), dtype=torch.int) ||| if "question_tokens" in sample_info: current_sample.text_len = torch.tensor(len(sample_info["question_tokens"]), dtype=torch.int)
Non-General ||| scores = torch.max(model_output["scores"], dim=-1)[1] ||| if "captions" in model_output: scores = model_output["captions"] else: scores = torch.max(model_output["scores"], dim=-1)[1]
Non-General ||| src = self.args.zip_file self.checksum(self.args.zip_file, self.POSSIBLE_CHECKSUMS) ||| if not bypass_checksum: self.checksum(self.args.zip_file, self.POSSIBLE_CHECKSUMS) src = self.args.zip_file
Non-General ||| def fetch_requirements(): with open("requirements.txt") as f: reqs = f.read() return reqs ||| def fetch_requirements(): requirements_file = "requirements.txt" if platform.system() == "Windows": DEPENDENCY_LINKS.append("https://download.pytorch.org/whl/torch_stable.html") with open(requirements_file) as f: reqs = f.read() return reqs
Non-General ||| self.base = UnimodalBase(self.config) num_features = 100 self._is_direct_features_input = self.config.direct_features_input if not self._is_direct_features_input: num_features = self.config.modal_encoder.params.num_output_features ||| self.base = UnimodalBase(self.config) self._is_direct_features_input = self.config.direct_features_input num_features = self.config.modal_encoder.params.num_output_features
Non-General ||| self.logger = logging.getLogger(__name__) self._file_only_logger = logging.getLogger(__name__) ||| if not name: name = __name__ self.logger = logging.getLogger(name) self._file_only_logger = logging.getLogger(name)
Non-General ||| def build(self): encoders = self._build_encoders(self.config) self.text_encoder, self.modal_encoder = encoders[0], encoders[1] self._encoder_config = self.text_encoder.config ||| def build(self): encoders = self._build_encoders(self.config) self.text_encoder, self.modal_encoder = encoders[0], encoders[1] self._encoder_config = None if self.text_encoder: self._encoder_config = self.text_encoder.config
Non-General ||| def _build_encoders(self, config): return (build_text_encoder(config.text_encoder), self._build_modal_encoder(config.modal_encoder),) ||| def _build_encoders(self, config): text_encoder = None if config.get("text_encoder", None): text_encoder = build_text_encoder(config.text_encoder) modal_encoder = None if config.get("modal_encoder", None): modal_encoder = self._build_modal_encoder(config.modal_encoder) return (text_encoder, modal_encoder)
Non-General ||| with open("./list", "r") as f: lines = f.readlines() for line in lines: exclude[line.strip("\n").split(os.path.sep)[-1].split(".")[0]] = 1 ||| if os.path.exists(self.args.exclude_list): with open(self.args.exclude_list, "r") as f: lines = f.readlines() for line in lines: exclude[line.strip("\n").split(os.path.sep)[-1].split(".")[0]] = 1
Non-General ||| def build_trainer(args, *rest, **kwargs): configuration = Configuration(args.config) configuration.override_with_cmd_config(args.config_override) configuration.override_with_cmd_opts(args.opts) configuration.update_with_args(args) configuration.freeze() ||| def build_trainer(args, *rest, **kwargs): configuration = Configuration(args) configuration.freeze()
Non-General ||| config_path = os.path.abspath(config_path) configuration = Configuration(config_path) ||| config_path = os.path.abspath(config_path) args = dummy_args() args.opts.append("config={}".format(config_path)) configuration = Configuration(args)
Non-General ||| if self.k is not None: num_k = self.k ||| num_k = self.determine_k(label_counts[1], len(reference), embeddings_come_from_same_source)
Non-General ||| index = faiss.IndexFlatL2(d) if faiss.get_num_gpus() > 0: index = faiss.index_cpu_to_all_gpus(index) index.add(reference_embeddings) distances, indices = index.search(test_embeddings, k + 1) ||| cpu_index = faiss.IndexFlatL2(d) distances, indices = try_gpu(cpu_index, reference_embeddings, test_embeddings, k)
Non-General ||| logits = self.forward(input_ids=input_ids, token_type_ids=input_type_ids, attention_mask=input_mask,) mlm_loss = self.mlm_loss(log_probs=logits[0], labels=output_ids, output_mask=output_mask) if self.only_mlm_loss: loss = mlm_loss else: nsp_loss = self.nsp_loss(logits=logits[1], labels=labels) loss = self.agg_loss(loss_1=mlm_loss, loss_2=nsp_loss) ||| forward_outputs = self.forward(input_ids=input_ids, token_type_ids=input_type_ids, attention_mask=input_mask) mlm_log_probs, nsp_logits = self._parse_forward_outputs(forward_outputs) _, _, loss = self._compute_losses(mlm_log_probs, nsp_logits, output_ids, output_mask, labels)
Non-General ||| self.setup_optimization(cfg.optim) ||| self.training_perplexity = Perplexity(dist_sync_on_step=True) self.validation_perplexity = Perplexity(compute_on_step=False) self.setup_optimization(cfg.optim)
Non-General ||| if self.extension in ("jpg", "JPG", "jpeg", "JPEG"): image = np.array(imread(uri=image_path,grayscale=self.grayscale,expand_dims=self.expand_dims,exifrotate=not self.clear_exif)) else: image = np.array(imread(uri=image_path,grayscale=self.grayscale,expand_dims=self.expand_dims)) ||| _, extension = os.path.splitext(image_path) kwargs = {"grayscale": self.grayscale, "expand_dims": self.expand_dims} if extension.lower() in {"jpg", "jpeg"}: kwargs["exifrotate"] = not self.clear_exif image = np.array(imread(uri=image_path, **kwargs))
Non-General ||| def process_all(self, pool: Pool): images = [*self.in_dir.glob(f"**/*.{self.extension}")] ||| def process_all(self, pool: Pool): images: List[Path] = [] for root, dirs, files in os.walk(self.in_dir): root = Path(root) images.extend([root / filename for filename in files if has_image_extension(filename)])
Non-General ||| def prepare_df_from_dirs(in_dir, tag_column_name): if not in_dir.endswith("/"): in_dir = f"{in_dir}/" dataset = create_dataset(f"{in_dir}/**", process_fn=lambda x: x.replace(f"{in_dir}", "")) df = create_dataframe(dataset, columns=[tag_column_name, "filepath"]) ||| def prepare_df_from_dirs(in_dirs, tag_column_name): dfs = [] splitted_dirs = in_dirs.strip(',').split(',')
Non-General ||| if self.batch_granularity: self.fieldnames = ['epoch', 'batch', 'size', 'time', 'lr'] else: self.fieldnames = ['epoch', 'time', 'lr'] ||| if self.batch_granularity: self.fieldnames = ['epoch', 'batch', 'size', 'time'] else: self.fieldnames = ['epoch', 'time'] if len(self.model.optimizer.param_groups) > 1: self.fieldnames += [f'lr_group_{i}' for i in range(len(self.model.optimizer.param_groups))] else: self.fieldnames += ['lr']
Non-General ||| def _get_current_learning_rates(self): learning_rates = [param_group['lr'] for param_group in self.model.optimizer.param_groups] return learning_rates[0] if len(learning_rates) == 1 else learning_rates ||| def _get_current_learning_rates(self): if len(self.model.optimizer.param_groups) > 1: learning_rates = {f'lr_group_{i}': param_group['lr'] for i, param_group in enumerate(self.model.optimizer.param_groups)} else: learning_rates = {'lr': self.model.optimizer.param_groups[0]['lr']} return learning_rates
Non-General ||| lr = self._get_current_learning_rates() if isinstance(lr, (list, )): self.writer.add_scalars('lr', {str(i): v for i, v in enumerate(lr)}, epoch_number) else: self.writer.add_scalars('lr', {'lr': lr}, epoch_number) ||| lr = self._get_current_learning_rates() self.writer.add_scalars('lr', lr, epoch_number)
Non-General ||| calls.append(call('lr', {'lr': self.lr}, h['epoch'])) ||| if len(lrs) == 1: calls.append(call('lr', {'lr': self.lr}, h['epoch'])) else: calls.append(call('lr', {f'lr_group_{i}': lr for i, lr in enumerate(lrs)}, h['epoch']))
Non-General ||| nb_subplot_cols = 3 gs = mpl.gridspec.GridSpec(1, nb_subplot_cols) ||| nb_subplot_cols = 3 if is_debug: nb_subplot_cols += dataset_info.include_ir + dataset_info.include_depth + dataset_info.include_ndvi gs = mpl.gridspec.GridSpec(1, nb_subplot_cols)
Non-General ||| a = fig.add_subplot(gs[subplot_index]) a.axes.get_xaxis().set_visible(False) a.axes.get_yaxis().set_visible(False) a.imshow(im.astype(np.uint8)) ||| a = fig.add_subplot(gs[subplot_index]) a.axes.get_xaxis().set_visible(False) a.axes.get_yaxis().set_visible(False) if is_rgb: a.imshow(im.astype(np.uint8)) else: a.imshow(im, cmap='gray', vmin=0, vmax=255)
Non-General ||| predictions_path = join(run_path, 'predictions', '{}.pdf'.format(sample_index)) ||| file_name = '{}_debug.pdf' if is_debug else '{}.pdf' file_name = file_name.format(sample_index) predictions_path = join(run_path, 'predictions', file_name)
Non-General ||| def make_split_generator(dataset_info, split, batch_size=32, shuffle=False, augment=False, scale=False, eval_mode=False): path = dataset_info.dataset_path split_path = join(path, split) ||| def make_split_generator(dataset_info, split, batch_size=32, shuffle=False,reset_interval=None, augment=False, scale=False,eval_mode=False): path = dataset_info.dataset_path split_path = join(path, split)
Non-General ||| a = 0 if anchors_mask is not None: prune_anchor_fn = lambda _: np.where(anchors_mask)[0] else: prune_anchor_fn = None ||| a = 0
Non-General ||| unmix = model.OpenUnmix(input_mean=scaler_mean,input_scale=scaler_std,nb_bins=args.nfft // 2 + 1,nb_channels=args.nb_channels,hidden_size=args.hidden_size,max_bin=max_bin,).to(device) ||| if args.model: unmix = utils.load_target_models(args.target, model_str_or_path=args.model, device=device, pretrained=True)[args.target] unmix = unmix.to(device) else: unmix = model.OpenUnmix(input_mean=scaler_mean,input_scale=scaler_std,nb_bins=args.nfft // 2 + 1,nb_channels=args.nb_channels,hidden_size=args.hidden_size,max_bin=max_bin,).to(device)
Non-General ||| if self.reduction_method == 'random_forrest': pass if self.reduction_method == 'extra_trees': pass ||| if self.reduction_method == 'forrest': self = forrest(self) if self.reduction_method == 'trees': self = trees(self)
Non-General ||| loss.backward() ||| if IS_AMP_AVAILABLE and hasattr(self.optimizer, '_amp_stash'): with amp.scale_loss(loss, self.optimizer) as scaled_loss: scaled_loss.backward() else: loss.backward()
Non-General ||| if self.separable: lh, hl, hh = torch.unbind(h, dim=2) filts = (self.g0_col, self.g1_col, self.g0_row, self.g1_row) ll = lowlevel.sfb2d(ll, lh, hl, hh, filts, mode=self.mode) else: c = torch.cat((ll[:,:,None], h), dim=2) ll = lowlevel.sfb2d_nonsep(c, self.h, mode=self.mode) ||| ll = lowlevel.SFB2D.apply(ll, h, self.g0_col, self.g1_col, self.g0_row, self.g1_row, mode)
Non-General ||| max_length = int(least_used_mem[0, 0].data.cpu().numpy()) ||| max_length = int(least_used_mem[0, 0].data.cpu().numpy()) if not self.mem_limit_reached else (m-1)
Non-General ||| if chx is None: chx = cuda(T.zeros(self.num_layers, batch_size, self.output_size), gpu_id=self.gpu_id) if self.rnn_type.lower() == 'lstm': chx = (chx, chx) ||| if chx is None: chx = cuda(T.zeros(batch_size, self.output_size), gpu_id=self.gpu_id) if self.rnn_type.lower() == 'lstm': chx = [ [ (chx.clone(), chx.clone()) for h in range(self.num_hidden_layers) ] for l in range(self.num_layers) ] else: chx = [ [ chx.clone() for h in range(self.num_hidden_layers) ] for l in range(self.num_layers) ]